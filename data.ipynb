{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Dataset: Exploration, Preparation, and Augmentation\n",
    "\n",
    "This notebook focuses on understanding and preparing the handwritten character dataset. It covers:\n",
    "- Essential library imports and device setup.\n",
    "- Custom data augmentation transformations.\n",
    "- A data pipeline (`HandwritingDataPipeline`) for loading, transforming, and splitting the dataset.\n",
    "- Showcasing combined data augmentations as applied by the pipeline.\n",
    "- Demonstrating individual image transformations and their effects.\n",
    "- Basic analysis of the dataset, including class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# Image processing and display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 # OpenCV for image operations\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch essentials\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Note: Ensure the 'data_root_example' variable later in this notebook \n",
    "# points to the correct path of your dataset for full functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Data Augmentation Transforms\n",
    "\n",
    "These are custom PyTorch transforms used to augment the image data, helping to make the model more robust to variations in handwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomChoice(torch.nn.Module):\n",
    "    \"\"\"Randomly applies one of the given transforms with given probability\"\"\"\n",
    "    def __init__(self, transforms, p=0.5):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            transform = random.choice(self.transforms)\n",
    "            return transform(img)\n",
    "        return img\n",
    "\n",
    "class ThicknessTransform(torch.nn.Module):\n",
    "    \"\"\"Apply morphological operations to change stroke thickness.\n",
    "    It randomly chooses between dilation (thicker) or erosion (thinner).\n",
    "    Args:\n",
    "        kernel_size (int): Size of the kernel for morphological operations (default: 3).\n",
    "        iterations (int): Number of times to apply the operation (default: 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, iterations=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to OpenCV format (numpy array)\n",
    "        img_cv = np.array(img)\n",
    "        \n",
    "        # Ensure image is grayscale for morphological operations\n",
    "        if len(img_cv.shape) == 3 and img_cv.shape[2] == 3:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2GRAY)\n",
    "        elif len(img_cv.shape) == 3 and img_cv.shape[2] == 1: # Already grayscale but 3-channel\n",
    "             img_cv = img_cv[:, :, 0]\n",
    "        \n",
    "        kernel = np.ones((self.kernel_size, self.kernel_size), np.uint8)\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            # Dilation (thicker)\n",
    "            processed_img = cv2.dilate(img_cv, kernel, iterations=self.iterations)\n",
    "        else:\n",
    "            # Erosion (thinner)\n",
    "            processed_img = cv2.erode(img_cv, kernel, iterations=self.iterations)\n",
    "        \n",
    "        # Convert back to PIL Image\n",
    "        return Image.fromarray(processed_img, mode='L') # 'L' for grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handwriting Data Pipeline\n",
    "\n",
    "The `HandwritingDataPipeline` class encapsulates all steps for data loading, transformation, and splitting into training, validation, and test sets. It's designed to work with datasets structured in the ImageFolder format (where each subdirectory represents a class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataPipeline:\n",
    "    def __init__(self, data_root, image_size=(64, 64), batch_size=32, do_transform=True, test_split=0.15, val_split=0.15):\n",
    "        self.data_root = data_root\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.do_transform = do_transform\n",
    "        self.test_split = test_split\n",
    "        self.val_split = val_split \n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self._setup_transforms()\n",
    "        self._load_and_split_datasets()\n",
    "\n",
    "    def _setup_transforms(self):\n",
    "        if self.do_transform:\n",
    "            # This is the full augmentation pipeline used for training.\n",
    "            # It assumes 3-channel output for compatibility with models like VGG.\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                RandomChoice([\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10, fill=255),\n",
    "                    transforms.RandomPerspective(distortion_scale=0.3, p=0.5, fill=255),\n",
    "                    transforms.RandomRotation(15, fill=255),\n",
    "                ], p=0.8),\n",
    "                ThicknessTransform(kernel_size=random.choice([1,2,3]), iterations=random.choice([1,2])),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.5))\n",
    "                ], p=0.3),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "                self.normalize,\n",
    "                transforms.RandomErasing(p=0.2, scale=(0.02, 0.03), ratio=(0.3, 3.3), value='random')\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "                self.normalize\n",
    "            ])\n",
    "\n",
    "        self.val_test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "            self.normalize\n",
    "        ])\n",
    "\n",
    "    def _load_and_split_datasets(self):\n",
    "        try:\n",
    "            full_dataset = datasets.ImageFolder(root=self.data_root)\n",
    "            self.class_names = full_dataset.classes\n",
    "            self.num_classes = len(self.class_names)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Dataset not found at {self.data_root}. Please check the path.\")\n",
    "            self.class_names = []\n",
    "            self.num_classes = 0\n",
    "            self.train_dataset = Dataset() # Empty dataset\n",
    "            self.val_dataset = Dataset()\n",
    "            self.test_dataset = Dataset()\n",
    "            self.sizes = {'train': 0, 'val': 0, 'test': 0}\n",
    "            return\n",
    "\n",
    "        total_size = len(full_dataset)\n",
    "        if total_size == 0:\n",
    "            print(f\"ERROR: Dataset at {self.data_root} is empty.\")\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = Dataset(), Dataset(), Dataset()\n",
    "            self.sizes = {'train': 0, 'val': 0, 'test': 0}\n",
    "            return\n",
    "            \n",
    "        test_size = int(total_size * self.test_split)\n",
    "        remaining_size = total_size - test_size\n",
    "        val_size = int(remaining_size * (self.val_split / (1.0 - self.test_split)) if (1.0 - self.test_split) > 0 else 0)\n",
    "        train_size = remaining_size - val_size\n",
    "\n",
    "        if train_size <= 0 or val_size < 0 or test_size < 0: # val_size or test_size can be 0 for small datasets\n",
    "            print(f\"Warning: Dataset too small for current split ratios (Total: {total_size}). Adjusting...\")\n",
    "            if total_size < 3:\n",
    "                # Use all data for all sets if very small, not ideal but prevents crashes\n",
    "                train_dataset_subset, val_dataset_subset, test_dataset_subset = full_dataset, full_dataset, full_dataset\n",
    "                self.sizes = {'train': total_size, 'val': total_size, 'test': total_size}\n",
    "            else:\n",
    "                # Prioritize training set, then validation, then test\n",
    "                train_size = max(1, int(total_size * 0.7))\n",
    "                val_size = max(1, int(total_size * 0.15))\n",
    "                test_size = total_size - train_size - val_size\n",
    "                if test_size < 0: test_size = 0 # Ensure non-negative\n",
    "                \n",
    "                # Perform the split with adjusted sizes\n",
    "                train_temp_dataset, test_dataset_subset = torch.utils.data.random_split(full_dataset, [train_size + val_size, test_size],\n",
    "                                                                              generator=torch.Generator().manual_seed(42))\n",
    "                train_dataset_subset, val_dataset_subset = torch.utils.data.random_split(train_temp_dataset, [train_size, val_size],\n",
    "                                                                           generator=torch.Generator().manual_seed(42))\n",
    "        else:\n",
    "            print(f\"Attempting to split: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "            try:\n",
    "                train_temp_dataset, test_dataset_subset = torch.utils.data.random_split(full_dataset, [train_size + val_size, test_size],\n",
    "                                                                              generator=torch.Generator().manual_seed(42))\n",
    "                train_dataset_subset, val_dataset_subset = torch.utils.data.random_split(train_temp_dataset, [train_size, val_size],\n",
    "                                                                           generator=torch.Generator().manual_seed(42))\n",
    "            except Exception as e:\n",
    "                print(f\"Error during dataset splitting: {e}. Using full dataset for all.\")\n",
    "                train_dataset_subset, val_dataset_subset, test_dataset_subset = full_dataset, full_dataset, full_dataset\n",
    "\n",
    "        self.train_dataset = TransformedDataset(train_dataset_subset, transform=self.train_transform)\n",
    "        self.val_dataset = TransformedDataset(val_dataset_subset, transform=self.val_test_transform)\n",
    "        self.test_dataset = TransformedDataset(test_dataset_subset, transform=self.val_test_transform)\n",
    "        \n",
    "        self.sizes = {'train': len(self.train_dataset), 'val': len(self.val_dataset), 'test': len(self.test_dataset)}\n",
    "\n",
    "    def get_loaders(self, shuffle_train=True, shuffle_val=False, shuffle_test=False):\n",
    "        if self.sizes['train'] == 0 and self.sizes['val'] == 0 and self.sizes['test'] == 0 and self.num_classes == 0:\n",
    "            print(\"Pipeline not initialized properly or dataset empty/not found. Returning empty DataLoaders.\")\n",
    "            return DataLoader(Dataset()), DataLoader(Dataset()), DataLoader(Dataset())\n",
    "            \n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=shuffle_train, num_workers=0)\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=shuffle_val, num_workers=0)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=shuffle_test, num_workers=0)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self.class_names\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Initialize Data Pipeline and Get Statistics\n",
    "\n",
    "Set the `data_root_example` variable to the path of your dataset. The dataset should be organized in an ImageFolder structure (root_directory -> class_subdirectories -> images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< USER: CHANGE THIS PATH to your dataset's root folder >>>\n",
    "data_root_example = \"./datasets/handwritten-english/augmented_images/augmented_images1\"\n",
    "\n",
    "print(f\"Attempting to initialize data pipeline with root: {data_root_example}\")\n",
    "\n",
    "if not os.path.exists(data_root_example) or not os.listdir(data_root_example):\n",
    "    print(f\"\\nWARNING: The directory '{data_root_example}' does not exist or is empty.\")\n",
    "    print(\"Please ensure your dataset is available at this path and structured correctly (ImageFolder format).\")\n",
    "    print(\"Subsequent cells requiring data may fail or show limited results.\")\n",
    "    example_pipeline = None\n",
    "    train_loader_example = None \n",
    "else:\n",
    "    try:\n",
    "        example_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=16, do_transform=True)\n",
    "        train_loader_example, _, _ = example_pipeline.get_loaders()\n",
    "        dataset_sizes_example = example_pipeline.sizes\n",
    "        num_classes_example = example_pipeline.num_classes\n",
    "        class_labels_example = example_pipeline.get_class_labels()\n",
    "\n",
    "        print(f\"\\nData pipeline initialized successfully.\")\n",
    "        print(f\"Dataset split sizes: {dataset_sizes_example}\")\n",
    "        print(f\"Number of classes: {num_classes_example}\")\n",
    "        if class_labels_example:\n",
    "            print(f\"First 10 Class labels: {class_labels_example[:10]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR initializing data pipeline or getting loaders: {e}\")\n",
    "        example_pipeline = None\n",
    "        train_loader_example = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Display Combined Augmented Images\n",
    "\n",
    "This function visualizes the effect of the complete augmentation pipeline applied to training images. It fetches a batch of images from the training loader and displays a few of them, each with multiple augmented versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_augmented_images(data_loader, num_images=5, num_augmentations=3):\n",
    "    \"\"\"Displays original and augmented images from the train_loader.\n",
    "    Assumes data_loader.dataset.subset.dataset is the ImageFolder-like dataset.\n",
    "    \"\"\"\n",
    "    if data_loader is None or not hasattr(data_loader, 'dataset') or len(data_loader.dataset) == 0:\n",
    "        print(\"Data loader is None or empty. Cannot display images.\")\n",
    "        return\n",
    "    if not hasattr(data_loader.dataset, 'subset') or not hasattr(data_loader.dataset.subset, 'dataset') \\\n",
    "       or not hasattr(data_loader.dataset.subset.dataset, 'class_to_idx') \\\n",
    "       or not hasattr(data_loader.dataset.subset.dataset, 'imgs'):\n",
    "        print(\"Data loader is not structured as expected (TransformedDataset -> Subset -> ImageFolder). Cannot display original image details.\")\n",
    "        # Fallback to just showing augmented images from the loader if structure is different\n",
    "        try:\n",
    "            inputs, _ = next(iter(data_loader))\n",
    "            plt.figure(figsize=(num_augmentations * 2, num_images * 2))\n",
    "            for i in range(min(num_images * num_augmentations, len(inputs))):\n",
    "                ax = plt.subplot(num_images, num_augmentations, i + 1)\n",
    "                img_display = inputs[i].cpu().numpy().transpose((1, 2, 0))\n",
    "                mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
    "                img_display = std * img_display + mean; img_display = np.clip(img_display, 0, 1)\n",
    "                plt.imshow(img_display.squeeze(), cmap='gray' if img_display.shape[2]==1 else None)\n",
    "                ax.set_title(f'Aug. Img {i+1}')\n",
    "                ax.axis('off')\n",
    "            plt.tight_layout(); plt.show()\n",
    "        except Exception as e_fallback:\n",
    "            print(f\"Fallback display failed: {e_fallback}\")\n",
    "        return\n",
    "\n",
    "    imagefolder_dataset = data_loader.dataset.subset.dataset\n",
    "    class_to_idx = imagefolder_dataset.class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    subset_indices = data_loader.dataset.subset.indices\n",
    "\n",
    "    if len(subset_indices) < num_images:\n",
    "        print(f\"Warning: Requested {num_images} images, but dataset subset has {len(subset_indices)}. Displaying available.\")\n",
    "        num_images = len(subset_indices)\n",
    "    if num_images == 0: print(\"No images to display from subset.\"); return\n",
    "\n",
    "    random_subset_indices = random.sample(range(len(subset_indices)), num_images)\n",
    "    fig = plt.figure(figsize=( (num_augmentations + 1) * 3, num_images * 3) )\n",
    "    train_transform = data_loader.dataset.transform\n",
    "\n",
    "    for i, random_idx_in_subset in enumerate(random_subset_indices):\n",
    "        original_dataset_idx = subset_indices[random_idx_in_subset]\n",
    "        original_path, true_label_idx = imagefolder_dataset.imgs[original_dataset_idx]\n",
    "        class_name = idx_to_class[true_label_idx]\n",
    "        original_pil = Image.open(original_path)\n",
    "\n",
    "        ax = plt.subplot(num_images, num_augmentations + 1, i * (num_augmentations + 1) + 1)\n",
    "        ax.imshow(original_pil.convert(\"RGB\")); ax.set_title(f'Original: {class_name}'); ax.axis('off')\n",
    "\n",
    "        for j in range(num_augmentations):\n",
    "            augmented_tensor = train_transform(original_pil.copy())\n",
    "            ax = plt.subplot(num_images, num_augmentations + 1, i * (num_augmentations + 1) + j + 2)\n",
    "            img_display = augmented_tensor.cpu().numpy().transpose((1, 2, 0))\n",
    "            mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
    "            img_display = std * img_display + mean; img_display = np.clip(img_display, 0, 1)\n",
    "            if img_display.shape[2] == 1: plt.imshow(img_display.squeeze(), cmap='gray')\n",
    "            else: plt.imshow(img_display)\n",
    "            ax.set_title(f'Aug {j+1}: {class_name}'); ax.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "if train_loader_example:\n",
    "    print(\"Displaying augmented images from the initialized pipeline...\")\n",
    "    display_augmented_images(train_loader_example, num_images=4, num_augmentations=3)\n",
    "else:\n",
    "    print(\"Skipping display_augmented_images example as 'train_loader_example' is not available. Please check data_root_example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Individual Transformation Showcase\n",
    "\n",
    "This section demonstrates the effect of individual transformations on a sample image. This helps in understanding how each augmentation technique contributes to data diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image_path(data_root):\n",
    "    \"\"\"Tries to get a path to a sample image from the dataset.\"\"\"\n",
    "    if not data_root or not os.path.exists(data_root):\n",
    "        return None\n",
    "    try:\n",
    "        full_dataset = datasets.ImageFolder(root=data_root)\n",
    "        if len(full_dataset.samples) > 0:\n",
    "            return full_dataset.samples[0][0] # Return path of the first sample\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load sample image from dataset: {e}\")\n",
    "    return None\n",
    "\n",
    "sample_image_path = get_sample_image_path(data_root_example)\n",
    "\n",
    "if not sample_image_path:\n",
    "    print(\"Sample image path not found. Creating a dummy image for transformation showcase.\")\n",
    "    # Create a dummy image if no dataset image is available\n",
    "    dummy_img_np = np.full((100, 100, 3), (200, 200, 200), dtype=np.uint8) # Light gray background\n",
    "    cv2.putText(dummy_img_np, \"A\", (25, 75), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,0), 5) # Black 'A'\n",
    "    sample_pil_image = Image.fromarray(dummy_img_np)\n",
    "else:\n",
    "    try:\n",
    "        sample_pil_image = Image.open(sample_image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening sample image {sample_image_path}: {e}. Using dummy image.\")\n",
    "        dummy_img_np = np.full((100, 100, 3), (200, 200, 200), dtype=np.uint8)\n",
    "        cv2.putText(dummy_img_np, \"A\", (25, 75), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,0), 5)\n",
    "        sample_pil_image = Image.fromarray(dummy_img_np)\n",
    "\n",
    "def plot_transformed_image(original_img, transformed_img, title_original=\"Original\", title_transformed=\"Transformed\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(title_original)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(transformed_img, cmap='gray' if isinstance(transformed_img, np.ndarray) and transformed_img.ndim == 2 else None)\n",
    "    axes[1].set_title(title_transformed)\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ensure sample_pil_image is not None before proceeding\n",
    "if sample_pil_image is None:\n",
    "    print(\"ERROR: sample_pil_image is None. Cannot proceed with individual transformation showcase.\")\n",
    "else:\n",
    "    print(\"Showing individual transformations (ensure sample_pil_image was loaded):\")\n",
    "    # 1. Random Rotation\n",
    "    rotator = transforms.RandomRotation(degrees=30, fill=255) # fill with white for rotation\n",
    "    rotated_image = rotator(sample_pil_image.copy())\n",
    "    plot_transformed_image(sample_pil_image, rotated_image, \"Original\", \"Random Rotation (30 deg, fill white)\")\n",
    "    print(\"RandomRotation: Randomly rotates the image. `fill` handles empty areas after rotation.\")\n",
    "\n",
    "    # 2. Gaussian Blur\n",
    "    blurrer = transforms.GaussianBlur(kernel_size=(5,9), sigma=(0.1, 5.0))\n",
    "    blurred_image = blurrer(sample_pil_image.copy())\n",
    "    plot_transformed_image(sample_pil_image, blurred_image, \"Original\", \"Gaussian Blur\")\n",
    "    print(\"GaussianBlur: Blurs the image, can simulate out-of-focus or pen strokes.\")\n",
    "\n",
    "    # 3. ThicknessTransform (custom)\n",
    "    # Ensure input is PIL, as ThicknessTransform expects it and converts to CV internally\n",
    "    thickness_transformer = ThicknessTransform(kernel_size=3, iterations=1)\n",
    "    thick_image = thickness_transformer(sample_pil_image.copy().convert('L')) # Convert to grayscale for this transform\n",
    "    plot_transformed_image(sample_pil_image.convert('L'), thick_image, \"Original Grayscale\", \"Thickness Transform\")\n",
    "    print(\"ThicknessTransform: Simulates variations in stroke thickness using dilation/erosion.\")\n",
    "\n",
    "    # 4. Canny Edge Detection (OpenCV)\n",
    "    img_cv_for_canny = cv2.cvtColor(np.array(sample_pil_image.copy()), cv2.COLOR_RGB2GRAY)\n",
    "    canny_edges = cv2.Canny(img_cv_for_canny, threshold1=100, threshold2=200)\n",
    "    plot_transformed_image(img_cv_for_canny, canny_edges, \"Original Grayscale\", \"Canny Edges\")\n",
    "    print(\"Canny Edge Detection: Detects strong edges. Can be useful for feature extraction or preprocessing.\")\n",
    "\n",
    "    # 5. Adaptive Thresholding (OpenCV)\n",
    "    img_cv_for_thresh = cv2.cvtColor(np.array(sample_pil_image.copy()), cv2.COLOR_RGB2GRAY)\n",
    "    adaptive_thresh_img = cv2.adaptiveThreshold(img_cv_for_thresh, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \\\n",
    "                                                cv2.THRESH_BINARY_INV, 11, 2) # Inverted: white text on black bg\n",
    "    plot_transformed_image(img_cv_for_thresh, adaptive_thresh_img, \"Original Grayscale\", \"Adaptive Threshold (Inverted)\")\n",
    "    print(\"Adaptive Thresholding: Useful for binarizing images with varying illumination. Here, THRESH_BINARY_INV makes objects white.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Analysis: Class Distribution\n",
    "\n",
    "Understanding the distribution of images across different classes is crucial. Significant imbalances can affect model training, potentially biasing the model towards over-represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if example_pipeline and example_pipeline.num_classes > 0:\n",
    "    print(f\"Analyzing class distribution for dataset at: {data_root_example}\")\n",
    "    # Accessing the full dataset before splitting to get overall distribution\n",
    "    # This assumes data_root_example is valid and points to an ImageFolder structure\n",
    "    try:\n",
    "        full_dataset_for_analysis = datasets.ImageFolder(root=data_root_example)\n",
    "        class_counts = collections.Counter([sample[1] for sample in full_dataset_for_analysis.samples])\n",
    "        class_names_analysis = [full_dataset_for_analysis.classes[i] for i in range(len(full_dataset_for_analysis.classes))]\n",
    "        counts = [class_counts[i] for i in range(len(class_names_analysis))]\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.bar(class_names_analysis, counts)\n",
    "        plt.xlabel(\"Class Label\")\n",
    "        plt.ylabel(\"Number of Images\")\n",
    "        plt.title(\"Class Distribution in the Dataset\")\n",
    "        plt.xticks(rotation=90, fontsize=8) # Rotate class labels for better readability if many classes\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Discussion of imbalance\n",
    "        mean_count = np.mean(counts)\n",
    "        std_dev_count = np.std(counts)\n",
    "        print(f\"\\nMean number of images per class: {mean_count:.2f}\")\n",
    "        print(f\"Standard deviation of images per class: {std_dev_count:.2f}\")\n",
    "        if std_dev_count > mean_count * 0.5: # Arbitrary threshold for 'significant' imbalance\n",
    "            print(\"There appears to be a notable class imbalance. This might affect model performance.\")\n",
    "            print(\"Consider techniques like weighted sampling, oversampling minority classes, or undersampling majority classes if performance is skewed.\")\n",
    "        else:\n",
    "            print(\"Class distribution appears relatively balanced.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform class distribution analysis: {e}\")\n",
    "else:\n",
    "    print(\"Skipping dataset analysis as 'data_root_example' is not valid, or the pipeline was not initialized.\")\n",
    "    print(\"Please ensure 'data_root_example' points to a valid dataset to see the class distribution analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
