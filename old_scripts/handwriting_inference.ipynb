{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ex6pQh3WcjGN",
        "outputId": "f42f77f9-f315-4391-a7c3-9396d5fda962"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdKb9M3lQKT4",
        "outputId": "37aa28b7-f42c-4b14-fdfd-da4b55bbcbd6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.models as models\n",
        "import matplotlib.patches as patches\n",
        "import gradio as gr\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxs3oPamsSOh",
        "outputId": "815bd51b-199c-47bc-98e2-bde55656ff15"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S976ss4vqZpG"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'username'\n",
        "os.environ['KAGGLE_KEY'] = 'key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rhIos0OBYBaz",
        "outputId": "b4cbdf00-3471-4072-d39b-becd725dae5f"
      },
      "outputs": [],
      "source": [
        "# RUN ON NEW ENVIRONMENT ONLY!!!\n",
        "!kaggle datasets download sujaymann/handwritten-english-characters-and-digits\n",
        "!unzip handwritten-english-characters-and-digits.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iD6NFxXaGAc"
      },
      "outputs": [],
      "source": [
        "class VGG19HandwritingModel(nn.Module):\n",
        "    def __init__(self, num_classes, device, pretrained=True):\n",
        "        super(VGG19HandwritingModel, self).__init__()\n",
        "\n",
        "        # Load pretrained VGG19 and move to device\n",
        "        vgg19 = models.vgg19(weights=('DEFAULT' if pretrained else None))\n",
        "        vgg19 = vgg19.to(device)\n",
        "\n",
        "        # Modify first layer to accept grayscale images\n",
        "        self.features = vgg19.features\n",
        "        self.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1).to(device)\n",
        "\n",
        "        # Custom classifier for our task\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 2 * 2, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(512, num_classes)\n",
        "        ).to(device)\n",
        "\n",
        "        # Initialize weights for the new layers\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJy9Nungj5G3"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image_path):\n",
        "    \"\"\"Prepare image for classification.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image)\n",
        "    return image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def get_class_labels(data_root):\n",
        "    \"\"\"Get class labels from the data directory.\"\"\"\n",
        "    class_names = sorted(os.listdir(data_root))\n",
        "    # Remove any hidden files (like .DS_Store)\n",
        "    class_names = [c for c in class_names if not c.startswith('.')]\n",
        "    return class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KebxSaMbjpBs",
        "outputId": "34955510-b0c6-403b-9a7b-1e82599c2c72"
      },
      "outputs": [],
      "source": [
        "# Get class labels and number of classes\n",
        "class_names = get_class_labels('/content/handwritten-english-characters-and-digits/combined_folder/train')\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1QbxsLb8cmem",
        "outputId": "1dadcd0f-19ea-4647-af39-ad28f5714112"
      },
      "outputs": [],
      "source": [
        "def load_model_inference(model, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # Optionally load other info if needed\n",
        "    epoch = checkpoint['epoch']\n",
        "    val_acc = checkpoint['val_acc']\n",
        "    print(f\"Loaded model from epoch {epoch+1} with validation accuracy: {val_acc:.2f}%\")\n",
        "    return model\n",
        "\n",
        "model = VGG19HandwritingModel(num_classes, \"cpu\")\n",
        "# model = load_model_inference(model, \"/content/drive/MyDrive/model_checkpoints_vgg/best_model.pth\")\n",
        "model = load_model_inference(model, \"/content/drive/MyDrive/model_checkpoints_vgg/best_model_vgg_full_train.pth\")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok8h8U9geO1y"
      },
      "outputs": [],
      "source": [
        "def visualize_letter_extraction(image_path, letters_info):\n",
        "    \"\"\"\n",
        "    Visualize the extracted letters and their bounding boxes\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the original image\n",
        "        letters_info (list): List of tuples containing letter and bounding box\n",
        "    \"\"\"\n",
        "    # Read the original image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Create a copy for visualization\n",
        "    visual = image.copy()\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for letter, bbox in letters_info:\n",
        "        x, y, w, h = bbox\n",
        "        cv2.rectangle(visual, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(visual, letter, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "    # Convert to RGB for displaying with matplotlib\n",
        "    visual_rgb = cv2.cvtColor(visual, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create a figure to display the original and processed images\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Display original image\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display processed image with bounding boxes\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.imshow(visual_rgb)\n",
        "    plt.title('Detected Letters')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create a grid of extracted letters\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.suptitle('Extracted Individual Letters', fontsize=16)\n",
        "\n",
        "    num_letters = len(letters_info)\n",
        "    cols = min(10, num_letters)\n",
        "    rows = (num_letters + cols - 1) // cols\n",
        "\n",
        "    for i, (letter, bbox) in enumerate(letters_info):\n",
        "        x, y, w, h = bbox\n",
        "        letter_img = image[y:y+h, x:x+w]\n",
        "        letter_img_rgb = cv2.cvtColor(letter_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(letter_img_rgb)\n",
        "        plt.title(f\"{letter}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfllKqJ8fPax"
      },
      "outputs": [],
      "source": [
        "def extract_letters(image_path, model, class_list, output_dir=\"extracted_letters\", spacing=0):\n",
        "    \"\"\"\n",
        "    Extract individual letters from a handwritten text image and classify them.\n",
        "    Uses two different thresholding methods:\n",
        "    1. Inverted OTSU thresholding for contour detection\n",
        "    2. Non-inverted adaptive thresholding for model input images\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Read and process the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
        "\n",
        "    # Visualize original image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.savefig(f\"{output_dir}/1_original.png\")\n",
        "    plt.show()\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Visualize grayscale image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(gray, cmap='gray')\n",
        "    plt.title(\"Grayscale Image\")\n",
        "    plt.savefig(f\"{output_dir}/2_grayscale.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Apply OTSU thresholding (inverted) for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    # Visualize binary image for contour detection\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(binary_contours, cmap='gray')\n",
        "    plt.title(\"Binary Image for Contours (Inverted OTSU)\")\n",
        "    plt.savefig(f\"{output_dir}/3a_binary_contours.png\")\n",
        "    plt.show()\n",
        "\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # binary_model = cv2.adaptiveThreshold(\n",
        "    #     gray,\n",
        "    #     255,\n",
        "    #     cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "    #     cv2.THRESH_BINARY,\n",
        "    #     11,  # Block size\n",
        "    #     2    # C constant (subtracted from mean)\n",
        "    # )\n",
        "    # Visualize binary image for model input\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(binary_model, cmap='gray')\n",
        "    plt.title(\"Binary Image for Model Input (Non-inverted Adaptive)\")\n",
        "    plt.savefig(f\"{output_dir}/3b_binary_model.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Find contours on the inverted binary image (for contour detection)\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Visualize contours\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"All Contours Found: {len(contours)}\")\n",
        "    plt.savefig(f\"{output_dir}/4_all_contours.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    print(f\"Found {len(contours)} total contours, {len(letter_contours)} after filtering by min area {min_area}\")\n",
        "\n",
        "    # Visualize filtered contours\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Filtered Contours: {len(letter_contours)}\")\n",
        "    plt.savefig(f\"{output_dir}/5_filtered_contours.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Sort contours from left to right (for reading order)\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Create a figure to show all extracted letters\n",
        "    num_letters = len(letter_contours)\n",
        "    fig_rows = max(1, (num_letters + 4) // 5)  # 5 letters per row\n",
        "    fig_cols = min(5, num_letters)\n",
        "\n",
        "    plt.figure(figsize=(15, 3 * fig_rows))\n",
        "    plt.suptitle(\"Extracted Letters Before Processing\", fontsize=16)\n",
        "\n",
        "    letters_info = []\n",
        "    model_input_images = []  # Store images that will be fed to the model\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract letter from the ADAPTIVE THRESHOLDED binary image for model input\n",
        "        # This uses the non-inverted adaptive thresholding for cleaner results\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "\n",
        "        # Save original extracted letter\n",
        "        letter_path = os.path.join(output_dir, f\"letter_{i}_original.png\")\n",
        "        cv2.imwrite(letter_path, letter_image)\n",
        "\n",
        "        # Add to the visualization figure\n",
        "        if i < fig_rows * fig_cols:\n",
        "            plt.subplot(fig_rows, fig_cols, i + 1)\n",
        "            plt.imshow(letter_image, cmap='gray')\n",
        "            plt.title(f\"Letter {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Fix the second issue: Maintain aspect ratio during resize\n",
        "        # Calculate target size while maintaining aspect ratio\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        # Determine which dimension is larger\n",
        "        if w > h:\n",
        "            # Width is the dominant dimension\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            # Height is the dominant dimension\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image (255 is white for non-inverted binary)\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Save the padded image\n",
        "        padded_image.save(os.path.join(output_dir, f\"letter_{i}_padded.png\"))\n",
        "\n",
        "        # Create the final 64x64 image with the specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Save the tensor representation\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        # Denormalize for visualization (undo normalization)\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)  # Just take the first channel\n",
        "\n",
        "        # Save the final processed image that will be fed to the model\n",
        "        cv2.imwrite(os.path.join(output_dir, f\"letter_{i}_model_input.png\"), transformed_image)\n",
        "        model_input_images.append(transformed_image)\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "            confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "            print(f\"Letter {i}: Classified as '{detected_char}' with confidence {confidence:.2f}\")\n",
        "\n",
        "        letters_info.append((detected_char, (x, y, w, h)))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Make room for the suptitle\n",
        "    plt.savefig(f\"{output_dir}/6_extracted_letters.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize the final processed images that are fed to the model\n",
        "    plt.figure(figsize=(15, 3 * fig_rows))\n",
        "    plt.suptitle(\"Processed Images Fed to Model (64x64 with spacing)\", fontsize=16)\n",
        "\n",
        "    for i, img in enumerate(model_input_images):\n",
        "        if i < fig_rows * fig_cols:\n",
        "            ax = plt.subplot(fig_rows, fig_cols, i + 1)\n",
        "\n",
        "            # Display the grayscale image\n",
        "            plt.imshow(img, cmap='gray')\n",
        "\n",
        "            # Add a red border around the full 64x64 image\n",
        "            rect_outer = patches.Rectangle((0, 0), 63, 63, linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect_outer)\n",
        "\n",
        "            # Add a blue border around the inner area\n",
        "            inner_size = 64 - spacing - 1\n",
        "            spacing_half = spacing // 2\n",
        "            rect_inner = patches.Rectangle((spacing_half, spacing_half), inner_size, inner_size,\n",
        "                                          linewidth=1, edgecolor='b', facecolor='none')\n",
        "            ax.add_patch(rect_inner)\n",
        "\n",
        "            plt.title(f\"Letter {i}: {letters_info[i][0]}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(f\"{output_dir}/7_model_inputs_with_border.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Draw bounding boxes on original image\n",
        "    result_img = image.copy()\n",
        "    for i, (char, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, char, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Final Result with Classifications\")\n",
        "    plt.savefig(f\"{output_dir}/8_final_result.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return letters_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qb7LSxfafPWa",
        "outputId": "f5864e6b-8f29-473a-f234-458d7e4e06f6"
      },
      "outputs": [],
      "source": [
        "def demo():\n",
        "    \"\"\"\n",
        "    Demonstrate the letter extraction on a sample image\n",
        "    \"\"\"\n",
        "    # Extract letters\n",
        "    print(\"Extracting letters...\")\n",
        "\n",
        "    image_path = \"/content/drive/MyDrive/sample_letters/here_we_go.jpg\"\n",
        "    image_path = \"/content/drive/MyDrive/sample_letters/20250331_193146.jpg\"\n",
        "    # image_path = \"/content/drive/MyDrive/sample_letters/20250331_230539.jpg\"\n",
        "\n",
        "    letters_info = extract_letters(image_path, model, class_names, spacing=24)\n",
        "\n",
        "    # Print detected letters\n",
        "    print(\"Detected letters:\")\n",
        "    for letter, bbox in letters_info:\n",
        "        print(f\"Letter: {letter}, Bounding box: {bbox}\")\n",
        "\n",
        "    # If you have a separate visualization function\n",
        "    # visualize_letter_extraction(image_path, letters_info)\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ3xxyRQduOS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_drawn_image(image, model, class_list, spacing=24):\n",
        "    \"\"\"\n",
        "    Process a drawn image from Gradio and extract/classify letters\n",
        "    \"\"\"\n",
        "    output_dir = \"extracted_letters\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Convert gradio image (RGB numpy array) to BGR for OpenCV compatibility\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Create a list to store all the visualization images\n",
        "    visualization_images = []\n",
        "\n",
        "    # 1. Original Image\n",
        "    orig_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Original Image\", orig_img))\n",
        "\n",
        "    # 2. Grayscale conversion\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    visualization_images.append((\"Grayscale Image\", gray))\n",
        "\n",
        "    # 3a. Binary image for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Contours (Inverted OTSU)\", binary_contours))\n",
        "\n",
        "    # 3b. Binary image for model input\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Model Input\", binary_model))\n",
        "\n",
        "    # 4. Find contours\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"All Contours Found: {len(contours)}\", contour_img_rgb))\n",
        "\n",
        "    # 5. Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    filtered_contour_img_rgb = cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"Filtered Contours: {len(letter_contours)}\", filtered_contour_img_rgb))\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Process each letter\n",
        "    letters_info = []\n",
        "    letter_images = []\n",
        "    model_input_images = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract letter from binary image\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "        letter_images.append((f\"Letter {i}\", letter_image))\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Maintain aspect ratio during resize\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        if w > h:\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Create the final 64x64 image with specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Save the processed image for visualization\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)\n",
        "        model_input_images.append((f\"Letter {i} Model Input\", transformed_image))\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "            confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "\n",
        "        letters_info.append((detected_char, confidence, (x, y, w, h)))\n",
        "\n",
        "    # Create final result image with classifications\n",
        "    result_img = image.copy()\n",
        "    detected_text = \"\"\n",
        "\n",
        "    for i, (char, confidence, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, f\"{char} ({confidence:.2f})\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        detected_text += char\n",
        "\n",
        "    result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Final Result with Classifications\", result_img_rgb))\n",
        "\n",
        "    # Combine all letter images and model inputs into group images for better display\n",
        "    letter_grid = create_image_grid([img for _, img in letter_images], f\"Extracted Letters ({len(letter_images)})\")\n",
        "    model_input_grid = create_image_grid([img for _, img in model_input_images],\n",
        "                                         f\"Model Input Images ({len(model_input_images)})\")\n",
        "\n",
        "    if letter_grid is not None:\n",
        "        visualization_images.append((\"Extracted Letters\", letter_grid))\n",
        "\n",
        "    if model_input_grid is not None:\n",
        "        visualization_images.append((\"Model Input Images\", model_input_grid))\n",
        "\n",
        "    return visualization_images, detected_text\n",
        "\n",
        "def create_image_grid(images, title):\n",
        "    \"\"\"Create a grid of images for better visualization\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    num_images = len(images)\n",
        "    cols = min(5, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 2 * rows))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Add images to the grid\n",
        "    for i, img in enumerate(images):\n",
        "        if i < rows * cols:\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Image {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Convert matplotlib figure to image - UPDATED METHOD\n",
        "    fig = plt.gcf()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    # Use the newer method to get the RGB buffer\n",
        "    w, h = fig.canvas.get_width_height()\n",
        "    # For newer matplotlib versions:\n",
        "    try:\n",
        "        buf = fig.canvas.buffer_rgba()\n",
        "        grid_image = np.asarray(buf)\n",
        "    except AttributeError:\n",
        "        # Alternative approach if buffer_rgba is not available\n",
        "        from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "        canvas = FigureCanvasAgg(fig)\n",
        "        canvas.draw()\n",
        "        buf = np.asarray(canvas.buffer_rgba())\n",
        "        grid_image = buf\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    # Convert RGBA to RGB if needed\n",
        "    if grid_image.shape[2] == 4:\n",
        "        grid_image = grid_image[:, :, :3]\n",
        "\n",
        "    return grid_image\n",
        "\n",
        "def gradio_interface(model, class_list):\n",
        "    \"\"\"Create a Gradio interface for the letter extraction and classification\"\"\"\n",
        "\n",
        "    def process_image(image):\n",
        "        if image is None:\n",
        "            return [(\"No Image Provided\", np.zeros((100, 100, 3), dtype=np.uint8))], \"No image provided\"\n",
        "\n",
        "        # Process the drawn image\n",
        "        visualization_images, detected_text = process_drawn_image(image, model, class_list)\n",
        "\n",
        "        # Return only the images as a list for the gallery\n",
        "        return [img for _, img in visualization_images], detected_text\n",
        "\n",
        "    # Define function to handle the sketchpad output\n",
        "    def process_sketch(sketch):\n",
        "        # Convert the black and white sketch to a proper image\n",
        "        if sketch is None:\n",
        "            return [], \"No image drawn\"\n",
        "\n",
        "        # Extract the image data from the sketch dictionary\n",
        "        if isinstance(sketch, dict):\n",
        "            if \"composite\" in sketch:\n",
        "                sketch = sketch[\"composite\"]\n",
        "            elif \"layers\" in sketch and isinstance(sketch[\"layers\"], list) and len(sketch[\"layers\"]) > 0:\n",
        "                sketch = np.array(sketch[\"layers\"][0])\n",
        "            else:\n",
        "                return [], \"Error: Unable to process sketch format\"\n",
        "\n",
        "        # Save the original sketch to inspect it\n",
        "        import cv2\n",
        "        import os\n",
        "\n",
        "        # Create debug directory if it doesn't exist\n",
        "        os.makedirs(\"debug\", exist_ok=True)\n",
        "\n",
        "        # Save the original sketch\n",
        "        if len(sketch.shape) == 3 and sketch.shape[2] == 4:\n",
        "            # For RGBA, save just the RGB channels\n",
        "            cv2.imwrite(\"debug/original_sketch_rgb.png\", cv2.cvtColor(sketch[:, :, :3], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            # Also separately save each channel to see what they contain\n",
        "            for i, channel_name in enumerate(['R', 'G', 'B', 'A']):\n",
        "                cv2.imwrite(f\"debug/original_sketch_channel_{channel_name}.png\", sketch[:, :, i])\n",
        "        else:\n",
        "            cv2.imwrite(\"debug/original_sketch.png\", cv2.cvtColor(sketch, cv2.COLOR_RGB2BGR) if len(sketch.shape) == 3 else sketch)\n",
        "\n",
        "        # Check if we have an RGBA image (4 channels)\n",
        "        if len(sketch.shape) == 3 and sketch.shape[2] == 4:\n",
        "            # Try extracting just the alpha channel, as it might contain the drawing\n",
        "            alpha_channel = sketch[:, :, 3]\n",
        "            cv2.imwrite(\"debug/alpha_channel.png\", alpha_channel)\n",
        "\n",
        "            # Check if alpha channel has useful information\n",
        "            if alpha_channel.max() > 0:\n",
        "                # If alpha has data, use it as the sketch\n",
        "                sketch = np.stack([alpha_channel, alpha_channel, alpha_channel], axis=2)\n",
        "            else:\n",
        "                # Otherwise use RGB channels\n",
        "                sketch = sketch[:, :, :3]\n",
        "\n",
        "        # Check if we need to convert grayscale to RGB\n",
        "        if len(sketch.shape) == 2:\n",
        "            sketch = np.stack([sketch, sketch, sketch], axis=2)\n",
        "\n",
        "        # Try both the original and inverted image\n",
        "        sketch_original = sketch.copy()\n",
        "        sketch_inverted = 255 - sketch.copy()\n",
        "\n",
        "        # Save both versions for inspection\n",
        "        cv2.imwrite(\"debug/sketch_for_processing.png\", cv2.cvtColor(sketch, cv2.COLOR_RGB2BGR))\n",
        "        cv2.imwrite(\"debug/sketch_inverted.png\", cv2.cvtColor(sketch_inverted, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Print histogram data to understand pixel distribution\n",
        "        print(\"Original sketch histogram:\", np.histogram(sketch, bins=10, range=(0, 255))[0])\n",
        "        print(\"Inverted sketch histogram:\", np.histogram(sketch_inverted, bins=10, range=(0, 255))[0])\n",
        "\n",
        "        # Let's try processing both the original and inverted image\n",
        "        results_original = process_image(sketch_original)\n",
        "        results_inverted = process_image(sketch_inverted)\n",
        "\n",
        "        # Check which result gives better output (more contours found)\n",
        "        # Compare the number of visualizations - more visualizations likely means better processing\n",
        "        if len(results_original[0]) > len(results_inverted[0]):\n",
        "            print(\"Using original sketch\")\n",
        "            return results_original\n",
        "        else:\n",
        "            print(\"Using inverted sketch\")\n",
        "            return results_inverted\n",
        "\n",
        "    # Create the interface with the compatible components\n",
        "    with gr.Blocks(title=\"Handwritten Letter Recognition\") as demo:\n",
        "        gr.Markdown(\"# Handwritten Letter Recognition\")\n",
        "        gr.Markdown(\"Draw letters or a sentence using your mouse, then click 'Process' to recognize the text.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # For older Gradio versions, use Sketchpad instead of Image with source=\"canvas\"\n",
        "                input_sketch = gr.Sketchpad(label=\"Draw Text Here\", height = 400, width = 600)\n",
        "\n",
        "                process_btn = gr.Button(\"Process\")\n",
        "                clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                output_text = gr.Textbox(label=\"Recognized Text\")\n",
        "                output_gallery = gr.Gallery(\n",
        "                    label=\"Processing Steps\",\n",
        "                    show_label=True,\n",
        "                    columns=2,\n",
        "                    rows=4,\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "\n",
        "        process_btn.click(fn=process_sketch, inputs=input_sketch, outputs=[output_gallery, output_text])\n",
        "        clear_btn.click(fn=lambda: None, inputs=None, outputs=input_sketch)\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## How It Works\n",
        "        1. Draw text using your mouse in the canvas\n",
        "        2. Click 'Process' to analyze the image\n",
        "        3. The system will:\n",
        "           - Convert the image to binary format\n",
        "           - Find letter contours\n",
        "           - Extract each letter\n",
        "           - Process for classification\n",
        "           - Predict the character for each letter\n",
        "        4. View all processing steps in the gallery\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "def launch_interface(model, class_list):\n",
        "    \"\"\"Launch the Gradio interface\"\"\"\n",
        "    interface = gradio_interface(model, class_list)\n",
        "    interface.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAG0B-aruTPt",
        "outputId": "f071a949-7453-46b6-928f-d7da0952a16a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming model and class_names are already defined in your code\n",
        "    launch_interface(model, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mNQZ7zildOo"
      },
      "outputs": [],
      "source": [
        "def process_drawn_image(image, model, class_list, spacing=24):\n",
        "    \"\"\"\n",
        "    Process a drawn image from Gradio and extract/classify letters\n",
        "    \"\"\"\n",
        "    output_dir = \"extracted_letters\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Convert gradio image (RGB numpy array) to BGR for OpenCV compatibility\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Create a list to store all the visualization images\n",
        "    visualization_images = []\n",
        "\n",
        "    # 1. Original Image\n",
        "    orig_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Original Image\", orig_img))\n",
        "\n",
        "    # 2. Grayscale conversion\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    visualization_images.append((\"Grayscale Image\", gray))\n",
        "\n",
        "    # 3a. Binary image for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Contours (Inverted OTSU)\", binary_contours))\n",
        "\n",
        "    # 3b. Binary image for model input\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Model Input\", binary_model))\n",
        "\n",
        "    # 4. Find contours\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"All Contours Found: {len(contours)}\", contour_img_rgb))\n",
        "\n",
        "    # 5. Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    filtered_contour_img_rgb = cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"Filtered Contours: {len(letter_contours)}\", filtered_contour_img_rgb))\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Process each letter\n",
        "    letters_info = []\n",
        "    letter_images = []\n",
        "    model_input_images = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract letter from binary image\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "        letter_images.append((f\"Letter {i}\", letter_image))\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Maintain aspect ratio during resize\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        if w > h:\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Create the final 64x64 image with specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Save the processed image for visualization\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)\n",
        "        model_input_images.append((f\"Letter {i} Model Input\", transformed_image))\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "            confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "\n",
        "        letters_info.append((detected_char, confidence, (x, y, w, h)))\n",
        "\n",
        "    # Create final result image with classifications\n",
        "    result_img = image.copy()\n",
        "    detected_text = \"\"\n",
        "\n",
        "    for i, (char, confidence, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, f\"{char} ({confidence:.2f})\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        detected_text += char\n",
        "\n",
        "    result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Final Result with Classifications\", result_img_rgb))\n",
        "\n",
        "    # Combine all letter images and model inputs into group images for better display\n",
        "    letter_grid = create_image_grid([img for _, img in letter_images], f\"Extracted Letters ({len(letter_images)})\")\n",
        "    model_input_grid = create_image_grid([img for _, img in model_input_images],\n",
        "                                         f\"Model Input Images ({len(model_input_images)})\")\n",
        "\n",
        "    if letter_grid is not None:\n",
        "        visualization_images.append((\"Extracted Letters\", letter_grid))\n",
        "\n",
        "    if model_input_grid is not None:\n",
        "        visualization_images.append((\"Model Input Images\", model_input_grid))\n",
        "\n",
        "    return visualization_images, detected_text\n",
        "\n",
        "def create_image_grid(images, title):\n",
        "    \"\"\"Create a grid of images for better visualization\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    num_images = len(images)\n",
        "    cols = min(5, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 2 * rows))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Add images to the grid\n",
        "    for i, img in enumerate(images):\n",
        "        if i < rows * cols:\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Image {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Convert matplotlib figure to image\n",
        "    fig = plt.gcf()\n",
        "    fig.canvas.draw()\n",
        "    grid_image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    grid_image = grid_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.close()\n",
        "\n",
        "    return grid_image\n",
        "\n",
        "def launch_interface(model, class_list):\n",
        "    \"\"\"Create the most basic compatible interface for older Gradio versions\"\"\"\n",
        "\n",
        "    def process_sketch(sketch):\n",
        "        # Convert the sketch to a proper image\n",
        "        if sketch is None:\n",
        "            return \"No image drawn\", None\n",
        "\n",
        "        # Make sure sketch is 3-channel RGB\n",
        "        if len(sketch.shape) == 2:\n",
        "            sketch = np.stack([sketch, sketch, sketch], axis=2)\n",
        "\n",
        "        # Process the drawn image\n",
        "        visualization_images, detected_text = process_drawn_image(sketch, model, class_list)\n",
        "\n",
        "        # Combine all images into a single display\n",
        "        combined_img = create_combined_image(visualization_images)\n",
        "\n",
        "        return detected_text, combined_img\n",
        "\n",
        "    def create_combined_image(visualization_images):\n",
        "        \"\"\"Create a single image with all visualization steps\"\"\"\n",
        "        # Determine a reasonable layout\n",
        "        num_images = len(visualization_images)\n",
        "        cols = min(3, num_images)\n",
        "        rows = (num_images + cols - 1) // cols\n",
        "\n",
        "        plt.figure(figsize=(15, 5 * rows))\n",
        "\n",
        "        for i, (title, img) in enumerate(visualization_images):\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "            # Handle grayscale images\n",
        "            if len(img.shape) == 2:\n",
        "                plt.imshow(img, cmap='gray')\n",
        "            else:\n",
        "                plt.imshow(img)\n",
        "\n",
        "            plt.title(title)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert matplotlib figure to image\n",
        "        fig = plt.gcf()\n",
        "        fig.canvas.draw()\n",
        "        combined_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        combined_img = combined_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close()\n",
        "\n",
        "        return combined_img\n",
        "\n",
        "    # Create a very basic interface that should work with any Gradio version\n",
        "    interface = gr.Interface(\n",
        "        fn=process_sketch,\n",
        "        inputs=gr.Sketchpad(),\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Recognized Text\"),\n",
        "            gr.Image(label=\"Processing Steps\")\n",
        "        ],\n",
        "        title=\"Handwritten Letter Recognition\",\n",
        "        description=\"Draw letters or a sentence using your mouse, then click submit to recognize the text.\"\n",
        "    )\n",
        "\n",
        "    interface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "1N7gBVRnuX2U",
        "outputId": "a4515704-9739-4e07-c5c7-63e9b543e06a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming model and class_names are already defined in your code\n",
        "    launch_interface(model, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DexXBMfA7ufj",
        "outputId": "53d3a3d8-0b71-48df-cb94-8d1275f66286"
      },
      "outputs": [],
      "source": [
        "print(gr.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNQ5WjFKp4MQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_uploaded_image(image, model, class_list, spacing=24):\n",
        "    \"\"\"\n",
        "    Process an uploaded image and extract/classify letters\n",
        "    \"\"\"\n",
        "    # Check if image is None\n",
        "    if image is None:\n",
        "        return [], \"No image provided\"\n",
        "\n",
        "    output_dir = \"extracted_letters\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Make sure image is in the right format for processing\n",
        "    # Convert PIL image to numpy if needed\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        image = np.array(image)\n",
        "\n",
        "    # Make sure image is in RGB format\n",
        "    if len(image.shape) == 2:  # If grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    elif len(image.shape) == 3 and image.shape[2] == 4:  # If RGBA\n",
        "        image = image[:, :, :3]  # Drop alpha channel\n",
        "\n",
        "    # Convert to BGR for OpenCV functions\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Create a list to store all the visualization images\n",
        "    visualization_images = []\n",
        "\n",
        "    # 1. Original Image\n",
        "    orig_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Original Image\", orig_img))\n",
        "\n",
        "    # 2. Grayscale conversion\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    visualization_images.append((\"Grayscale Image\", gray))\n",
        "\n",
        "    # 3a. Binary image for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Contours (Inverted OTSU)\", binary_contours))\n",
        "\n",
        "    # 3b. Binary image for model input\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Model Input\", binary_model))\n",
        "\n",
        "    # 4. Find contours\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"All Contours Found: {len(contours)}\", contour_img_rgb))\n",
        "\n",
        "    # Check if we found any contours\n",
        "    if len(contours) == 0:\n",
        "        return visualization_images, \"No letters detected\"\n",
        "\n",
        "    # 5. Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    filtered_contour_img_rgb = cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"Filtered Contours: {len(letter_contours)}\", filtered_contour_img_rgb))\n",
        "\n",
        "    # Check if we have any letters after filtering\n",
        "    if len(letter_contours) == 0:\n",
        "        return visualization_images, \"No valid letters detected after filtering\"\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Process each letter\n",
        "    letters_info = []\n",
        "    letter_images = []\n",
        "    model_input_images = []\n",
        "\n",
        "    # Ensure model is available\n",
        "    if model is None:\n",
        "        return visualization_images, \"Model not loaded\"\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Ensure device compatibility\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Skip extremely small contours that passed the area filter\n",
        "        if w < 5 or h < 5:\n",
        "            continue\n",
        "\n",
        "        # Extract letter from binary image\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "        letter_images.append((f\"Letter {i}\", letter_image))\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Maintain aspect ratio during resize\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        if w > h:\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Create the final 64x64 image with specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "        letter_tensor = letter_tensor.to(device)\n",
        "\n",
        "        # Save the processed image for visualization\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)\n",
        "        model_input_images.append((f\"Letter {i} Model Input\", transformed_image))\n",
        "\n",
        "        try:\n",
        "            # Classify the letter using the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(letter_tensor)\n",
        "                predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "                detected_char = class_list[predicted_idx]\n",
        "                confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "\n",
        "            letters_info.append((detected_char, confidence, (x, y, w, h)))\n",
        "        except Exception as e:\n",
        "            print(f\"Error classifying letter {i}: {e}\")\n",
        "            # Continue with next letter instead of failing completely\n",
        "            continue\n",
        "\n",
        "    # Create final result image with classifications\n",
        "    result_img = image.copy()\n",
        "    detected_text = \"\"\n",
        "\n",
        "    for i, (char, confidence, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, f\"{char} ({confidence:.2f})\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        detected_text += char\n",
        "\n",
        "    result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Final Result with Classifications\", result_img_rgb))\n",
        "\n",
        "    # Combine all letter images and model inputs into group images for better display\n",
        "    if letter_images:\n",
        "        letter_grid = create_image_grid([img for _, img in letter_images], f\"Extracted Letters ({len(letter_images)})\")\n",
        "        if letter_grid is not None:\n",
        "            visualization_images.append((\"Extracted Letters\", letter_grid))\n",
        "\n",
        "    if model_input_images:\n",
        "        model_input_grid = create_image_grid([img for _, img in model_input_images],\n",
        "                                            f\"Model Input Images ({len(model_input_images)})\")\n",
        "        if model_input_grid is not None:\n",
        "            visualization_images.append((\"Model Input Images\", model_input_grid))\n",
        "\n",
        "    return visualization_images, detected_text\n",
        "\n",
        "def create_image_grid(images, title):\n",
        "    \"\"\"Create a grid of images for better visualization\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    num_images = len(images)\n",
        "    cols = min(5, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 2 * rows))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Add images to the grid\n",
        "    for i, img in enumerate(images):\n",
        "        if i < rows * cols:\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Image {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    try:\n",
        "        # Convert matplotlib figure to image\n",
        "        fig = plt.gcf()\n",
        "        fig.canvas.draw()\n",
        "        grid_image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        grid_image = grid_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close()\n",
        "        return grid_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating image grid: {e}\")\n",
        "        plt.close()\n",
        "        return None\n",
        "\n",
        "def create_combined_image(visualization_images):\n",
        "    \"\"\"Create a single image with all visualization steps\"\"\"\n",
        "    # Check if we have any images\n",
        "    if not visualization_images:\n",
        "        # Return empty image\n",
        "        empty_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "        cv2.putText(empty_img, \"No visualization available\", (50, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
        "        return empty_img\n",
        "\n",
        "    # Determine a reasonable layout\n",
        "    num_images = len(visualization_images)\n",
        "    cols = min(3, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 5 * rows))\n",
        "\n",
        "        for i, (title, img) in enumerate(visualization_images):\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "            # Handle grayscale images\n",
        "            if len(img.shape) == 2:\n",
        "                plt.imshow(img, cmap='gray')\n",
        "            else:\n",
        "                plt.imshow(img)\n",
        "\n",
        "            plt.title(title)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert matplotlib figure to image\n",
        "        fig = plt.gcf()\n",
        "        fig.canvas.draw()\n",
        "        combined_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        combined_img = combined_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close()\n",
        "\n",
        "        return combined_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating combined image: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "        # Return a simple error image instead\n",
        "        error_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "        cv2.putText(error_img, f\"Error: {str(e)}\", (50, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        return error_img\n",
        "\n",
        "def process_image(image, model, class_list):\n",
        "    \"\"\"Process the uploaded image and handle potential errors\"\"\"\n",
        "    try:\n",
        "        if image is None:\n",
        "            error_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "            cv2.putText(error_img, \"No image uploaded\", (50, 150),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
        "            return \"No image uploaded\", error_img\n",
        "\n",
        "        # Process the uploaded image\n",
        "        visualization_images, detected_text = process_uploaded_image(image, model, class_list)\n",
        "\n",
        "        # Combine all images into a single display\n",
        "        combined_img = create_combined_image(visualization_images)\n",
        "\n",
        "        return detected_text, combined_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in process_image: {e}\")\n",
        "\n",
        "        # Create error image\n",
        "        error_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "        cv2.putText(error_img, f\"Error: {str(e)}\", (50, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        return f\"Error: {str(e)}\", error_img\n",
        "\n",
        "def launch_interface(model, class_list):\n",
        "    \"\"\"Create a robust interface with image upload that handles errors gracefully\"\"\"\n",
        "\n",
        "    # Create a function that pre-binds the model and class_list\n",
        "    def process_image_bound(image):\n",
        "        return process_image(image, model, class_list)\n",
        "\n",
        "    # Create a user-friendly interface with image upload\n",
        "    with gr.Blocks() as interface:\n",
        "        gr.Markdown(\"# Handwritten Letter Recognition\")\n",
        "        gr.Markdown(\"Upload an image with handwritten letters to recognize the text.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload Handwritten Image\")\n",
        "                submit_btn = gr.Button(\"Recognize Text\")\n",
        "\n",
        "                # Add example images if you have any\n",
        "                example_folder = \"example_images\"\n",
        "                if os.path.exists(example_folder):\n",
        "                    example_images = [os.path.join(example_folder, f) for f in os.listdir(example_folder)\n",
        "                                     if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "                    if example_images:\n",
        "                        gr.Examples(examples=example_images, inputs=image_input)\n",
        "\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Recognized Text\")\n",
        "                image_output = gr.Image(label=\"Processing Steps\")\n",
        "\n",
        "        # Add preprocessing options\n",
        "        with gr.Accordion(\"Advanced Options\", open=False):\n",
        "            gr.Markdown(\"These options will be implemented in a future version\")\n",
        "            # Placeholder for future enhancements\n",
        "            # resize_slider = gr.Slider(minimum=0.1, maximum=2.0, value=1.0, step=0.1, label=\"Resize Factor\")\n",
        "            # invert_checkbox = gr.Checkbox(label=\"Invert Image\", value=False)\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=process_image_bound,\n",
        "            inputs=image_input,\n",
        "            outputs=[text_output, image_output]\n",
        "        )\n",
        "\n",
        "        # Also process when the image is uploaded\n",
        "        image_input.change(\n",
        "            fn=process_image_bound,\n",
        "            inputs=image_input,\n",
        "            outputs=[text_output, image_output]\n",
        "        )\n",
        "\n",
        "    # Launch the interface\n",
        "    interface.launch(share=True)\n",
        "\n",
        "# Example of how to define class_names if not already defined\n",
        "# class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "#               'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        "#               'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        "#               'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "#               'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
        "#               'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
        "#               'u', 'v', 'w', 'x', 'y', 'z']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "K8cttf12ulQ2",
        "outputId": "6421eaed-c1fb-4feb-e102-5c1762da4776"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure model and class_names are properly defined before launching\n",
        "    # If they're not defined, you'll need code like this:\n",
        "    #\n",
        "    # import torch\n",
        "    # model = YourModelClass(...)  # Initialize your model\n",
        "    # model.load_state_dict(torch.load('your_model_path.pth'))\n",
        "    # class_names = [...]  # Define your class names list\n",
        "\n",
        "    try:\n",
        "        launch_interface(model, class_names)\n",
        "    except NameError as e:\n",
        "        print(f\"Error: {e}. Make sure 'model' and 'class_names' are defined before calling launch_interface.\")\n",
        "        print(\"Example code:\")\n",
        "        print(\"model = YourModelClass(...)\")\n",
        "        print(\"model.load_state_dict(torch.load('your_model_path.pth'))\")\n",
        "        print(\"class_names = ['A', 'B', 'C', ...] # Your list of characters\")\n",
        "        print(\"launch_interface(model, class_names)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
