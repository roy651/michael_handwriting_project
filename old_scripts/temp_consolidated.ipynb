{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition\n",
    "\n",
    "This notebook demonstrates the process of building and training a handwritten character recognition system using PyTorch. It covers data loading and augmentation, defining several Convolutional Neural Network (CNN) models (including custom architectures and transfer learning with VGG19), training these models, and finally, performing inference on single images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Image processing and display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 # OpenCV for image operations\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch essentials\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from pathlib import Path # MERGED FROM handwriting_training.ipynb\n",
    "import albumentations as A # MERGED FROM handwriting_training.ipynb\n",
    "from albumentations.pytorch import ToTensorV2 # MERGED FROM handwriting_training.ipynb\n",
    "import matplotlib.patches as patches # MERGED FROM handwriting_training.ipynb\n",
    "\n",
    "# MERGED FROM handwriting_training.ipynb: Kaggle API setup and dataset download (commented out)\n",
    "# Users should uncomment and run these if they need to download the dataset.\n",
    "# print(\"Kaggle API setup and dataset download cells from handwriting_training.ipynb follow (commented out).\")\n",
    "# print(\"Uncomment these if you need to set up Kaggle and download the dataset.\")\n",
    "\n",
    "# Cell for Google Colab file upload (usually for kaggle.json)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload() # This would prompt for file upload in Colab\n",
    "# os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "# # Assuming 'kaggle.json' is uploaded, move it\n",
    "# if 'kaggle.json' in uploaded:\n",
    "#     os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
    "#     os.chmod('/root/.kaggle/kaggle.json', 600)\n",
    "# else:\n",
    "#     print(\"kaggle.json not found in uploaded files.\")\n",
    "\n",
    "# Cell for setting Kaggle environment variables (alternative to file)\n",
    "# print(\"Setting Kaggle API credentials (replace with your own if using this method).\")\n",
    "# os.environ['KAGGLE_USERNAME'] = 'your_kaggle_username' # Replace with your username\n",
    "# os.environ['KAGGLE_KEY'] = 'your_kaggle_key'       # Replace with your key\n",
    "\n",
    "# Cell for downloading and unzipping the dataset\n",
    "# print(\"Kaggle dataset download and unzip command (ensure Kaggle API is set up).\")\n",
    "# !kaggle datasets download sujaymann/handwritten-english-characters-and-digits -p ./datasets/handwritten-english --unzip\n",
    "# print(\"Dataset download attempted. Check ./datasets/handwritten-english directory.\")\n",
    "\n",
    "# Note to user: The data_root_example variable later in the notebook should point to the actual dataset path,\n",
    "# e.g., './datasets/handwritten-english/augmented_images/augmented_images1' or similar depending on the unzipped structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# In a Jupyter Notebook, multiprocessing start method handling is less critical \n",
    "# than in standalone scripts, especially if not using num_workers > 0 in DataLoader\n",
    "# or if issues arise. If needed, it can be set.\n",
    "# try:\n",
    "#     torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "#     print(\"Set multiprocessing start method to 'spawn'.\")\n",
    "# except RuntimeError as e:\n",
    "#     print(f\"Note: {e}. Multiprocessing start method might have been already set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data Augmentation Transforms\n",
    "\n",
    "The following cells define custom PyTorch transforms used for data augmentation. `RandomChoice` applies one randomly selected transform from a list, and `ThicknessTransform` simulates variations in stroke thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGED FROM handwriting_training.ipynb: RandomChoice (added probability p)\n",
    "class RandomChoice(torch.nn.Module):\n",
    "    \"\"\"Randomly applies one of the given transforms with given probability\"\"\"\n",
    "    def __init__(self, transforms, p=0.5):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            transform = random.choice(self.transforms)\n",
    "            return transform(img)\n",
    "        return img\n",
    "\n",
    "# NOTE: handwriting_training.ipynb had an alternative ThicknessTransform not as torch.nn.Module.\n",
    "# Kept the version from handwriting_recognition.ipynb as it's already a torch.nn.Module.\n",
    "class ThicknessTransform(torch.nn.Module):\n",
    "    \"\"\"Apply morphological operations to change stroke thickness.\n",
    "    It randomly chooses between dilation (thicker) or erosion (thinner).\n",
    "    Args:\n",
    "        kernel_size (int): Size of the kernel for morphological operations (default: 3).\n",
    "        iterations (int): Number of times to apply the operation (default: 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, iterations=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to OpenCV format (numpy array)\n",
    "        img_cv = np.array(img)\n",
    "        \n",
    "        # Ensure image is grayscale for morphological operations\n",
    "        if len(img_cv.shape) == 3 and img_cv.shape[2] == 3:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2GRAY)\n",
    "        elif len(img_cv.shape) == 3 and img_cv.shape[2] == 1: # Already grayscale but 3-channel\n",
    "             img_cv = img_cv[:, :, 0]\n",
    "        \n",
    "        kernel = np.ones((self.kernel_size, self.kernel_size), np.uint8)\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            # Dilation (thicker)\n",
    "            processed_img = cv2.dilate(img_cv, kernel, iterations=self.iterations)\n",
    "        else:\n",
    "            # Erosion (thinner)\n",
    "            processed_img = cv2.erode(img_cv, kernel, iterations=self.iterations)\n",
    "        \n",
    "        # Convert back to PIL Image\n",
    "        return Image.fromarray(processed_img, mode='L') # 'L' for grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting Data Pipeline\n",
    "\n",
    "The `HandwritingDataPipeline` class manages the loading, augmentation, and splitting of the image dataset. \n",
    "\n",
    "Key features:\n",
    "- **Purpose**: Encapsulates all data handling steps, from reading images from disk to preparing them in batches for training, validation, and testing.\n",
    "- **Parameters**:\n",
    "    - `data_root`: The root directory where the dataset is stored. Images should be organized into subdirectories, where each subdirectory name corresponds to a class label (e.g., 'A', 'B', '0', '1').\n",
    "    - `image_size`: The target size (height, width) to which all images will be resized.\n",
    "    - `batch_size`: The number of images per batch for the DataLoaders.\n",
    "    - `do_transform`: A boolean flag to enable or disable data augmentation. Augmentation is typically enabled for the training set to improve model generalization.\n",
    "- **Data Split**: The pipeline splits the dataset into training, validation, and test sets (typically 70%/15%/15% split). This is crucial for evaluating the model's performance on unseen data and for hyperparameter tuning.\n",
    "- **Normalization**: Image pixel values are normalized (mean and standard deviation) based on ImageNet statistics, which is a common practice, especially when using pretrained models. For grayscale images, the mean and std are often applied across all three channels if the pretrained model expects 3-channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataPipeline:\n",
    "    def __init__(self, data_root, image_size=(64, 64), batch_size=32, do_transform=True, test_split=0.15, val_split=0.15):\n",
    "        self.data_root = data_root\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.do_transform = do_transform\n",
    "        self.test_split = test_split\n",
    "        self.val_split = val_split # Val split is from the remaining data after test_split\n",
    "\n",
    "        # Normalization parameters (ImageNet defaults, suitable for transfer learning)\n",
    "        # For grayscale, these are often applied as (mean, mean, mean) and (std, std, std)\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self._setup_transforms()\n",
    "        self._load_and_split_datasets()\n",
    "\n",
    "    def _setup_transforms(self):\n",
    "        if self.do_transform:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1), # Ensure grayscale\n",
    "                # MERGED FROM handwriting_training.ipynb: Enhanced augmentations\n",
    "                RandomChoice([\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10, fill=255), # Added fill from training\n",
    "                    transforms.RandomPerspective(distortion_scale=0.3, p=0.5, fill=255), # Added fill from training\n",
    "                    transforms.RandomRotation(15, fill=255), # Added from training\n",
    "                ], p=0.8), # Added p for RandomChoice\n",
    "                ThicknessTransform(kernel_size=random.choice([1,2,3]), iterations=random.choice([1,2])), # Kept recognition's ThicknessTransform\n",
    "                transforms.RandomApply([ # Added from training\n",
    "                    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.5))\n",
    "                ], p=0.3),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3), # Still useful for grayscale\n",
    "                transforms.ToTensor(),\n",
    "                # NOTE: handwriting_training.ipynb used transforms.Normalize((0.5,), (0.5,)) for 1-channel images.\n",
    "                # This pipeline converts to 3-channel and uses ImageNet normalization below.\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), # Repeat grayscale channel for models expecting 3 channels\n",
    "                self.normalize,\n",
    "                transforms.RandomErasing(p=0.2, scale=(0.02, 0.03), ratio=(0.3, 3.3), value='random') # Added from training, value='random' for 3 channels\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "                self.normalize\n",
    "            ])\n",
    "\n",
    "        self.val_test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "            self.normalize\n",
    "        ])\n",
    "\n",
    "    def _load_and_split_datasets(self):\n",
    "        full_dataset = datasets.ImageFolder(root=self.data_root)\n",
    "        self.class_names = full_dataset.classes\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "        total_size = len(full_dataset)\n",
    "        test_size = int(total_size * self.test_split)\n",
    "        remaining_size = total_size - test_size\n",
    "        val_size = int(remaining_size * (self.val_split / (1.0 - self.test_split))) # val_split is a fraction of the original total\n",
    "        train_size = remaining_size - val_size\n",
    "\n",
    "        # Check if sizes are valid\n",
    "        if train_size <= 0 or val_size <=0 or test_size <=0:\n",
    "            print(f\"Warning: Dataset too small for current split ratios. Total: {total_size}\")\n",
    "            # Fallback to simpler split if calculated sizes are problematic\n",
    "            if total_size < 3:\n",
    "                train_set, val_set, test_set = full_dataset, full_dataset, full_dataset # Use all for all if tiny\n",
    "            else:\n",
    "                # Simplified split for small datasets\n",
    "                train_size = max(1, int(total_size * 0.7))\n",
    "                val_size = max(1, int(total_size * 0.15))\n",
    "                test_size = total_size - train_size - val_size\n",
    "                if test_size <= 0: # Ensure test_size is at least 1 if possible\n",
    "                    test_size = 1\n",
    "                    val_size = total_size - train_size - test_size\n",
    "                    if val_size <=0:\n",
    "                        val_size = 1\n",
    "                        train_size = total_size - val_size - test_size\n",
    "                \n",
    "        print(f\"Attempting to split: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "        try:\n",
    "            train_temp_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size + val_size, test_size],\n",
    "                                                                          generator=torch.Generator().manual_seed(42))\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(train_temp_dataset, [train_size, val_size],\n",
    "                                                                       generator=torch.Generator().manual_seed(42))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dataset splitting: {e}. Adjusting split sizes or check dataset.\")\n",
    "            # Fallback: if split fails, assign datasets to avoid crashing, though this is not ideal.\n",
    "            # This might happen if dataset is extremely small. User should be warned.\n",
    "            print(\"Using full dataset for train/val/test due to splitting error. THIS IS NOT RECOMMENDED FOR ACTUAL TRAINING.\")\n",
    "            train_dataset, val_dataset, test_dataset = full_dataset, full_dataset, full_dataset\n",
    "\n",
    "        # Assign transforms\n",
    "        # We need to wrap these datasets to apply transforms\n",
    "        self.train_dataset = TransformedDataset(train_dataset, transform=self.train_transform)\n",
    "        self.val_dataset = TransformedDataset(val_dataset, transform=self.val_test_transform)\n",
    "        self.test_dataset = TransformedDataset(test_dataset, transform=self.val_test_transform)\n",
    "        \n",
    "        self.sizes = {'train': len(self.train_dataset), 'val': len(self.val_dataset), 'test': len(self.test_dataset)}\n",
    "\n",
    "    def get_loaders(self, shuffle_train=True, shuffle_val=False, shuffle_test=False):\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=shuffle_train, num_workers=0) # num_workers=0 for notebooks generally safer\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=shuffle_val, num_workers=0)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=shuffle_test, num_workers=0)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self.class_names\n",
    "\n",
    "# Helper class to apply transforms to subsets from random_split\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Initialization and Loader Retrieval Example\n",
    "\n",
    "# IMPORTANT: Replace this path with the actual path to your dataset's root folder.\n",
    "# The dataset should be structured with subfolders for each class (e.g., data_root/A, data_root/B, ...).\n",
    "data_root_example = \"./content/augmented_images/augmented_images1\" # <<< USER: CHANGE THIS PATH\n",
    "\n",
    "print(f\"Attempting to initialize data pipeline with root: {data_root_example}\")\n",
    "print(\"If this path is incorrect or the dataset is not structured as expected (ImageFolder format), this cell will error.\")\n",
    "\n",
    "# Check if the placeholder path exists, if not, this cell will likely error later but we can warn now.\n",
    "if not os.path.exists(data_root_example):\n",
    "    print(f\"\\nWARNING: The directory '{data_root_example}' does not exist. \\nPlease ensure your dataset is available at this path or update 'data_root_example'.\")\n",
    "    print(\"Skipping pipeline initialization and loader retrieval for now as the path is invalid.\")\n",
    "    # Assign None to loaders to prevent subsequent cells from crashing immediately if they use these variables\n",
    "    # Users will need to fix the path and re-run for those cells to work.\n",
    "    train_loader_example, val_loader_example, test_loader_example, dataset_sizes_example, num_classes_example = None, None, None, None, None\n",
    "    example_pipeline = None\n",
    "else:\n",
    "    try:\n",
    "        example_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=16, do_transform=True)\n",
    "        train_loader_example, val_loader_example, test_loader_example = example_pipeline.get_loaders()\n",
    "        dataset_sizes_example = example_pipeline.sizes\n",
    "        num_classes_example = example_pipeline.num_classes\n",
    "\n",
    "        print(f\"\\nData pipeline initialized successfully.\")\n",
    "        print(f\"Dataset split sizes: {dataset_sizes_example}\")\n",
    "        print(f\"Number of classes: {num_classes_example}\")\n",
    "        print(f\"Class labels: {example_pipeline.get_class_labels()}\")\n",
    "        print(f\"Train loader: {train_loader_example}, Val loader: {val_loader_example}, Test loader: {test_loader_example}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR initializing data pipeline or getting loaders: {e}\")\n",
    "        print(\"Please check the 'data_root_example' path and the dataset structure.\")\n",
    "        train_loader_example, val_loader_example, test_loader_example, dataset_sizes_example, num_classes_example = None, None, None, None, None\n",
    "        example_pipeline = None\n",
    "\n",
    "# Note to user:\n",
    "# For the subsequent cells (like displaying augmented images or training models) to run,\n",
    "# the 'data_root_example' path must be correctly set and the dataset must be present and structured correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Augmented Images\n",
    "\n",
    "This function helps visualize the effect of the data augmentation techniques applied to the training images. It fetches a batch of images from the training loader and displays a few of them, each with multiple augmented versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGED FROM handwriting_training.ipynb: display_augmented_images (more detailed original image retrieval)\n",
    "# Note: This function assumes the dataloader's dataset is a TransformedDataset wrapping a Subset, \n",
    "# which in turn wraps an ImageFolder dataset, to access original paths and class_to_idx.\n",
    "# This matches the structure of HandwritingDataPipeline in this notebook.\n",
    "def display_augmented_images(data_loader, num_images=5, num_augmentations=3):\n",
    "    \"\"\"Displays original and augmented images from the train_loader.\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader for the training set.\n",
    "        num_images (int): Number of unique images to display.\n",
    "        num_augmentations (int): Number of augmented versions to show per image.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if data_loader is None or not hasattr(data_loader, 'dataset') or not hasattr(data_loader.dataset, 'subset') \\\n",
    "       or not hasattr(data_loader.dataset.subset, 'dataset') or not hasattr(data_loader.dataset.subset.dataset, 'class_to_idx') \\\n",
    "       or not hasattr(data_loader.dataset.subset.dataset, 'imgs'):\n",
    "        print(\"Data loader is None or not structured as expected (TransformedDataset -> Subset -> ImageFolder). Cannot display images.\")\n",
    "        print(\"Please ensure the data pipeline was initialized correctly and is feeding this function.\")\n",
    "        return\n",
    "\n",
    "    # Get class names from the dataset\n",
    "    # Accessing through the layers of TransformedDataset -> Subset -> ImageFolder\n",
    "    imagefolder_dataset = data_loader.dataset.subset.dataset\n",
    "    class_to_idx = imagefolder_dataset.class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "    # Get random indices from the subset within TransformedDataset\n",
    "    subset_indices = data_loader.dataset.subset.indices\n",
    "    if len(subset_indices) < num_images:\n",
    "        print(f\"Warning: Requested {num_images} images, but dataset only has {len(subset_indices)}. Displaying all available.\")\n",
    "        num_images = len(subset_indices)\n",
    "    \n",
    "    if num_images == 0:\n",
    "        print(\"No images to display.\")\n",
    "        return\n",
    "\n",
    "    random_subset_indices = random.sample(range(len(subset_indices)), num_images)\n",
    "\n",
    "    fig = plt.figure(figsize=(num_augmentations * 3, num_images * 3))\n",
    "\n",
    "    # Determine the transform used for training (for augmentations) and a basic one for original\n",
    "    # This assumes data_loader.dataset.transform is the training transform\n",
    "    train_transform = data_loader.dataset.transform\n",
    "    # Basic transform for original image (Resize + ToTensor + Grayscale, no denormalization needed before display if handled by imshow)\n",
    "    # The train_transform in the pipeline already includes ToTensor and Normalize, so we mostly need to fetch original PIL\n",
    "    # and then apply train_transform for augmented views.\n",
    "\n",
    "    for i, random_idx_in_subset in enumerate(random_subset_indices):\n",
    "        original_dataset_idx = subset_indices[random_idx_in_subset]\n",
    "        original_path, true_label_idx = imagefolder_dataset.imgs[original_dataset_idx]\n",
    "        class_name = idx_to_class[true_label_idx]\n",
    "\n",
    "        original_pil = Image.open(original_path) # Load as PIL\n",
    "\n",
    "        # Show true original image (minimal processing, just resize for consistency if needed)\n",
    "        ax = plt.subplot(num_images, num_augmentations + 1, i * (num_augmentations + 1) + 1)\n",
    "        ax.imshow(original_pil.convert(\"RGB\")) # Display original PIL image (converted to RGB for plt)\n",
    "        ax.set_title(f'Original: {class_name}')\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Show augmented versions\n",
    "        for j in range(num_augmentations):\n",
    "            # Apply the full training transform to the original PIL image\n",
    "            # The train_transform should handle Grayscale, ToTensor, Normalize, and all augmentations\n",
    "            augmented_tensor = train_transform(original_pil.copy()) # Use a copy\n",
    "\n",
    "            ax = plt.subplot(num_images, num_augmentations + 1, i * (num_augmentations + 1) + j + 2)\n",
    "            # Unnormalize for display if normalization is part of train_transform\n",
    "            # Assuming ImageNet normalization as used in the pipeline\n",
    "            img_display = augmented_tensor.cpu().numpy().transpose((1, 2, 0))\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img_display = std * img_display + mean\n",
    "            img_display = np.clip(img_display, 0, 1)\n",
    "            # If the output is single channel after repeat, squeeze it, else imshow will handle 3 channels.\n",
    "            if img_display.shape[2] == 1:\n",
    "                 plt.imshow(img_display.squeeze(), cmap='gray')\n",
    "            else:\n",
    "                 plt.imshow(img_display)\n",
    "            ax.set_title(f'Aug {j+1}: {class_name}')\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (ensure train_loader_example and example_pipeline are valid from the previous cell)\n",
    "if train_loader_example and example_pipeline:\n",
    "    print(\"Displaying augmented images...\")\n",
    "    # The class_names are now derived inside the function from the loader itself\n",
    "    display_augmented_images(train_loader_example, num_images=4, num_augmentations=3)\n",
    "else:\n",
    "    print(\"Skipping display_augmented_images example as train_loader_example or example_pipeline is not available. Please fix the data path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions: Custom CNNs\n",
    "\n",
    "Below are definitions for two custom Convolutional Neural Network (CNN) architectures:\n",
    "- `LetterCNN64`: A basic CNN model designed for 64x64 pixel input images.\n",
    "- `ImprovedLetterCNN`: An enhanced version of `LetterCNN64` that incorporates Batch Normalization for stable training and Dropout for regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterCNN64(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LetterCNN64, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) # Input 3 channels (RGB-like after transform)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input size: 64x64 -> After conv1 (padding=1): 64x64 -> After pool1: 32x32\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input size: 32x32 -> After conv2 (padding=1): 32x32 -> After pool2: 16x16\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input size: 16x16 -> After conv3 (padding=1): 16x16 -> After pool3: 8x8\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Flattened size: 128 channels * 8 * 8 = 8192\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8) # Flatten the tensor\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImprovedLetterCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ImprovedLetterCNN, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 64x64 -> 32x32\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 32x32 -> 16x16\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 16x16 -> 8x8\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 8x8 -> 4x4\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Flattened size: 256 channels * 4 * 4 = 4096\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5) # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.dropout1(self.relu_fc1(self.bn_fc1(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu_fc2(self.bn_fc2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition: VGG19 Transfer Learning\n",
    "\n",
    "This section defines `VGG19HandwritingModel`, which leverages a pre-trained VGG19 model for transfer learning. \n",
    "- **Transfer Learning**: Instead of training a model from scratch, we use the VGG19 architecture with weights pre-trained on the large ImageNet dataset. These learned features are often effective for various computer vision tasks.\n",
    "- **Adaptation for Grayscale**: The first convolutional layer of VGG19 is modified to accept single-channel (grayscale) input images, as our handwriting dataset is primarily grayscale. The weights of this layer can be initialized by averaging the original weights across the three color channels or by other strategies.\n",
    "- **Custom Classifier**: The original VGG19 classifier (fully connected layers) is replaced with a new classifier suited to the number of classes in our handwriting dataset. This new classifier will be trained on our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19HandwritingModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, pretrained=True):\n",
    "        super(VGG19HandwritingModel, self).__init__()\n",
    "        self.device = device # Added device to be used within model if needed\n",
    "        # Load a pretrained VGG19 model\n",
    "        # MERGED FROM handwriting_training.ipynb: Note on vgg19 weights parameter\n",
    "        # 'DEFAULT' is preferred for pretrained=True in newer torchvision for vgg19\n",
    "        # models.VGG19_BN_Weights.IMAGENET1K_V1 is also valid.\n",
    "        vgg19 = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        vgg19 = vgg19.to(device) # MERGED FROM handwriting_training.ipynb: move model to device early\n",
    "\n",
    "        # NOTE FROM MERGE: handwriting_recognition.ipynb explains why it sticks to 3-channel input for VGG\n",
    "        # due to its data pipeline. handwriting_training.ipynb modifies the first conv layer for 1-channel.\n",
    "        # We are keeping the 3-channel approach from handwriting_recognition.ipynb for consistency with its pipeline.\n",
    "        # If a 1-channel pipeline was adopted, the modification from handwriting_training.ipynb would be:\n",
    "        # self.features = vgg19.features\n",
    "        # self.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1).to(device)\n",
    "        self.features = vgg19.features\n",
    "\n",
    "        # Freeze feature parameters if using pretrained model (common practice for transfer learning)\n",
    "        if pretrained:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier with a new one appropriate for num_classes\n",
    "        # VGG19's classifier input features: 512 * 7 * 7 (if input is 224x224)\n",
    "        # Since our input is 64x64, the output feature map size from self.features will be different.\n",
    "        # VGG19 with 64x64 input: after 5 maxpools (64 -> 32 -> 16 -> 8 -> 4 -> 2)\n",
    "        # So, the output from features will be (batch_size, 512, 2, 2)\n",
    "        num_features_output = 512 * 2 * 2 \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features_output, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        ).to(device) # MERGED FROM handwriting_training.ipynb: move classifier to device\n",
    "\n",
    "        # MERGED FROM handwriting_training.ipynb: Custom weight initialization for the classifier\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # MERGED FROM handwriting_training.ipynb: _initialize_weights method\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The data pipeline ensures x is (batch, 3, H, W)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Utility Functions: Training, Loading, and Testing\n",
    "\n",
    "These functions are essential for the model development lifecycle:\n",
    "- `train_model`: Handles the training loop, including forward pass, loss calculation, backpropagation, optimizer steps, and learning rate scheduling. It also includes logic for validating the model periodically and saving the best performing model checkpoint as well as the final model.\n",
    "- `load_model`: Utility to load saved model checkpoints (weights and optimizer state) to resume training or for inference.\n",
    "- `test_model`: Evaluates the trained model on the test dataset to assess its generalization performance on unseen data, reporting accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, save_dir='model_checkpoints'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Created directory: {save_dir}\")\n",
    "        \n",
    "    start_time = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Store history\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            if dataloader is None or len(dataloader.dataset) == 0:\n",
    "                print(f\"Skipping {phase} phase as dataloader is None or dataset is empty.\")\n",
    "                if phase == 'val' and best_acc == 0: # if val loader is bad, cannot determine best model based on val_acc\n",
    "                     # Save model based on training performance or just save last if no val set\n",
    "                     pass # Or consider other metrics if val is unavailable\n",
    "                continue\n",
    "\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                total_samples += inputs.size(0)\n",
    "            \n",
    "            if total_samples == 0: # Avoid division by zero if dataset was empty\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                print(f\"No samples found for {phase} phase in epoch {epoch+1}.\")\n",
    "            else:\n",
    "                epoch_loss = running_loss / total_samples\n",
    "                epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item()) # .item() to get Python number\n",
    "            else: # phase == 'val'\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                if scheduler:\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(epoch_loss)\n",
    "                    # For other schedulers like CosineAnnealingLR, step is usually called after optimizer.step()\n",
    "                    # For this structure, it's fine here if not ReduceLROnPlateau\n",
    "                    elif phase == 'train' and scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                         pass # Handled below after optimizer step for most schedulers\n",
    "\n",
    "                if epoch_acc > best_acc and total_samples > 0 : # ensure val set was processed\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch + 1\n",
    "                    # Save best model checkpoint\n",
    "                    best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "                    torch.save({\n",
    "                        'epoch': best_epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': epoch_loss,\n",
    "                        'accuracy': best_acc.item(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
    "                    }, best_model_path)\n",
    "                    print(f\"Best model saved to {best_model_path} (Epoch {best_epoch}, Val Acc: {best_acc:.4f})\")\n",
    "        \n",
    "        if phase == 'train' and scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step() # Step for schedulers like CosineAnnealing, StepLR etc.\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f} at epoch {best_epoch}')\n",
    "\n",
    "    # Load best model weights back\n",
    "    if best_acc > 0: # only load if a best model was found (i.e. val phase ran)\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Save final model (MERGED FROM handwriting_training.ipynb: to include scheduler state consistently)\n",
    "    final_model_path = os.path.join(save_dir, 'final_model.pth')\n",
    "    torch.save({\n",
    "        'epoch': num_epochs, # Save as num_epochs, consistent with best_model save for epoch number\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None, # MERGED: Ensure scheduler state is saved\n",
    "        'loss': history['val_loss'][-1] if history['val_loss'] else (history['train_loss'][-1] if history['train_loss'] else 0.0),\n",
    "        'accuracy': history['val_acc'][-1] if history['val_acc'] else (history['train_acc'][-1] if history['train_acc'] else 0.0),\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # MERGED FROM handwriting_training.ipynb: Plot training and validation loss\n",
    "    if history['train_loss'] and history['val_loss']:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(history['train_loss'], label=\"Train Loss\")\n",
    "        plt.plot(history['val_loss'], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "        plt.show()\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def load_model(model, optimizer, checkpoint_path, scheduler=None):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint path {checkpoint_path} does not exist. Returning initial model.\")\n",
    "        return model, optimizer, scheduler, 0, 0.0, 0.0\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) # map_location ensures tensors are loaded to the correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state only if optimizer is provided and state exists in checkpoint\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        try:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not load optimizer state: {e}. Optimizer will be reinitialized.\")\n",
    "    \n",
    "    # Load scheduler state only if scheduler is provided and state exists in checkpoint\n",
    "    if scheduler is not None and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:\n",
    "        try:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        except Exception as e: # Catching general exception as state loading can have various issues\n",
    "            print(f\"Warning: Could not load scheduler state: {e}. Scheduler may be reinitialized or use default state.\")\n",
    "            \n",
    "    start_epoch = checkpoint.get('epoch', 0)\n",
    "    loss = checkpoint.get('loss', 0.0)\n",
    "    accuracy = checkpoint.get('accuracy', 0.0)\n",
    "    \n",
    "    print(f\"Model loaded from {checkpoint_path}. Epoch: {start_epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return model, optimizer, scheduler, start_epoch, loss, accuracy\n",
    "\n",
    "def test_model(model, test_loader, criterion=None):\n",
    "    if test_loader is None or len(test_loader.dataset) == 0:\n",
    "        print(\"Test loader is None or dataset is empty. Skipping testing.\")\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    model.eval() # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss() # Default criterion if not provided\n",
    "\n",
    "    with torch.no_grad(): # No need to track gradients during testing\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        print(\"No samples found in the test set.\")\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "    return test_loss, test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Training a Custom CNN (`ImprovedLetterCNN`)\n",
    "\n",
    "In this experiment, we will train the `ImprovedLetterCNN` model defined earlier. This model includes improvements like Batch Normalization and Dropout. We will use data augmentation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 1: Training ImprovedLetterCNN ---\n",
    "print(\"--- Experiment 1: Training ImprovedLetterCNN ---\")\n",
    "\n",
    "# Ensure data_root_example and device are defined from earlier cells.\n",
    "# If example_pipeline was successfully initialized, we can use its properties.\n",
    "if example_pipeline is not None and num_classes_example is not None:\n",
    "    save_dir_cnn = 'model_checkpoints_cnn'\n",
    "    if not os.path.exists(save_dir_cnn):\n",
    "        os.makedirs(save_dir_cnn)\n",
    "\n",
    "    # Re-initialize pipeline for this experiment if needed, or use existing one\n",
    "    # For clarity, let's assume we might want different settings, so re-init or use specific vars\n",
    "    # This assumes data_root_example is correctly set by the user.\n",
    "    print(f\"Using data_root: {data_root_example} for CNN experiment.\")\n",
    "    cnn_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=32, do_transform=True)\n",
    "    train_loader_cnn, val_loader_cnn, test_loader_cnn = cnn_pipeline.get_loaders()\n",
    "    num_classes_cnn = cnn_pipeline.num_classes\n",
    "    \n",
    "    if num_classes_cnn > 0 and len(train_loader_cnn.dataset) > 0:\n",
    "        print(f\"Number of classes for CNN: {num_classes_cnn}\")\n",
    "        print(f\"Train samples: {len(train_loader_cnn.dataset)}, Val samples: {len(val_loader_cnn.dataset)}, Test samples: {len(test_loader_cnn.dataset)}\")\n",
    "\n",
    "        model_cnn = ImprovedLetterCNN(num_classes_cnn).to(device)\n",
    "        criterion_cnn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer and Scheduler Setup\n",
    "        optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.0005) # Adjusted LR from 0.001 to 0.0005\n",
    "        scheduler_cnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_cnn, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "        # --- Other Scheduler Examples (commented out) ---\n",
    "        # scheduler_cnn_step = torch.optim.lr_scheduler.StepLR(optimizer_cnn, step_size=7, gamma=0.1)\n",
    "        # # Note: For StepLR, T_max and eta_min are not applicable. StepLR reduces LR by gamma every step_size epochs.\n",
    "\n",
    "        # scheduler_cnn_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn, T_max=20, eta_min=1e-6) # T_max = num_epochs\n",
    "        # # Note: CosineAnnealingLR gradually decreases LR following a cosine curve.\n",
    "\n",
    "        # num_epochs_cnn = 20 # Define num_epochs if using schedulers that require it like OneCycleLR or CosineAnnealingLR\n",
    "        # scheduler_cnn_onecycle = torch.optim.lr_scheduler.OneCycleLR(optimizer_cnn, max_lr=0.001, epochs=num_epochs_cnn, steps_per_epoch=len(train_loader_cnn))\n",
    "        # # Note: OneCycleLR varies LR, increasing then decreasing it over the epochs. max_lr should typically be determined with an LR range test.\n",
    "        # --- End of Other Scheduler Examples ---\n",
    "\n",
    "        num_epochs_cnn_train = 20 # Set number of epochs for this experiment\n",
    "        print(f\"Starting training for ImprovedLetterCNN for {num_epochs_cnn_train} epochs...\")\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model_cnn, history_cnn = train_model(model_cnn, train_loader_cnn, val_loader_cnn, \n",
    "                                                   criterion_cnn, optimizer_cnn, scheduler_cnn, \n",
    "                                                   num_epochs=num_epochs_cnn_train, save_dir=save_dir_cnn)\n",
    "        \n",
    "        print(\"\\nTesting the final trained ImprovedLetterCNN model on the test set:\")\n",
    "        test_model(trained_model_cnn, test_loader_cnn, criterion_cnn)\n",
    "\n",
    "        print(\"\\nLoading the best saved ImprovedLetterCNN model and testing it on the test set:\")\n",
    "        # Initialize a new model instance for loading\n",
    "        best_model_cnn_instance = ImprovedLetterCNN(num_classes_cnn).to(device)\n",
    "        # Create a dummy optimizer for load_model function, its state will be overwritten if saved in checkpoint\n",
    "        # Or, if you need to resume training with this optimizer, re-initialize it as before.\n",
    "        dummy_optimizer_cnn = optim.Adam(best_model_cnn_instance.parameters()) \n",
    "        best_cnn_model_loaded, _, _, _, _, _ = load_model(best_model_cnn_instance, \n",
    "                                                       dummy_optimizer_cnn, \n",
    "                                                       os.path.join(save_dir_cnn, 'best_model.pth'))\n",
    "        test_model(best_cnn_model_loaded, test_loader_cnn, criterion_cnn)\n",
    "    else:\n",
    "        print(\"Skipping Experiment 1: CNN training, due to invalid number of classes or empty train loader.\")\n",
    "        print(f\"  Number of classes: {num_classes_cnn if 'num_classes_cnn' in locals() else 'Not determined'}\")\n",
    "        print(f\"  Train loader dataset length: {len(train_loader_cnn.dataset) if 'train_loader_cnn' in locals() and hasattr(train_loader_cnn, 'dataset') else 'Not determined'}\")\n",
    "else:\n",
    "    print(\"Skipping Experiment 1: CNN training, as the data pipeline was not initialized successfully in the earlier cell.\")\n",
    "    print(\"Please ensure 'data_root_example' is set correctly and the dataset is accessible.\")\n",
    "\n",
    "# MERGED FROM handwriting_training.ipynb: freeze_layers function\n",
    "def freeze_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"Freezes the first num_layers_to_freeze layers of the model's features.\"\"\"\n",
    "    if hasattr(model, 'features') and isinstance(model.features, nn.Sequential):\n",
    "        # Count actual layers (Conv2d, Linear, etc.) if features is a sequential block\n",
    "        # This is a simplified counter; a more robust way might be needed for complex feature extractors\n",
    "        layer_idx = 0\n",
    "        for child in model.features.children():\n",
    "            if isinstance(child, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)):\n",
    "                 if layer_idx < num_layers_to_freeze:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = False\n",
    "                 layer_idx +=1\n",
    "        print(f\"Froze {min(num_layers_to_freeze, layer_idx)} layers in model.features.\")\n",
    "    else:\n",
    "        # Fallback for generic parameters if model.features is not a typical Sequential block\n",
    "        # This part might need adjustment based on specific model structure\n",
    "        params = list(model.parameters()) \n",
    "        actual_layers_to_freeze = min(num_layers_to_freeze, len(params))\n",
    "        for i, param in enumerate(params):\n",
    "            if i < actual_layers_to_freeze:\n",
    "                param.requires_grad = False\n",
    "        print(f\"Froze first {actual_layers_to_freeze} parameter groups (layers) of the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Training with Transfer Learning (`VGG19HandwritingModel`)\n",
    "\n",
    "This experiment explores transfer learning using the `VGG19HandwritingModel`. We will initialize the model with weights pre-trained on ImageNet and fine-tune it on our handwriting dataset. We will also demonstrate how to set up the optimizer with different learning rates for the pre-trained feature extractor and the newly initialized classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 2: Training VGG19HandwritingModel ---\n",
    "print(\"--- Experiment 2: Training VGG19HandwritingModel ---\")\n",
    "\n",
    "if example_pipeline is not None and num_classes_example is not None:\n",
    "    save_dir_vgg = 'model_checkpoints_vgg'\n",
    "    if not os.path.exists(save_dir_vgg):\n",
    "        os.makedirs(save_dir_vgg)\n",
    "\n",
    "    print(f\"Using data_root: {data_root_example} for VGG experiment.\")\n",
    "    # It's good practice to have a separate pipeline instance if batch_size or other params differ\n",
    "    # Or re-use cnn_pipeline if settings are identical\n",
    "    vgg_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=32, do_transform=True)\n",
    "    train_loader_vgg, val_loader_vgg, test_loader_vgg = vgg_pipeline.get_loaders()\n",
    "    num_classes_vgg = vgg_pipeline.num_classes\n",
    "    num_epochs_vgg = 20 # Define number of epochs for VGG training\n",
    "\n",
    "    if num_classes_vgg > 0 and len(train_loader_vgg.dataset) > 0:\n",
    "        print(f\"Number of classes for VGG: {num_classes_vgg}\")\n",
    "        print(f\"Train samples: {len(train_loader_vgg.dataset)}, Val samples: {len(val_loader_vgg.dataset)}, Test samples: {len(test_loader_vgg.dataset)}\")\n",
    "\n",
    "        # Model Initialization\n",
    "        # Option 1: Use pretrained VGG19\n",
    "        use_pretrained_vgg = True # Set to False to train VGG from scratch\n",
    "        model_vgg = VGG19HandwritingModel(num_classes=num_classes_vgg, device=device, pretrained=use_pretrained_vgg).to(device)\n",
    "        print(f\"VGG19 Model initialized {'with pretrained ImageNet weights' if use_pretrained_vgg else 'from scratch'}.\")\n",
    "\n",
    "        criterion_vgg = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer and Scheduler Setup for VGG\n",
    "        if use_pretrained_vgg:\n",
    "            # Different learning rates for feature extractor (lower LR) and classifier (higher LR)\n",
    "            optimizer_vgg = optim.Adam([\n",
    "                {'params': model_vgg.features.parameters(), 'lr': 1e-5}, # Lower LR for frozen or slowly unfrozen features\n",
    "                {'params': model_vgg.classifier.parameters(), 'lr': 1e-4} # Higher LR for the new classifier part\n",
    "            ], weight_decay=1e-4) # Added weight decay\n",
    "            # Scheduler for fine-tuning pretrained model\n",
    "            scheduler_vgg = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vgg, T_max=num_epochs_vgg, eta_min=1e-6)\n",
    "            print(\"Optimizer set up for pretrained VGG model with differential learning rates.\")\n",
    "        else:\n",
    "            # Training VGG from scratch (typically requires more epochs and careful LR tuning)\n",
    "            optimizer_vgg = optim.Adam(model_vgg.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "            # OneCycleLR is often good for training from scratch\n",
    "            scheduler_vgg = torch.optim.lr_scheduler.OneCycleLR(optimizer_vgg, \n",
    "                                                                max_lr=1e-3, \n",
    "                                                                epochs=num_epochs_vgg, \n",
    "                                                                steps_per_epoch=len(train_loader_vgg))\n",
    "            print(\"Optimizer set up for VGG model training from scratch.\")\n",
    "\n",
    "        print(f\"Starting training for VGG19HandwritingModel for {num_epochs_vgg} epochs...\")\n",
    "        trained_model_vgg, history_vgg = train_model(model_vgg, train_loader_vgg, val_loader_vgg, \n",
    "                                                   criterion_vgg, optimizer_vgg, scheduler_vgg, \n",
    "                                                   num_epochs=num_epochs_vgg, save_dir=save_dir_vgg)\n",
    "\n",
    "        print(\"\\nTesting the final trained VGG19 model on the test set:\")\n",
    "        test_model(trained_model_vgg, test_loader_vgg, criterion_vgg)\n",
    "\n",
    "        print(\"\\nLoading the best saved VGG19 model and testing it on the test set:\")\n",
    "        best_model_vgg_instance = VGG19HandwritingModel(num_classes=num_classes_vgg, device=device, pretrained=False).to(device) # `pretrained` here only affects init, weights are loaded next\n",
    "        dummy_optimizer_vgg = optim.Adam(best_model_vgg_instance.parameters())\n",
    "        best_vgg_model_loaded, _, _, _, _, _ = load_model(best_model_vgg_instance, \n",
    "                                                       dummy_optimizer_vgg, \n",
    "                                                       os.path.join(save_dir_vgg, 'best_model.pth'))\n",
    "        test_model(best_vgg_model_loaded, test_loader_vgg, criterion_vgg)\n",
    "    else:\n",
    "        print(\"Skipping Experiment 2: VGG training, due to invalid number of classes or empty train loader.\")\n",
    "        print(f\"  Number of classes: {num_classes_vgg if 'num_classes_vgg' in locals() else 'Not determined'}\")\n",
    "        print(f\"  Train loader dataset length: {len(train_loader_vgg.dataset) if 'train_loader_vgg' in locals() and hasattr(train_loader_vgg, 'dataset') else 'Not determined'}\")\n",
    "else:\n",
    "    print(\"Skipping Experiment 2: VGG training, as the data pipeline was not initialized successfully.\")\n",
    "    print(\"Please ensure 'data_root_example' is set correctly and the dataset is accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on a Single Image\n",
    "\n",
    "This section shows how to load a trained model (either the custom CNN or the VGG model) and use it to predict the character from a single image. You'll need to provide the path to your trained model and the image you want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(data_root):\n",
    "    \"\"\"Gets class labels from the folder names in data_root.\"\"\"\n",
    "    if not os.path.exists(data_root):\n",
    "        print(f\"Error: Data root directory '{data_root}' not found for getting class labels.\")\n",
    "        return []\n",
    "    # Assuming ImageFolder structure where subdirectories are class names\n",
    "    try:\n",
    "        dataset = datasets.ImageFolder(root=data_root)\n",
    "        return dataset.classes\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset from '{data_root}' to get class labels: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_path, image_size=(64, 64)):\n",
    "    \"\"\"Loads an image, converts to grayscale, resizes, and prepares it for model inference.\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image path '{image_path}' not found.\")\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('L') # Convert to grayscale\n",
    "        \n",
    "        # Define transformations similar to validation/test transform but without dataset context\n",
    "        # Normalization values should be consistent with training\n",
    "        normalize_inference = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                  std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        inference_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(), # This will convert grayscale PIL image (H,W) to (1,H,W) tensor\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), # Repeat for 3 channels\n",
    "            normalize_inference\n",
    "        ])\n",
    "        \n",
    "        img_tensor = inference_transform(img)\n",
    "        return img_tensor.unsqueeze(0) # Add batch dimension -> (1, 3, H, W)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform Inference --- \n",
    "print(\"--- Inference Section ---\")\n",
    "\n",
    "# USER ACTION REQUIRED:\n",
    "# 1. Set `inference_model_path` to the path of your trained model checkpoint (e.g., best_model.pth from CNN or VGG experiment).\n",
    "# 2. Set `data_root_for_labels` to the *same data_root* used during training to ensure class labels are mapped correctly.\n",
    "# 3. Set `image_path_for_inference` to the path of the image you want to classify.\n",
    "\n",
    "inference_model_path = os.path.join('model_checkpoints_cnn', 'best_model.pth') # Example: using best CNN model\n",
    "# Or for VGG: inference_model_path = os.path.join('model_checkpoints_vgg', 'best_model.pth')\n",
    "\n",
    "data_root_for_labels = data_root_example # Use the same data_root as defined in cell 6 (or your training data path)\n",
    "\n",
    "# Example image path - replace with your own image.\n",
    "# You might need to upload an image to your Colab environment or provide a full path if running locally.\n",
    "# image_path_for_inference = \"path/to/your/character/image.png\" \n",
    "image_path_for_inference = \"\" # Intentionally blank: USER MUST PROVIDE A PATH\n",
    "\n",
    "print(f\"Using model: {inference_model_path}\")\n",
    "print(f\"Using data root for labels: {data_root_for_labels}\")\n",
    "\n",
    "if not os.path.exists(inference_model_path):\n",
    "    print(f\"ERROR: Model checkpoint '{inference_model_path}' not found. Please train a model or provide a valid path.\")\n",
    "elif not image_path_for_inference or not os.path.exists(image_path_for_inference):\n",
    "    print(f\"INFO: 'image_path_for_inference' is not set or the file does not exist ('{image_path_for_inference}').\")\n",
    "    print(\"Please provide a valid image path to perform inference.\")\n",
    "    # You could create a dummy image here for demonstration if you have cv2/numpy and know the dataset structure\n",
    "    # For example, to create a dummy 'A.png':\n",
    "    # if not os.path.exists(\"example_char.png\"):\n",
    "    #     dummy_img_arr = np.zeros((64,64,1), dtype=np.uint8)\n",
    "    #     cv2.putText(dummy_img_arr, 'A', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255), 3)\n",
    "    #     cv2.imwrite(\"example_char.png\", dummy_img_arr)\n",
    "    #     image_path_for_inference = \"example_char.png\"\n",
    "    #     print(\"Created a dummy 'example_char.png'. Re-run this cell with this path or your own.\")\n",
    "    # else:\n",
    "    #     image_path_for_inference = \"example_char.png\"\n",
    "    #     print(f\"Using existing 'example_char.png'. Re-run cell if you want to use your own image.\")\n",
    "else:\n",
    "    class_labels = get_class_labels(data_root_for_labels)\n",
    "    if not class_labels:\n",
    "        print(\"ERROR: Could not retrieve class labels. Ensure 'data_root_for_labels' is correct.\")\n",
    "    else:\n",
    "        num_classes_inference = len(class_labels)\n",
    "        print(f\"Number of classes for inference: {num_classes_inference}, Labels: {class_labels}\")\n",
    "\n",
    "        # Initialize the model architecture\n",
    "        # IMPORTANT: This must match the architecture of the saved model in 'inference_model_path'\n",
    "        # If loading a VGG model, use: VGG19HandwritingModel(num_classes=num_classes_inference, device=device, pretrained=False).to(device)\n",
    "        # The `pretrained=False` here is fine because we are loading weights from our checkpoint.\n",
    "        inference_model_arch = ImprovedLetterCNN(num_classes_inference).to(device) # Assuming CNN model\n",
    "        # If using VGG, uncomment below and comment out the ImprovedLetterCNN line:\n",
    "        # inference_model_arch = VGG19HandwritingModel(num_classes=num_classes_inference, device=device, pretrained=False).to(device)\n",
    "        \n",
    "        # Create a dummy optimizer for load_model. Its state isn't used if only inferring.\n",
    "        dummy_optimizer_inference = optim.Adam(inference_model_arch.parameters())\n",
    "\n",
    "        # Load the trained model weights\n",
    "        loaded_inference_model, _, _, _, _, _ = load_model(inference_model_arch, \n",
    "                                                          dummy_optimizer_inference, \n",
    "                                                          inference_model_path)\n",
    "        loaded_inference_model.eval() # Set model to evaluation mode\n",
    "\n",
    "        # Prepare the input image\n",
    "        input_tensor = prepare_image(image_path_for_inference).to(device)\n",
    "\n",
    "        if input_tensor is not None:\n",
    "            with torch.no_grad(): # Disable gradient calculations for inference\n",
    "                outputs = loaded_inference_model(input_tensor)\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                confidence, predicted_class_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                predicted_label = class_labels[predicted_class_idx.item()]\n",
    "                confidence_percent = confidence.item() * 100\n",
    "                \n",
    "                print(f\"\\nImage: {image_path_for_inference}\")\n",
    "                print(f\"Predicted Class: {predicted_label}\")\n",
    "                print(f\"Confidence: {confidence_percent:.2f}%\")\n",
    "\n",
    "                # Display the image (optional)\n",
    "                try:\n",
    "                    img_display = Image.open(image_path_for_inference)\n",
    "                    plt.imshow(img_display)\n",
    "                    plt.title(f\"Predicted: {predicted_label} ({confidence_percent:.2f}%)\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not display the image: {e}\")\n",
    "        else:\n",
    "            print(\"Could not prepare image for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive walkthrough of a handwritten character recognition task using PyTorch. We covered:\n",
    "- Setting up a data pipeline with image loading, transformations, and augmentations.\n",
    "- Defining and comparing custom CNN architectures (`LetterCNN64`, `ImprovedLetterCNN`).\n",
    "- Implementing transfer learning with a pre-trained VGG19 model (`VGG19HandwritingModel`), including modifying it for the specific task.\n",
    "- Core functions for training, model checkpointing (saving best and final models), loading models, and evaluating them on a test set.\n",
    "- Running two distinct training experiments: one for the custom `ImprovedLetterCNN` and another for the `VGG19HandwritingModel`.\n",
    "- A section demonstrating how to perform inference on a single image using a trained model.\n",
    "\n",
    "This framework can be extended and adapted for various image classification tasks. For further improvements, one could explore more advanced architectures, different augmentation strategies, hyperparameter optimization techniques, or larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio (quietly)\n",
    "# MERGED FROM handwriting_training.ipynb: Install Gradio and import (already present but ensures it's noted)\n",
    "!pip install gradio -q\n",
    "import gradio as gr\n",
    "print(f\"Gradio version: {gr.__version__}\")\n",
    "\n",
    "# MERGED FROM handwriting_training.ipynb: Added Path for directory operations\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demos with Gradio"
   ]
  },
  {
    "# MERGED FROM handwriting_training.ipynb: This Gradio section is more comprehensive.\n",
    "# The previous Gradio cells (Demo 1, Demo 2, predict_single_character, segment_characters, predict_multiple_characters, iface_single, iface_multi, TabbedInterface)\n",
    "# will be replaced by the more detailed implementation from handwriting_training.ipynb.\n",
    "\n",
    "# Placeholder for where models are saved (ensure these paths are correct)\n",
    "# These should align with the save_dir used in your training cells.\n",
    "MODEL_PATH_CNN_GRADIO = os.path.join('model_checkpoints_cnn', 'best_model.pth') \n",
    "MODEL_PATH_VGG_GRADIO = os.path.join('model_checkpoints_vgg', 'best_model.pth')\n",
    "\n",
    "# Attempt to get class_names. This assumes 'example_pipeline' is initialized and 'data_root_example' is set.\n",
    "# If not, a placeholder or manual list will be used for Gradio.\n",
    "try:\n",
    "    if example_pipeline is not None:\n",
    "        class_names_gradio = example_pipeline.get_class_labels()\n",
    "        num_classes_gradio = example_pipeline.num_classes\n",
    "        print(f\"Using class_names from example_pipeline for Gradio: {len(class_names_gradio)} classes.\")\n",
    "    elif data_root_example and os.path.exists(data_root_example):\n",
    "        class_names_gradio = get_class_labels(data_root_example) # from existing function in recognition.ipynb\n",
    "        num_classes_gradio = len(class_names_gradio)\n",
    "        print(f\"Using class_names from get_class_labels(data_root_example) for Gradio: {len(class_names_gradio)} classes.\")\n",
    "    else:\n",
    "        raise ValueError(\"Pipeline or data_root_example not available for class names.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not automatically get class_names for Gradio: {e}\")\n",
    "    print(\"Using a placeholder list of 62 common characters. THIS MAY NOT MATCH YOUR TRAINED MODEL.\")\n",
    "    class_names_gradio = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "                        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    num_classes_gradio = len(class_names_gradio)\n",
    "\n",
    "# Load a chosen model for Gradio. Default to ImprovedLetterCNN if available.\n",
    "gradio_model_loaded = None\n",
    "selected_model_type_for_gradio = \"ImprovedLetterCNN\" # Default\n",
    "\n",
    "if os.path.exists(MODEL_PATH_CNN_GRADIO):\n",
    "    print(f\"Loading ImprovedLetterCNN from {MODEL_PATH_CNN_GRADIO} for Gradio.\")\n",
    "    gradio_model_loaded = ImprovedLetterCNN(num_classes_gradio).to(device)\n",
    "    checkpoint_cnn = torch.load(MODEL_PATH_CNN_GRADIO, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint_cnn:\n",
    "        gradio_model_loaded.load_state_dict(checkpoint_cnn['model_state_dict'])\n",
    "    else:\n",
    "        gradio_model_loaded.load_state_dict(checkpoint_cnn) # Older format\n",
    "    gradio_model_loaded.eval()\n",
    "elif os.path.exists(MODEL_PATH_VGG_GRADIO):\n",
    "    print(f\"Loading VGG19HandwritingModel from {MODEL_PATH_VGG_GRADIO} for Gradio as CNN not found.\")\n",
    "    gradio_model_loaded = VGG19HandwritingModel(num_classes_gradio, device=device, pretrained=False).to(device)\n",
    "    checkpoint_vgg = torch.load(MODEL_PATH_VGG_GRADIO, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint_vgg:\n",
    "        gradio_model_loaded.load_state_dict(checkpoint_vgg['model_state_dict'])\n",
    "    else:\n",
    "        gradio_model_loaded.load_state_dict(checkpoint_vgg)\n",
    "    gradio_model_loaded.eval()\n",
    "    selected_model_type_for_gradio = \"VGG19\"\n",
    "else:\n",
    "    print(\"WARNING: No pre-trained model checkpoints found for Gradio. Inference will likely fail or use an uninitialized model.\")\n",
    "    # Optionally initialize a dummy model to prevent crashes, though predictions will be random.\n",
    "    # gradio_model_loaded = ImprovedLetterCNN(num_classes_gradio).to(device)\n",
    "    # gradio_model_loaded.eval()\n",
    "\n",
    "# MERGED FROM handwriting_training.ipynb: process_drawn_image and helper functions\n",
    "def process_drawn_image(image, model, class_list, spacing=24):\n",
    "    \"\"\"\n",
    "    Process a drawn image from Gradio and extract/classify letters.\n",
    "    The 'model' parameter here is the pre-loaded model for Gradio.\n",
    "    \"\"\"\n",
    "    output_dir = \"extracted_letters_gradio\" # Use a different dir for Gradio outputs\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    if image is None:\n",
    "      empty_img = np.ones((100,100,3), dtype=np.uint8) * 255\n",
    "      cv2.putText(empty_img, \"No Image\", (10,50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
    "      return [(\"No Image Provided\", empty_img)], \"No image provided\"\n",
    "\n",
    "    # Convert gradio image (RGB numpy array) to BGR for OpenCV compatibility\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    visualization_images = []\n",
    "    orig_img_viz = image.copy() # RGB for visualization\n",
    "    visualization_images.append((\"Original Drawn Image\", orig_img_viz))\n",
    "\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    visualization_images.append((\"Grayscale Image\", cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    # Binary image for contour detection (black text on white background -> white text on black for findContours)\n",
    "    _, binary_contours_inv = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    visualization_images.append((\"Binary for Contours (Inverted)\", cv2.cvtColor(binary_contours_inv, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    # Binary image for model input (black text on white background)\n",
    "    _, binary_model_input = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    visualization_images.append((\"Binary for Model Input\", cv2.cvtColor(binary_model_input, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    contours, _ = cv2.findContours(binary_contours_inv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_img_viz = image.copy()\n",
    "    cv2.drawContours(contour_img_viz, contours, -1, (0, 255, 0), 2)\n",
    "    visualization_images.append((f\"All Contours ({len(contours)})\", contour_img_viz))\n",
    "\n",
    "    if not contours:\n",
    "        return visualization_images, \"No contours found\"\n",
    "\n",
    "    min_area = 30 # Adjusted min_area, sketchpad might produce smaller artifacts\n",
    "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    if not letter_contours:\n",
    "        return visualization_images, f\"No contours above min_area {min_area}\"\n",
    "        \n",
    "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
    "    \n",
    "    filtered_contour_img_viz = image.copy()\n",
    "    cv2.drawContours(filtered_contour_img_viz, letter_contours, -1, (255,0,0), 2)\n",
    "    visualization_images.append((f\"Filtered Contours ({len(letter_contours)})\", filtered_contour_img_viz))\n",
    "\n",
    "    letters_info_for_text = []\n",
    "    extracted_letters_viz = []\n",
    "    model_inputs_viz = []\n",
    "\n",
    "    if model is None:\n",
    "        return visualization_images, \"ERROR: Model not loaded for Gradio inference.\"\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "\n",
    "    for i, contour in enumerate(letter_contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w < 5 or h < 5: continue # Skip tiny contours\n",
    "\n",
    "        # Extract from binary_model_input (black text, white bg)\n",
    "        letter_roi = binary_model_input[y:y+h, x:x+w]\n",
    "        extracted_letters_viz.append((f\"L{i}\", cv2.cvtColor(letter_roi, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "        letter_pil = Image.fromarray(letter_roi).convert('L')\n",
    "        \n",
    "        target_s = 64 - spacing\n",
    "        if w > h: nw, nh = target_s, int((h/w)*target_s)\n",
    "        else: nh, nw = target_s, int((w/h)*target_s)\n",
    "        \n",
    "        resized = letter_pil.resize((nw, nh), Image.LANCZOS)\n",
    "        padded = Image.new('L', (target_s, target_s), 255) # 255 for white background\n",
    "        px, py = (target_s - nw)//2, (target_s - nh)//2\n",
    "        padded.paste(resized, (px, py))\n",
    "\n",
    "        final_img_for_model = Image.new('L', (64,64), 255)\n",
    "        spacing_pad = spacing//2\n",
    "        final_img_for_model.paste(padded, (spacing_pad, spacing_pad))\n",
    "        model_inputs_viz.append((f\"M{i}\", cv2.cvtColor(np.array(final_img_for_model), cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "        # Transform for the model (aligns with recognition pipeline: Grayscale -> ToTensor -> Repeat Channels -> Normalize)\n",
    "        # The image is already grayscale. The key is the normalization and channel repeat.\n",
    "        current_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x_tensor: x_tensor.repeat(3,1,1) if x_tensor.size(0)==1 else x_tensor), # Ensure 3 channels\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet norm\n",
    "        ])\n",
    "        img_tensor = current_transform(final_img_for_model).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "            char_name = class_list[predicted_idx.item()]\n",
    "            letters_info_for_text.append((char_name, confidence.item(), (x,y,w,h)))\n",
    "\n",
    "    if extracted_letters_viz:\n",
    "        visualization_images.append((\"Extracted Segments\", create_image_grid_gradio([img for _, img in extracted_letters_viz], \"Extracted Segments\")))\n",
    "    if model_inputs_viz:\n",
    "        visualization_images.append((\"Processed for Model\", create_image_grid_gradio([img for _, img in model_inputs_viz], \"Processed for Model\")))\n",
    "\n",
    "    final_pred_img_viz = image.copy() # Original color image\n",
    "    detected_text_output = \"\"\n",
    "    for char, conf, (x,y,w,h) in letters_info_for_text:\n",
    "        cv2.rectangle(final_pred_img_viz, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "        cv2.putText(final_pred_img_viz, f\"{char} ({conf:.2f})\", (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255),2)\n",
    "        detected_text_output += char\n",
    "    visualization_images.append((\"Predictions on Image\", final_pred_img_viz))\n",
    "\n",
    "    return visualization_images, detected_text_output\n",
    "\n",
    "def create_image_grid_gradio(images_list, title):\n",
    "    if not images_list:\n",
    "        return None\n",
    "    cols = 5\n",
    "    rows = (len(images_list) + cols -1) // cols\n",
    "    fig_w, fig_h = cols*2, rows*2\n",
    "    \n",
    "    # Check if running in a headless environment for Matplotlib\n",
    "    if os.environ.get('DISPLAY','') == '' and os.name != 'posix':\n",
    "        plt.switch_backend('Agg') # Use Agg backend if no display available (common in some server environments)\n",
    "        print(\"Matplotlib backend switched to Agg for non-interactive plotting.\")\n",
    "        \n",
    "    fig = plt.figure(figsize=(fig_w, fig_h))\n",
    "    plt.suptitle(title)\n",
    "    for i, img_data in enumerate(images_list):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        if isinstance(img_data, tuple):\n",
    "            img_title, img = img_data\n",
    "            ax.set_title(img_title)\n",
    "        else:\n",
    "            img = img_data\n",
    "        cmap = 'gray' if len(img.shape) == 2 else None\n",
    "        ax.imshow(img, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(rect=[0,0,1,0.95])\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    grid_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    grid_img = grid_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    return grid_img\n",
    "\n",
    "def gradio_interface_combined(loaded_model_for_gradio, class_names_list_for_gradio):\n",
    "    def handle_sketch(sketch_img):\n",
    "        if sketch_img is None:\n",
    "            return [], \"Please draw something or upload an image.\"\n",
    "        # Sketchpad provides RGB numpy array. process_drawn_image expects this.\n",
    "        return process_drawn_image(sketch_img, loaded_model_for_gradio, class_names_list_for_gradio)\n",
    "    \n",
    "    def handle_upload(uploaded_img):\n",
    "        if uploaded_img is None:\n",
    "            return [], \"Please upload an image or draw something.\"\n",
    "        # Image upload provides PIL Image. process_drawn_image expects numpy array.\n",
    "        return process_drawn_image(np.array(uploaded_img), loaded_model_for_gradio, class_names_list_for_gradio)\n",
    "\n",
    "    with gr.Blocks(title=\"Handwritten Character Recognition\") as demo:\n",
    "        gr.Markdown(\"## Handwritten Character Recognition\")\n",
    "        gr.Markdown(\"Draw characters/words in the sketchpad OR upload an image. The system will try to recognize them.\")\n",
    "        \n",
    "        if loaded_model_for_gradio is None:\n",
    "            gr.Markdown(\"**WARNING: No model loaded. Predictions will not work. Please train a model and ensure checkpoints are available.**\")\n",
    "        else:\n",
    "             gr.Markdown(f\"**Using Model: {selected_model_type_for_gradio}** with {len(class_names_list_for_gradio)} classes.\")\n",
    "\n",
    "        with gr.Tab(\"Draw Text\"):\n",
    "            with gr.Row():\n",
    "                sketch_input = gr.Sketchpad(label=\"Draw Here\", type=\"numpy\", image_mode=\"RGB\", invert_colors=False, shape=(600,200))\n",
    "            sketch_button = gr.Button(\"Recognize Drawing\")\n",
    "        \n",
    "        with gr.Tab(\"Upload Image\"):\n",
    "            with gr.Row():\n",
    "                upload_input = gr.Image(label=\"Upload Image\", type=\"pil\", image_mode=\"RGB\")\n",
    "            upload_button = gr.Button(\"Recognize Uploaded Image\")\n",
    "\n",
    "        with gr.Row():\n",
    "            recognized_text_output = gr.Textbox(label=\"Recognized Text\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            gallery_output = gr.Gallery(label=\"Processing Steps & Results\", columns=[3], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "\n",
    "        sketch_button.click(fn=handle_sketch, inputs=sketch_input, outputs=[gallery_output, recognized_text_output])\n",
    "        upload_button.click(fn=handle_upload, inputs=upload_input, outputs=[gallery_output, recognized_text_output])\n",
    "        \n",
    "        gr.Markdown(\"### Notes:\")\n",
    "        gr.Markdown(\"- For best results, write clearly. Black text on white background is preferred for uploads.\")\n",
    "        gr.Markdown(\"- Segmentation of characters is a challenging step. Results may vary based on handwriting style and image quality.\")\n",
    "        gr.Markdown(\"- The 'Processing Steps & Results' gallery shows how the image is analyzed.\")\n",
    "        \n",
    "    return demo\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface = gradio_interface_combined(gradio_model_loaded, class_names_gradio)\n",
    "iface.launch(share=True, debug=True) # share=True to get a public link if running in Colab/cloud\n",
    "\n",
    "# MERGED FROM handwriting_inference.ipynb: The following is the highly detailed extract_letters function\n",
    "# from handwriting_inference.ipynb, renamed to avoid conflict with any simpler segmentation\n",
    "# that might be used directly within the Gradio processing functions.\n",
    "# This function is excellent for detailed, step-by-step visualization and debugging of character segmentation.\n",
    "def extract_letters_detailed_visualization(image_path, model, class_list, output_dir=\"extracted_letters_viz\", spacing=0):\n",
    "    \"\"\"\n",
    "    Extract individual letters from a handwritten text image and classify them.\n",
    "    Uses two different thresholding methods:\n",
    "    1. Inverted OTSU thresholding for contour detection\n",
    "    2. Non-inverted adaptive thresholding for model input images\n",
    "    Also includes extensive visualization steps, saving intermediate images.\n",
    "    The 'model' parameter should be the pre-loaded model for inference.\n",
    "    The 'class_list' provides the mapping from prediction index to character label.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Read and process the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
    "\n",
    "    # Visualize original image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.savefig(f\"{output_dir}/1_original.png\")\n",
    "    # plt.show() # Commented out plt.show() for non-interactive consolidation\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Visualize grayscale image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(gray, cmap='gray')\n",
    "    plt.title(\"Grayscale Image\")\n",
    "    plt.savefig(f\"{output_dir}/2_grayscale.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Apply OTSU thresholding (inverted) for contour detection\n",
    "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    # Visualize binary image for contour detection\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(binary_contours, cmap='gray')\n",
    "    plt.title(\"Binary Image for Contours (Inverted OTSU)\")\n",
    "    plt.savefig(f\"{output_dir}/3a_binary_contours.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Binary image for model input (black text on white background)\n",
    "    _, binary_model_prep = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # Visualize binary image for model input\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(binary_model_prep, cmap='gray')\n",
    "    plt.title(\"Binary Image for Model Input (Non-inverted OTSU)\")\n",
    "    plt.savefig(f\"{output_dir}/3b_binary_model_prep.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Find contours on the inverted binary image (for contour detection)\n",
    "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Visualize contours\n",
    "    contour_img = image.copy()\n",
    "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"All Contours Found: {len(contours)}\")\n",
    "    plt.savefig(f\"{output_dir}/4_all_contours.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Filter small contours\n",
    "    min_area = 50\n",
    "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    print(f\"Found {len(contours)} total contours, {len(letter_contours)} after filtering by min area {min_area}\")\n",
    "\n",
    "    # Visualize filtered contours\n",
    "    filtered_contour_img = image.copy()\n",
    "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Filtered Contours: {len(letter_contours)}\")\n",
    "    plt.savefig(f\"{output_dir}/5_filtered_contours.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    if not letter_contours:\n",
    "        print(\"No letter contours found after filtering.\")\n",
    "        return []\n",
    "\n",
    "    # Sort contours from left to right (for reading order)\n",
    "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
    "\n",
    "    # Create a figure to show all extracted letters before final processing for model\n",
    "    num_letters_viz = len(letter_contours)\n",
    "    fig_rows_viz = max(1, (num_letters_viz + 4) // 5)  # 5 letters per row\n",
    "    fig_cols_viz = min(5, num_letters_viz)\n",
    "\n",
    "    if num_letters_viz > 0:\n",
    "        plt.figure(figsize=(15, 3 * fig_rows_viz))\n",
    "        plt.suptitle(\"Extracted ROIs Before Processing for Model\", fontsize=16)\n",
    "        for i_viz, contour_viz in enumerate(letter_contours):\n",
    "            x_viz, y_viz, w_viz, h_viz = cv2.boundingRect(contour_viz)\n",
    "            letter_image_viz = binary_model_prep[y_viz:y_viz+h_viz, x_viz:x_viz+w_viz]\n",
    "            if i_viz < fig_rows_viz * fig_cols_viz:\n",
    "                plt.subplot(fig_rows_viz, fig_cols_viz, i_viz + 1)\n",
    "                plt.imshow(letter_image_viz, cmap='gray')\n",
    "                plt.title(f\"ROI {i_viz}\")\n",
    "                plt.axis('off')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f\"{output_dir}/6_extracted_rois.png\")\n",
    "        # plt.show()\n",
    "\n",
    "    letters_info_final = []\n",
    "    model_input_images_viz = []\n",
    "\n",
    "    for i, contour in enumerate(letter_contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w < 5 or h < 5: continue # Skip very small artifacts\n",
    "\n",
    "        # Extract letter from the non-inverted binary image (binary_model_prep)\n",
    "        letter_image_roi = binary_model_prep[y:y+h, x:x+w]\n",
    "        \n",
    "        letter_image_pil = Image.fromarray(letter_image_roi).convert('L')\n",
    "\n",
    "        target_s = 64 - spacing # Target size for the character within the 64x64 image\n",
    "        if w > h:\n",
    "            new_w, new_h = target_s, int((h / w) * target_s)\n",
    "        else:\n",
    "            new_h, new_w = target_s, int((w / h) * target_s)\n",
    "        \n",
    "        resized_image = letter_image_pil.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # Create a new square image with white background (255)\n",
    "        padded_image = Image.new('L', (target_s, target_s), 255)\n",
    "        pad_x = (target_s - new_w) // 2\n",
    "        pad_y = (target_s - new_h) // 2\n",
    "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
    "\n",
    "        # Create the final 64x64 image, pasting the padded character with specified spacing\n",
    "        final_model_input_image = Image.new('L', (64, 64), 255)\n",
    "        spacing_offset = spacing // 2\n",
    "        final_model_input_image.paste(padded_image, (spacing_offset, spacing_offset))\n",
    "\n",
    "        # Store image for visualization after processing\n",
    "        model_input_images_viz.append(np.array(final_model_input_image))\n",
    "\n",
    "        # Transform for the model (aligns with recognition pipeline: Grayscale -> ToTensor -> Repeat Channels -> Normalize)\n",
    "        # This matches the `prepare_image` or `gradio_transform` in the main notebook\n",
    "        inference_transform_detailed = transforms.Compose([\n",
    "            transforms.ToTensor(), # Converts PIL (H,W,1) or (H,W) to (1,H,W)\n",
    "            transforms.Lambda(lambda x_tensor: x_tensor.repeat(3, 1, 1) if x_tensor.size(0)==1 else x_tensor), # Ensure 3 channels\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet norm\n",
    "        ])\n",
    "        letter_tensor = inference_transform_detailed(final_model_input_image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(letter_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence_val, predicted_idx = torch.max(probabilities, 1)\n",
    "            detected_char = class_list[predicted_idx.item()]\n",
    "            print(f\"Letter {i}: Classified as '{detected_char}' with confidence {confidence_val.item():.2f}\")\n",
    "\n",
    "        letters_info_final.append((detected_char, (x, y, w, h)))\n",
    "\n",
    "    # Visualize the final processed images that are fed to the model\n",
    "    if model_input_images_viz:\n",
    "        plt.figure(figsize=(15, 3 * fig_rows_viz)) # Re-use fig_rows_viz for consistency\n",
    "        plt.suptitle(\"Processed Images Fed to Model (64x64 with spacing)\", fontsize=16)\n",
    "        for i_viz, img_viz in enumerate(model_input_images_viz):\n",
    "            if i_viz < fig_rows_viz * fig_cols_viz:\n",
    "                ax = plt.subplot(fig_rows_viz, fig_cols_viz, i_viz + 1)\n",
    "                plt.imshow(img_viz, cmap='gray')\n",
    "                # Add border visualization\n",
    "                rect_outer = patches.Rectangle((0,0),63,63,linewidth=1,edgecolor='r',facecolor='none')\n",
    "                ax.add_patch(rect_outer)\n",
    "                inner_size = 64 - spacing -1\n",
    "                spacing_half_viz = spacing // 2\n",
    "                rect_inner = patches.Rectangle((spacing_half_viz,spacing_half_viz),inner_size,inner_size,linewidth=1,edgecolor='b',facecolor='none')\n",
    "                ax.add_patch(rect_inner)\n",
    "                plt.title(f\"Model In {i_viz}: {letters_info_final[i_viz][0]}\")\n",
    "                plt.axis('off')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f\"{output_dir}/7_model_inputs_with_border.png\")\n",
    "        # plt.show()\n",
    "\n",
    "    # Draw bounding boxes on original image\n",
    "    result_img_final = image.copy()\n",
    "    for char_viz, (x_viz, y_viz, w_viz, h_viz) in letters_info_final:\n",
    "        cv2.rectangle(result_img_final, (x_viz, y_viz), (x_viz + w_viz, y_viz + h_viz), (0, 255, 0), 2)\n",
    "        cv2.putText(result_img_final, char_viz, (x_viz, y_viz - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(result_img_final, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Final Result with Classifications\")\n",
    "    plt.savefig(f\"{output_dir}/8_final_result.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    return letters_info_final\n",
    "\n",
    "# Example of how to call this detailed visualization function (assuming 'model' and 'class_names_gradio' are loaded):\n",
    "# if gradio_model_loaded and class_names_gradio:\n",
    "#     print(\"Running detailed letter extraction and visualization example...\")\n",
    "#     # Create a dummy image path or use an actual one for testing\n",
    "#     example_image_path_for_detail_viz = \"./datasets/handwritten-english/example_image.png\" \n",
    "#     # Ensure this image exists or point to a valid one. For example, one of the test images from your dataset.\n",
    "#     # if not os.path.exists(example_image_path_for_detail_viz):\n",
    "#     #     # Create a dummy image if it doesn't exist (e.g., using OpenCV)\n",
    "#     #     dummy_image_detailed = np.full((100, 400, 3), 255, dtype=np.uint8) # White background\n",
    "#     #     cv2.putText(dummy_image_detailed, \"Test Text\", (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,0), 3) # Black text\n",
    "#     #     cv2.imwrite(example_image_path_for_detail_viz, dummy_image_detailed)\n",
    "#     #     print(f\"Created dummy image: {example_image_path_for_detail_viz}\")\n",
    "        \n",
    "#     # if os.path.exists(example_image_path_for_detail_viz):\n",
    "#     #    extract_letters_detailed_visualization(example_image_path_for_detail_viz, gradio_model_loaded, class_names_gradio, spacing=12)\n",
    "#     # else:\n",
    "#     #    print(f\"Example image for detailed visualization not found at: {example_image_path_for_detail_viz}\")\n",
    "# else:\n",
    "#     print(\"Gradio model or class names not loaded, skipping detailed visualization example.\")\n",
    "\n",
    "\n",
    "# NOTE on VGG19 model version for inference:\n",
    "# handwriting_inference.ipynb defines and loads a VGG19HandwritingModel that is modified for 1-channel input.\n",
    "# The main consolidated notebook currently uses a VGG19 that expects 3-channel input (as prepared by its data pipeline).\n",
    "# If using the VGG model from this consolidated notebook for the extract_letters_detailed_visualization or Gradio,\n",
    "# ensure the image preprocessing aligns (i.e., provides 3-channel tensors).\n",
    "# The `inference_transform_detailed` within `extract_letters_detailed_visualization` is set up for 3-channel input.\n",
    "\n",
    "# Note on `prepare_image` function for inference:\n",
    "# The `handwriting_recognition.ipynb` (now in `temp_consolidated.ipynb`) has a `prepare_image` function\n",
    "# that takes `image_path` and `image_size`, converts to grayscale, then to 3-channel tensor with ImageNet normalization.\n",
    "# The `handwriting_inference.ipynb` also has a `prepare_image` that takes `image_path` (no size), converts to grayscale,\n",
    "# and then to a 1-channel tensor with (0.5,) normalization. \n",
    "# The version in `temp_consolidated.ipynb` is currently retained for general single image inference, \n",
    "# as it aligns with the 3-channel models (like the current VGG19 version and ImprovedLetterCNN).\n",
    "\n",
    "# Final check on Gradio interface logic:\n",
    "# The Gradio interface merged from `handwriting_training.ipynb` (`gradio_interface_combined` and its helpers\n",
    "# `process_drawn_image`, `create_image_grid_gradio`) is quite comprehensive.\n",
    "# `handwriting_inference.ipynb` also had Gradio functions (`process_uploaded_image`, `create_combined_image`, `launch_interface`).\n",
    "# The current `gradio_interface_combined` already handles both sketch and upload.\n",
    "# The key is to ensure that `process_drawn_image` (which handles both sketch and upload now after adaptation)\n",
    "# correctly preprocesses images for the loaded Gradio model (which is 3-channel as per current setup).\n",
    "# The `current_transform` inside `process_drawn_image` has been aligned with this.\n"
   ]
  }
 ],
 "metadata": {
