{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition - Inference\n",
    "\n",
    "This notebook demonstrates how to use trained models for handwritten character recognition. It covers:\n",
    "- Loading pre-trained model weights.\n",
    "- Preparing single images for inference.\n",
    "- Performing predictions on single images.\n",
    "- A detailed function for extracting and visualizing characters from an image of text, along with their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import random\n",
    "import copy # For load_model\n",
    "import time # For load_model (though not strictly used in inference part)\n",
    "\n",
    "# Image processing and display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 # OpenCV for image operations\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch essentials\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim # For load_model compatibility\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets # For get_class_labels_from_dir\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from pathlib import Path \n",
    "import matplotlib.patches as patches # For detailed visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "The definitions of the models are needed to load the saved weights. These definitions must match the ones used during training. The following models expect 3-channel input images as prepared by the data pipeline in `training.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Custom CNNs (`LetterCNN64`, `ImprovedLetterCNN`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterCNN64(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LetterCNN64, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImprovedLetterCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ImprovedLetterCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = self.dropout1(self.relu_fc1(self.bn_fc1(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu_fc2(self.bn_fc2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. VGG19 Transfer Learning Model (`VGG19HandwritingModel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19HandwritingModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, pretrained=True):\n",
    "        super(VGG19HandwritingModel, self).__init__()\n",
    "        self.device = device\n",
    "        vgg19 = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        vgg19 = vgg19.to(device)\n",
    "        self.features = vgg19.features\n",
    "        # Note: This model definition assumes 3-channel input as prepared by the pipeline.\n",
    "        # If using a model trained with 1-channel input VGG, the first conv layer would need modification here too.\n",
    "        if pretrained:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_features_output = 512 * 2 * 2 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features_output, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        ).to(device)\n",
    "        self._initialize_weights() # From merged code, good practice\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_inference(model, checkpoint_path):\n",
    "    \"\"\"Loads a model checkpoint for inference. Optimizer and scheduler states are ignored if not present.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"ERROR: Checkpoint path {checkpoint_path} does not exist. Returning initial model.\")\n",
    "        return None # Return None if checkpoint not found\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # Fallback for checkpoints that are just the state_dict itself\n",
    "            model.load_state_dict(checkpoint)\n",
    "        model.eval() # IMPORTANT: Set model to evaluation mode\n",
    "        print(f\"Model loaded successfully from {checkpoint_path} and set to evaluation mode.\")\n",
    "        # Optionally print other info from checkpoint if useful for context\n",
    "        if 'epoch' in checkpoint: print(f\"  Checkpoint saved at epoch: {checkpoint['epoch']}\")\n",
    "        if 'accuracy' in checkpoint: print(f\"  Checkpoint validation accuracy (if saved): {checkpoint['accuracy']:.4f}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading model from {checkpoint_path}: {e}\")\n",
    "        return None # Return None on error\n",
    "\n",
    "def get_class_labels_from_dir(data_root_dir):\n",
    "    \"\"\"Gets class labels from the folder names in data_root_dir (ImageFolder structure).\"\"\"\n",
    "    if not os.path.exists(data_root_dir) or not os.path.isdir(data_root_dir):\n",
    "        print(f\"Error: Data root directory '{data_root_dir}' not found or not a directory.\")\n",
    "        return []\n",
    "    try:\n",
    "        # Create a temporary ImageFolder dataset just to get class names\n",
    "        # This assumes that subdirectories in data_root_dir are the class names\n",
    "        temp_dataset = datasets.ImageFolder(root=data_root_dir)\n",
    "        return temp_dataset.classes\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting class labels from '{data_root_dir}': {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_image_for_inference(image_path, image_size=(64, 64)):\n",
    "    \"\"\"Loads an image, converts to grayscale, resizes, and prepares it for 3-channel model inference.\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image path '{image_path}' not found.\")\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('L') # Convert to grayscale\n",
    "        # Normalization parameters should be consistent with training (ImageNet stats for 3-channel models here)\n",
    "        normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        inference_transform_pipeline = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), # Ensure 3 channels\n",
    "            normalize_transform\n",
    "        ])\n",
    "        img_tensor = inference_transform_pipeline(img)\n",
    "        return img_tensor.unsqueeze(0).to(device) # Add batch dimension and send to device\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single Image Inference Example\n",
    "\n",
    "This section demonstrates loading a trained model and performing inference on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Single Image Inference Example ---\")\n",
    "\n",
    "# --- USER CONFIGURATION REQUIRED --- #\n",
    "# 1. Path to the saved model checkpoint (e.g., from './model_checkpoints/cnn/best_model.pth')\n",
    "user_model_checkpoint_path = \"./model_checkpoints/cnn/best_model.pth\"  # <<< UPDATE THIS PATH\n",
    "\n",
    "# 2. Path to the root directory of the dataset used for training (to get class labels)\n",
    "#    (e.g., './datasets/handwritten-english/augmented_images/augmented_images1')\n",
    "user_data_root_for_labels = \"./datasets/handwritten-english/augmented_images/augmented_images1\"  # <<< UPDATE THIS PATH\n",
    "\n",
    "# 3. Path to the image you want to classify\n",
    "user_image_for_prediction = \"\"  # <<< UPDATE THIS PATH (e.g., \"./path/to/your/image.png\")\n",
    "# --- END OF USER CONFIGURATION --- #\n",
    "\n",
    "loaded_inference_model = None\n",
    "class_labels_for_inference = []\n",
    "\n",
    "if not os.path.exists(user_model_checkpoint_path):\n",
    "    print(f\"ERROR: Model checkpoint '{user_model_checkpoint_path}' not found. Please update the path.\")\n",
    "elif not os.path.exists(user_data_root_for_labels):\n",
    "    print(f\"ERROR: Data root for labels '{user_data_root_for_labels}' not found. Please update the path.\")\n",
    "else:\n",
    "    class_labels_for_inference = get_class_labels_from_dir(user_data_root_for_labels)\n",
    "    if not class_labels_for_inference:\n",
    "        print(\"ERROR: Could not retrieve class labels.\")\n",
    "    else:\n",
    "        num_classes = len(class_labels_for_inference)\n",
    "        print(f\"Successfully loaded {num_classes} class labels. Example: {class_labels_for_inference[:5]}\")\n",
    "        \n",
    "        # Initialize model architecture - this MUST match the checkpoint's model type\n",
    "        # Assuming ImprovedLetterCNN for this example. Change if your checkpoint is for a different model.\n",
    "        # E.g., for VGG19: model_architecture = VGG19HandwritingModel(num_classes, device, pretrained=False).to(device)\n",
    "        model_architecture = ImprovedLetterCNN(num_classes).to(device)\n",
    "        \n",
    "        loaded_inference_model = load_model_for_inference(model_architecture, user_model_checkpoint_path)\n",
    "\n",
    "if loaded_inference_model and class_labels_for_inference:\n",
    "    if not user_image_for_prediction or not os.path.exists(user_image_for_prediction):\n",
    "        print(f\"INFO: 'user_image_for_prediction' ('{user_image_for_prediction}') is not set or does not exist.\")\n",
    "        print(\"Please provide a valid image path to perform inference.\")\n",
    "    else:\n",
    "        input_image_tensor = prepare_image_for_inference(user_image_for_prediction)\n",
    "        if input_image_tensor is not None:\n",
    "            with torch.no_grad():\n",
    "                outputs = loaded_inference_model(input_image_tensor)\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                predicted_label = class_labels_for_inference[predicted_idx.item()]\n",
    "                confidence_percent = confidence.item() * 100\n",
    "\n",
    "                print(f\"\\n--- Prediction Results for: {user_image_for_prediction} ---\")\n",
    "                print(f\"Predicted Class: {predicted_label}\")\n",
    "                print(f\"Confidence: {confidence_percent:.2f}%\")\n",
    "\n",
    "                try:\n",
    "                    img_to_display = Image.open(user_image_for_prediction)\n",
    "                    plt.figure(figsize=(3,3))\n",
    "                    plt.imshow(img_to_display)\n",
    "                    plt.title(f\"Predicted: {predicted_label} ({confidence_percent:.2f}%)\")\n",
    "                    plt.axis('off'); plt.show()\n",
    "                except Exception as e_disp: print(f\"Error displaying image: {e_disp}\")\n",
    "        else:\n",
    "            print(f\"Could not prepare image '{user_image_for_prediction}' for inference.\")\n",
    "else:\n",
    "    print(\"Cannot proceed with single image inference due to model or class label loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Multi-Character Image Analysis\n",
    "\n",
    "The `extract_letters_detailed_visualization` function segments characters from an image, classifies each segment, and visualizes the intermediate processing steps. This is useful for understanding the segmentation and classification process in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_letters_detailed_visualization(image_path, model, class_list, output_dir=\"extracted_letters_viz\", spacing=0):\n",
    "    \"\"\"\n",
    "    Extract individual letters from a handwritten text image and classify them.\n",
    "    Includes extensive visualization steps, saving intermediate images.\n",
    "    The 'model' parameter should be the pre-loaded model for inference.\n",
    "    The 'class_list' provides the mapping from prediction index to character label.\n",
    "    \"\"\"\n",
    "    if model is None: print(\"Model not provided to extract_letters_detailed_visualization.\"); return []\n",
    "    if not class_list: print(\"Class list not provided to extract_letters_detailed_visualization.\"); return []\n",
    "    \n",
    "    model.eval() \n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None: raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
    "\n",
    "    print(f\"Visualizing steps for {image_path}, outputting to {output_dir}/\")\n",
    "    # Step 1: Original Image\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)); plt.title(\"Original Image\"); plt.savefig(f\"{output_dir}/1_original.png\"); plt.show()\n",
    "\n",
    "    # Step 2: Grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(gray, cmap='gray'); plt.title(\"Grayscale Image\"); plt.savefig(f\"{output_dir}/2_grayscale.png\"); plt.show()\n",
    "\n",
    "    # Step 3a: Binary for Contours (Inverted OTSU)\n",
    "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(binary_contours, cmap='gray'); plt.title(\"Binary for Contours (Inverted OTSU)\"); plt.savefig(f\"{output_dir}/3a_binary_contours.png\"); plt.show()\n",
    "\n",
    "    # Step 3b: Binary for Model Input (Non-inverted OTSU)\n",
    "    _, binary_model_prep = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(binary_model_prep, cmap='gray'); plt.title(\"Binary for Model Input (Non-inverted OTSU)\"); plt.savefig(f\"{output_dir}/3b_binary_model_prep.png\"); plt.show()\n",
    "\n",
    "    # Step 4: Find & Visualize All Contours\n",
    "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_img = image.copy(); cv2.drawContours(contour_img, contours, -1, (0,255,0), 2)\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)); plt.title(f\"All Contours ({len(contours)})\"); plt.savefig(f\"{output_dir}/4_all_contours.png\"); plt.show()\n",
    "\n",
    "    # Step 5: Filter & Visualize Filtered Contours\n",
    "    min_area = 50\n",
    "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    print(f\"Found {len(contours)} total contours, {len(letter_contours)} after filtering by min area {min_area}\")\n",
    "    filtered_contour_img = image.copy(); cv2.drawContours(filtered_contour_img, letter_contours, -1, (0,255,0), 2)\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)); plt.title(f\"Filtered Contours ({len(letter_contours)})\"); plt.savefig(f\"{output_dir}/5_filtered_contours.png\"); plt.show()\n",
    "\n",
    "    if not letter_contours: print(\"No letter contours after filtering.\"); return []\n",
    "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
    "\n",
    "    # Step 6: Visualize Extracted ROIs before processing\n",
    "    num_letters_viz = len(letter_contours)\n",
    "    if num_letters_viz > 0:\n",
    "        fig_rows_viz = max(1, (num_letters_viz + 4) // 5); fig_cols_viz = min(5, num_letters_viz)\n",
    "        plt.figure(figsize=(15, 3 * fig_rows_viz)); plt.suptitle(\"Extracted ROIs Before Processing\", fontsize=16)\n",
    "        for i_viz, c_viz in enumerate(letter_contours):\n",
    "            x_viz, y_viz, w_viz, h_viz = cv2.boundingRect(c_viz)\n",
    "            roi_viz = binary_model_prep[y_viz:y_viz+h_viz, x_viz:x_viz+w_viz]\n",
    "            if i_viz < fig_rows_viz*fig_cols_viz: plt.subplot(fig_rows_viz,fig_cols_viz,i_viz+1); plt.imshow(roi_viz,cmap='gray'); plt.title(f\"ROI {i_viz}\"); plt.axis('off')\n",
    "        plt.tight_layout(rect=[0,0,1,0.95]); plt.savefig(f\"{output_dir}/6_extracted_rois.png\"); plt.show()\n",
    "\n",
    "    letters_info_final = []\n",
    "    model_input_images_viz = []\n",
    "    # This transform should match prepare_image_for_inference's core logic for 3-channel models\n",
    "    inference_transform_detailed = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x_tensor: x_tensor.repeat(3,1,1) if x_tensor.size(0)==1 else x_tensor),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    for i, contour in enumerate(letter_contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w < 5 or h < 5: continue\n",
    "        letter_roi = binary_model_prep[y:y+h, x:x+w]\n",
    "        letter_pil = Image.fromarray(letter_roi).convert('L')\n",
    "        target_s = 64-spacing\n",
    "        if w > h: new_w,new_h = target_s,int((h/w)*target_s)\n",
    "        else: new_h,new_w = target_s,int((w/h)*target_s)\n",
    "        resized = letter_pil.resize((new_w,new_h), Image.LANCZOS)\n",
    "        padded = Image.new('L', (target_s,target_s), 255) # White background\n",
    "        px,py = (target_s-new_w)//2, (target_s-new_h)//2\n",
    "        padded.paste(resized, (px,py))\n",
    "        final_img = Image.new('L', (64,64), 255); spacing_offset=spacing//2\n",
    "        final_img.paste(padded, (spacing_offset,spacing_offset))\n",
    "        model_input_images_viz.append(np.array(final_img))\n",
    "        letter_tensor = inference_transform_detailed(final_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(letter_tensor)\n",
    "            probabs = torch.nn.functional.softmax(outputs,dim=1)\n",
    "            conf,pred_idx = torch.max(probabs,1)\n",
    "            char = class_list[pred_idx.item()]\n",
    "            print(f\"Segment {i}: Classified as '{char}' with conf {conf.item():.2f}\")\n",
    "        letters_info_final.append((char, (x,y,w,h)))\n",
    "\n",
    "    # Step 7: Visualize Model Inputs with Borders\n",
    "    if model_input_images_viz:\n",
    "        fig_rows_viz = max(1, (len(model_input_images_viz) + 4) // 5); fig_cols_viz = min(5, len(model_input_images_viz))\n",
    "        plt.figure(figsize=(15, 3*fig_rows_viz)); plt.suptitle(\"Processed Segments for Model\", fontsize=16)\n",
    "        for i_viz, img_viz in enumerate(model_input_images_viz):\n",
    "            if i_viz < fig_rows_viz*fig_cols_viz: \n",
    "                ax=plt.subplot(fig_rows_viz,fig_cols_viz,i_viz+1); plt.imshow(img_viz,cmap='gray')\n",
    "                rect_o=patches.Rectangle((0,0),63,63,lw=1,edgecolor='r',fc='none'); ax.add_patch(rect_o)\n",
    "                in_s=64-spacing-1; sp_h=spacing//2\n",
    "                rect_i=patches.Rectangle((sp_h,sp_h),in_s,in_s,lw=1,edgecolor='b',fc='none'); ax.add_patch(rect_i)\n",
    "                plt.title(f\"Input {i_viz}: {letters_info_final[i_viz][0]}\"); plt.axis('off')\n",
    "        plt.tight_layout(rect=[0,0,1,0.95]); plt.savefig(f\"{output_dir}/7_model_inputs_bordered.png\"); plt.show()\n",
    "\n",
    "    # Step 8: Final Result Visualization\n",
    "    result_img = image.copy()\n",
    "    for char, (x,y,w,h) in letters_info_final:\n",
    "        cv2.rectangle(result_img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "        cv2.putText(result_img, char, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)\n",
    "    plt.figure(figsize=(10,5)); plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)); plt.title(\"Final Classification on Image\"); plt.savefig(f\"{output_dir}/8_final_classification.png\"); plt.show()\n",
    "    return letters_info_final\n",
    "\n",
    "# --- Example Usage for Detailed Multi-Character Extraction ---\n",
    "print(\"\\n--- Detailed Multi-Character Image Analysis Example ---\")\n",
    "user_multi_char_image_path = \"\"  # <<< USER: PROVIDE PATH TO AN IMAGE WITH MULTIPLE CHARACTERS\n",
    "\n",
    "if not user_multi_char_image_path or not os.path.exists(user_multi_char_image_path):\n",
    "    print(f\"INFO: 'user_multi_char_image_path' ('{user_multi_char_image_path}') is not set or does not exist.\")\n",
    "    print(\"Skipping detailed multi-character extraction example. Provide a valid image path to run this.\")\n",
    "elif not loaded_inference_model: # Check if a model was successfully loaded in the previous section\n",
    "    print(\"ERROR: Model not loaded from single image inference section. Cannot run detailed extraction.\")\n",
    "    print(f\"Ensure '{user_model_checkpoint_path}' is a valid model path and was loaded successfully.\")\n",
    "elif not class_labels_for_inference:\n",
    "     print(\"ERROR: Class labels not loaded. Cannot run detailed extraction.\")\n",
    "else:\n",
    "    print(f\"Running detailed extraction for image: {user_multi_char_image_path}\")\n",
    "    # We use the 'loaded_inference_model' and 'class_labels_for_inference' from the single image example section.\n",
    "    # Ensure that 'loaded_inference_model' is the model you intend to use for this detailed analysis.\n",
    "    detailed_extracted_info = extract_letters_detailed_visualization(\n",
    "        user_multi_char_image_path, \n",
    "        loaded_inference_model, \n",
    "        class_labels_for_inference, \n",
    "        output_dir=\"./detailed_extraction_output\", # You can change the output directory\n",
    "        spacing=12 # Adjust spacing around characters if needed\n",
    "    )\n",
    "    if detailed_extracted_info:\n",
    "        print(\"\\n--- Detailed Extraction Results ---\")\n",
    "        for char_info_item in detailed_extracted_info:\n",
    "            print(f\"Character: {char_info_item[0]}, Bounding Box: {char_info_item[1]}\")\n",
    "    else:\n",
    "        print(\"No characters were extracted or classified in the detailed analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
