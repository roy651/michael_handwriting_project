{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition - Enhanced Gradio Application\n",
    "\n",
    "This notebook launches an interactive Gradio application for handwritten character recognition. It allows users to select a model, and then either draw characters, upload an image, or use a webcam for character recognition. The application displays the recognized text and visualizes intermediate processing steps.\n",
    "\n",
    "**Prerequisites:**\n",
    "1.  **Trained Models:** Ensure you have trained models saved in the appropriate checkpoint directories (e.g., `./model_checkpoints/cnn/best_model.pth` or `./model_checkpoints/vgg/best_model.pth`). The `training.ipynb` notebook should produce these.\n",
    "2.  **Dataset for Class Labels:** The path to the root of the training dataset is needed to derive the class labels. Update `DATA_ROOT_FOR_LABELS` if your dataset is located elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import traceback\n",
    "import subprocess # For robust pip install\n",
    "\n",
    "# Image processing and display\n",
    "import matplotlib\n",
    "# Ensure a non-interactive backend is used if running in a headless environment for plt.figure creation for Gradio gallery\n",
    "if os.environ.get('DISPLAY','') == '' and os.name != 'posix':\n",
    "    print(\"Setting Matplotlib backend to 'Agg' for non-interactive environment.\")\n",
    "    matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps \n",
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "# PyTorch essentials\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from pathlib import Path \n",
    "import matplotlib.patches as patches \n",
    "\n",
    "# Gradio for the web application\n",
    "try:\n",
    "    import gradio as gr\n",
    "except ImportError:\n",
    "    print(\"Gradio not installed. Installing gradio==3.50.2...\") # Pinned version for stability\n",
    "    subprocess.check_call([\"pip\", \"install\", \"gradio==3.50.2\", \"-q\"])\n",
    "    import gradio as gr\n",
    "\n",
    "print(f\"Gradio version: {gr.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "import torchvision\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "These must match the definitions used during training to load the weights correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterCNN64(nn.Module):\n",
    "    def __init__(self, num_classes, device='cpu'): # Added device parameter\n",
    "        super(LetterCNN64, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.to(device) # Move model to device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImprovedLetterCNN(nn.Module):\n",
    "    def __init__(self, num_classes, device='cpu'): # Added device parameter\n",
    "        super(ImprovedLetterCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        self.to(device) # Move model to device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = self.dropout1(self.relu_fc1(self.bn_fc1(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu_fc2(self.bn_fc2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class VGG19HandwritingModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, pretrained=True):\n",
    "        super(VGG19HandwritingModel, self).__init__()\n",
    "        self.device = device \n",
    "        vgg19 = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        self.features = vgg19.features\n",
    "        if pretrained:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_features_output = 512 * 2 * 2 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features_output, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "        self.to(self.device) # Ensure the entire model is on the correct device\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_app(model_class_fn, num_classes, checkpoint_path, model_name_display, current_device):\n",
    "    \"\"\"Loads a specified model class for the Gradio app, ensuring it's on the correct device.\"\"\"\n",
    "    model_instance = model_class_fn(num_classes=num_classes, device=current_device)\n",
    "    # Model is moved to device within its __init__ method.\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"WARNING: Checkpoint for {model_name_display} not found at {checkpoint_path}. Model will be uninitialized.\")\n",
    "        return model_instance \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=current_device)\n",
    "        state_dict = checkpoint.get('model_state_dict', checkpoint) # Handle older checkpoints that saved the state_dict directly\n",
    "        model_instance.load_state_dict(state_dict)\n",
    "        model_instance.eval()\n",
    "        print(f\"{model_name_display} model loaded successfully from {checkpoint_path} and set to {current_device}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {model_name_display} from {checkpoint_path}: {e}. Model remains uninitialized or partially loaded.\")\n",
    "        traceback.print_exc()\n",
    "    return model_instance\n",
    "\n",
    "def get_class_labels_from_dir(data_root_dir):\n",
    "    if not os.path.exists(data_root_dir) or not os.path.isdir(data_root_dir):\n",
    "        print(f\"Warning: Data root for labels '{data_root_dir}' not found or not a directory.\")\n",
    "        return []\n",
    "    try:\n",
    "        temp_dataset = datasets.ImageFolder(root=data_root_dir)\n",
    "        if not temp_dataset.classes: print(f\"Warning: No classes found in '{data_root_dir}'. Check dataset structure.\")\n",
    "        return temp_dataset.classes\n",
    "    except Exception as e: print(f\"Error getting class labels from '{data_root_dir}': {e}\"); return []\n",
    "\n",
    "def prepare_char_roi_for_model(char_pil_image, image_size=(64,64), spacing=12):\n",
    "    \"\"\"Prepares a single character ROI (PIL Image, L mode) for 3-channel model input.\"\"\"\n",
    "    target_s = image_size[0] - spacing \n",
    "    w, h = char_pil_image.size\n",
    "    if w == 0 or h == 0: return None\n",
    "    new_w, new_h = (target_s, int((h/w) * target_s)) if w > h else (int((w/h) * target_s), target_s)\n",
    "    new_w, new_h = max(1, new_w), max(1, new_h)\n",
    "    resized = char_pil_image.resize((new_w, new_h), Image.LANCZOS)\n",
    "    padded = Image.new('L', (target_s, target_s), 255) # White background\n",
    "    px, py = (target_s - new_w) // 2, (target_s - new_h) // 2\n",
    "    padded.paste(resized, (px,py))\n",
    "    final_img = Image.new('L', image_size, 255); spacing_offset = spacing//2\n",
    "    final_img.paste(padded, (spacing_offset, spacing_offset))\n",
    "    transform_to_tensor = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0)==1 else x),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    return transform_to_tensor(final_img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio Application Setup and Core Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Models and Class Labels --- #\n",
    "MODEL_CKPT_DIR_CNN = './model_checkpoints/cnn/'\n",
    "MODEL_CKPT_DIR_VGG = './model_checkpoints/vgg/'\n",
    "DATA_ROOT_FOR_LABELS = \"./datasets/handwritten-english/augmented_images/augmented_images1\" # USER: Update if needed\n",
    "\n",
    "print(f\"Attempting to load class labels from: {DATA_ROOT_FOR_LABELS}\")\n",
    "app_class_labels = get_class_labels_from_dir(DATA_ROOT_FOR_LABELS)\n",
    "if not app_class_labels:\n",
    "    print(\"CRITICAL WARNING: Class labels could not be loaded. Using a fallback list of 62 common chars.\")\n",
    "    app_class_labels = ([str(i) for i in range(10)] + \n",
    "                        [chr(ord('A')+i) for i in range(26)] + \n",
    "                        [chr(ord('a')+i) for i in range(26)])\n",
    "app_num_classes = len(app_class_labels)\n",
    "print(f\"Loaded {app_num_classes} class labels. First 5: {app_class_labels[:5]}\")\n",
    "\n",
    "loaded_models_dict = {}\n",
    "available_model_names_list = []\n",
    "\n",
    "if app_num_classes > 0:\n",
    "    cnn_checkpoint_file = os.path.join(MODEL_CKPT_DIR_CNN, 'best_model.pth')\n",
    "    if os.path.exists(cnn_checkpoint_file):\n",
    "        cnn_model_instance = load_model_for_app(ImprovedLetterCNN, app_num_classes, cnn_checkpoint_file, \"ImprovedCNN\", device)\n",
    "        if cnn_model_instance: loaded_models_dict[\"ImprovedCNN\"] = cnn_model_instance; available_model_names_list.append(\"ImprovedCNN\")\n",
    "    else:\n",
    "        print(f\"Checkpoint for ImprovedCNN not found at {cnn_checkpoint_file}\")\n",
    "\n",
    "    vgg_checkpoint_file = os.path.join(MODEL_CKPT_DIR_VGG, 'best_model.pth')\n",
    "    if os.path.exists(vgg_checkpoint_file):\n",
    "        vgg_model_instance = load_model_for_app(VGG19HandwritingModel, app_num_classes, vgg_checkpoint_file, \"VGG19\", device)\n",
    "        if vgg_model_instance: loaded_models_dict[\"VGG19\"] = vgg_model_instance; available_model_names_list.append(\"VGG19\")\n",
    "    else:\n",
    "        print(f\"Checkpoint for VGG19 not found at {vgg_checkpoint_file}\")\n",
    "else:\n",
    "    print(\"CRITICAL: Number of classes is 0, cannot load models.\")\n",
    "\n",
    "if not loaded_models_dict and app_num_classes > 0:\n",
    "    print(\"WARNING: No trained models found. Creating dummy models to allow UI to load. Predictions will be random.\")\n",
    "    dummy_cnn = ImprovedLetterCNN(num_classes=app_num_classes, device=device)\n",
    "    loaded_models_dict[\"ImprovedCNN (Dummy)\"] = dummy_cnn\n",
    "    if not available_model_names_list: available_model_names_list.append(\"ImprovedCNN (Dummy)\")\n",
    "\n",
    "default_model_choice_app = available_model_names_list[0] if available_model_names_list else \"No models available\"\n",
    "print(f\"Available models for Gradio: {available_model_names_list}\")\n",
    "print(f\"Default model for Gradio: {default_model_choice_app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_grid_for_gradio(images_data_with_labels, title):\n",
    "    \"\"\"Helper to create a grid of images for Gradio gallery. Expects list of (image_array, label) tuples.\"\"\"\n",
    "    if not images_data_with_labels: return None\n",
    "    cols = max(1, min(len(images_data_with_labels), 4))\n",
    "    rows = (len(images_data_with_labels) + cols - 1) // cols\n",
    "    fig_w, fig_h = cols * 2.5, rows * 2.7 \n",
    "    if os.environ.get('DISPLAY','') == '' and os.name != 'posix': \n",
    "        try: matplotlib.use('Agg')\n",
    "        except Exception as e: print(f\"Matplotlib backend switch failed: {e}\")\n",
    "        \n",
    "    fig = plt.figure(figsize=(fig_w, fig_h)); \n",
    "    if title: fig.suptitle(title, fontsize=10)\n",
    "    for i, (img_to_show, img_label) in enumerate(images_data_with_labels):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_title(img_label, fontsize=8)\n",
    "        cmap_val = 'gray' if len(img_to_show.shape) == 2 or (len(img_to_show.shape) == 3 and img_to_show.shape[2] == 1) else None\n",
    "        ax.imshow(img_to_show.squeeze(), cmap=cmap_val); ax.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95 if title else 1])\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    grid_img_np = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    grid_img_np = grid_img_np.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    return grid_img_np\n",
    "\n",
    "def process_image_for_gradio_tabs(input_pil_image, selected_model_name_str, is_single_char_mode=False, spacing=12):\n",
    "    gallery_output_images = [] # This will store the actual NumPy arrays for the gallery\n",
    "    \n",
    "    if input_pil_image is None: \n",
    "        return \"No image provided.\", [np.zeros((100,100,3), dtype=np.uint8)]\n",
    "\n",
    "    current_model_to_use = loaded_models_dict.get(selected_model_name_str)\n",
    "    if current_model_to_use is None or selected_model_name_str == \"No models available\":\n",
    "        return f\"Error: Model '{selected_model_name_str}' not loaded/available.\", [np.array(input_pil_image.convert('RGB'))]\n",
    "\n",
    "    try:\n",
    "        input_image_np_rgb = np.array(input_pil_image.convert('RGB'))\n",
    "        gallery_output_images.append(input_image_np_rgb.copy()) # Original\n",
    "        \n",
    "        gray_image = cv2.cvtColor(input_image_np_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        gallery_output_images.append(cv2.cvtColor(gray_image.copy(), cv2.COLOR_GRAY2RGB)) # Grayscale\n",
    "\n",
    "        if is_single_char_mode:\n",
    "            # Sketchpad is black ink on white bg. Model expects black char on white bg for ROI processing.\n",
    "            _, char_roi_binarized_inv = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            char_roi_binarized = 255 - char_roi_binarized_inv # Black char on white\n",
    "            gallery_output_images.append(cv2.cvtColor(char_roi_binarized.copy(),cv2.COLOR_GRAY2RGB)) # Binarized ROI\n",
    "            \n",
    "            char_pil = Image.fromarray(char_roi_binarized).convert('L')\n",
    "            tensor_input = prepare_char_roi_for_model(char_pil, spacing=0)\n",
    "            if tensor_input is None: return \"Error: Could not process drawn character.\", gallery_output_images\n",
    "            \n",
    "            model_input_vis_np = tensor_input.squeeze(0).cpu().permute(1,2,0).numpy()\n",
    "            mean, std = np.array([0.485,0.456,0.406]), np.array([0.229,0.224,0.225])\n",
    "            model_input_vis_np=(model_input_vis_np*std)+mean; model_input_vis_np=np.clip(model_input_vis_np,0,1)\n",
    "            gallery_output_images.append((model_input_vis_np * 255).astype(np.uint8)) # Processed for Model\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = current_model_to_use(tensor_input.to(device))\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                char_name = app_class_labels[predicted_idx.item()]\n",
    "            return f\"{char_name} (Conf: {confidence.item()*100:.1f}%)\", gallery_output_images\n",
    "\n",
    "        # Multi-character segmentation logic\n",
    "        _, binary_for_contours = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        gallery_output_images.append(cv2.cvtColor(binary_for_contours.copy(), cv2.COLOR_GRAY2RGB)) # Binary for Contours\n",
    "        \n",
    "        _, binary_for_model_rois = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # Black text on white for ROIs\n",
    "        gallery_output_images.append(cv2.cvtColor(binary_for_model_rois.copy(), cv2.COLOR_GRAY2RGB)) # Binary for ROIs\n",
    "\n",
    "        contours, _ = cv2.findContours(binary_for_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contour_img_viz = input_image_np_rgb.copy(); cv2.drawContours(contour_img_viz, contours, -1, (0,255,0), 2)\n",
    "        gallery_output_images.append(contour_img_viz) # Contours on Original\n",
    "        if not contours: return \"No contours found.\", gallery_output_images\n",
    "\n",
    "        letter_contours = sorted([c for c in contours if cv2.contourArea(c)>20], key=lambda ctr:cv2.boundingRect(ctr)[0])\n",
    "        if not letter_contours: return \"No significant contours found.\", gallery_output_images\n",
    "        \n",
    "        rois_for_grid_visualization = [] # List of (numpy_array, label_str) for create_image_grid_for_gradio\n",
    "        recognized_chars_list = []\n",
    "        output_image_with_boxes = input_image_np_rgb.copy()\n",
    "\n",
    "        for i, contour in enumerate(letter_contours):\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            if w < 5 or h < 5: continue\n",
    "            char_roi_binarized_np = binary_for_model_rois[y:y+h, x:x+w]\n",
    "            char_pil = Image.fromarray(char_roi_binarized_np).convert('L')\n",
    "            tensor_input = prepare_char_roi_for_model(char_pil, spacing=spacing)\n",
    "            if tensor_input is None: continue\n",
    "            \n",
    "            model_input_vis_np = tensor_input.squeeze(0).cpu().permute(1,2,0).numpy()\n",
    "            mean, std = np.array([0.485,0.456,0.406]), np.array([0.229,0.224,0.225])\n",
    "            model_input_vis_np=(model_input_vis_np*std)+mean; model_input_vis_np=np.clip(model_input_vis_np,0,1)\n",
    "            rois_for_grid_visualization.append( ( (model_input_vis_np*255).astype(np.uint8), f\"ROI {i}\" ) )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = current_model_to_use(tensor_input.to(device))\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                char_name = app_class_labels[predicted_idx.item()]\n",
    "            recognized_chars_list.append(char_name)\n",
    "            cv2.rectangle(output_image_with_boxes, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "            cv2.putText(output_image_with_boxes, f\"{char_name} ({confidence.item()*100:.0f}%)\", (x,y-10 if y-10 > 10 else y+h+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0),1)\n",
    "        \n",
    "        if rois_for_grid_visualization:\n",
    "            rois_grid_img = create_image_grid_for_gradio(rois_for_grid_visualization, \"Processed Segments for Model\")\n",
    "            if rois_grid_img is not None: gallery_output_images.append(rois_grid_img)\n",
    "        \n",
    "        gallery_output_images.append(output_image_with_boxes) # Final Predictions\n",
    "        return \"\".join(recognized_chars_list), gallery_output_images\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Gradio processing: {e}\")\n",
    "        traceback.print_exc()\n",
    "        error_img_placeholder = np.zeros((100,300,3),dtype=np.uint8); cv2.putText(error_img_placeholder,\"Processing Error\",(10,50),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),1)\n",
    "        return f\"Error: {str(e)}\", [error_img_placeholder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch Gradio Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_enhanced_gradio_app():\n",
    "    if not app_class_labels or app_num_classes == 0:\n",
    "        gr.Warning(\"CRITICAL: Class labels could not be loaded or are empty. App functionality will be severely limited.\")\n",
    "    if not loaded_models_dict or default_model_choice_app == \"No models available\":\n",
    "        gr.Warning(\"CRITICAL: No models could be loaded. Predictions will not work. Ensure models are trained and paths are correct.\")\n",
    "\n",
    "    with gr.Blocks(title=\"Handwritten Character Recognition App\", theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue, secondary_hue=gr.themes.colors.indigo)) as app_interface:\n",
    "        gr.Markdown(\"<h1><center>Handwritten Character Recognition App</center></h1>\")\n",
    "        gr.Markdown(\"Select a model, then provide an image via drawing, uploading, or webcam.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            model_selector_dropdown = gr.Dropdown(choices=available_model_names_list, value=default_model_choice_app, label=\"Select Recognition Model\")\n",
    "        \n",
    "        output_text_display = gr.Textbox(label=\"Recognized Text (Multi-Character)\", lines=1, placeholder=\"Recognized characters will appear here...\")\n",
    "        # For single character, prediction will be shown in a Label within its tab.\n",
    "        gallery_display = gr.Gallery(label=\"Processing Visualizations\", columns=3, height=600, object_fit=\"contain\", preview=True, show_label=False)\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"✏️ Draw Single Character\"):\n",
    "                with gr.Row():\n",
    "                    sketch_input_single = gr.Sketchpad(type=\"pil\", label=\"Draw Single Character Here\", image_mode=\"RGB\", shape=(280,280), \n",
    "                                                         invert_colors=False, brush_radius=10, brush_color=\"#000000\", background_color=\"#FFFFFF\", \n",
    "                                                         show_label=True, elem_id=\"sketchpad_single\")\n",
    "                    with gr.Column(scale=1):\n",
    "                        single_char_pred_output = gr.Label(label=\"Prediction\") \n",
    "                        single_char_processed_img_display = gr.Image(label=\"Processed for Model\", type=\"numpy\", width=128, height=128, image_mode=\"RGB\")\n",
    "                single_char_button = gr.Button(\"Recognize Single Drawn Character\", variant=\"primary\")\n",
    "                \n",
    "                def handle_single_char_draw_tab(sketch_pil, model_name):\n",
    "                    if sketch_pil is None: return \"Draw a character.\", None, []\n",
    "                    text_out, gallery_numpy_imgs_list = process_image_for_gradio_tabs(sketch_pil, model_name, is_single_char_mode=True)\n",
    "                    # For the small display next to sketchpad, show the last image (processed for model)\n",
    "                    processed_img_for_small_display = gallery_numpy_imgs_list[-1] if gallery_numpy_imgs_list and gallery_numpy_imgs_list[-1] is not None else None\n",
    "                    return text_out, processed_img_for_small_display, gallery_numpy_imgs_list\n",
    "\n",
    "                single_char_button.click(fn=handle_single_char_draw_tab, \n",
    "                                         inputs=[sketch_input_single, model_selector_dropdown], \n",
    "                                         outputs=[single_char_pred_output, single_char_processed_img_display, gallery_display])\n",
    "\n",
    "            with gr.TabItem(\"🖼️ Upload Multi-Character Image\"):\n",
    "                upload_input_multi = gr.Image(type=\"pil\", label=\"Upload Image (Multiple Characters)\", image_mode=\"RGB\")\n",
    "                upload_button_multi = gr.Button(\"Recognize Uploaded Image\", variant=\"primary\")\n",
    "                upload_button_multi.click(lambda img, model_name: process_image_for_gradio_tabs(img, model_name, is_single_char_mode=False), \n",
    "                                          inputs=[upload_input_multi, model_selector_dropdown], \n",
    "                                          outputs=[output_text_display, gallery_display])\n",
    "\n",
    "            with gr.TabItem(\"📷 Webcam Capture\"):\n",
    "                webcam_input_multi = gr.Image(source=\"webcam\", type=\"pil\", label=\"Capture from Webcam\", image_mode=\"RGB\")\n",
    "                webcam_button_multi = gr.Button(\"Recognize from Webcam\", variant=\"primary\")\n",
    "                webcam_button_multi.click(lambda img, model_name: process_image_for_gradio_tabs(img, model_name, is_single_char_mode=False), \n",
    "                                         inputs=[webcam_input_multi, model_selector_dropdown], \n",
    "                                         outputs=[output_text_display, gallery_display])\n",
    "\n",
    "        gr.Markdown(\"--- \")\n",
    "        gr.Markdown(\"**Notes:**\\n\"+\n",
    "                    \"- **Model Selection:** Choose a model before processing an image. Models are loaded from `./model_checkpoints/`\\n\"+\n",
    "                    \"- **Drawing Single Char:** Draw clearly in the center. The sketchpad is set to black ink on white background.\\n\"+\n",
    "                    \"- **Uploading/Webcam (Multi-Char):** Clear, well-lit images with good contrast work best. Ensure adequate spacing between characters if possible.\\n\"+\n",
    "                    \"- **Segmentation:** This is a complex step. Results depend on handwriting clarity and image quality.\")\n",
    "    \n",
    "    app_interface.launch(share=True, debug=True) # Debug=True for more detailed error messages during development\n",
    "\n",
    "# Automatically launch if in a Jupyter-like environment and components are ready.\n",
    "if __name__ == '__main__' and ('ipykernel' in __import__('sys').modules):\n",
    "    if not app_class_labels or app_num_classes == 0:\n",
    "        print(\"CRITICAL WARNING: Class labels could not be loaded/are empty. Gradio app functionality will be severely limited.\")\n",
    "    elif not loaded_models_dict or default_model_choice_app == \"No models available\":\n",
    "        print(\"CRITICAL WARNING: No models could be loaded. Predictions will not work. Gradio app may be non-functional. Ensure models are trained and paths are correct.\")\n",
    "    else:\n",
    "        print(\"Attempting to launch Gradio app...\")\n",
    "    launch_enhanced_gradio_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
