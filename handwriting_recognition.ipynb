{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition\n",
    "\n",
    "This notebook demonstrates the process of building and training a handwritten character recognition system using PyTorch. It covers data loading and augmentation, defining several Convolutional Neural Network (CNN) models (including custom architectures and transfer learning with VGG19), training these models, and finally, performing inference on single images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Image processing and display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 # OpenCV for image operations\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch essentials\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# In a Jupyter Notebook, multiprocessing start method handling is less critical \n",
    "# than in standalone scripts, especially if not using num_workers > 0 in DataLoader\n",
    "# or if issues arise. If needed, it can be set.\n",
    "# try:\n",
    "#     torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "#     print(\"Set multiprocessing start method to 'spawn'.\")\n",
    "# except RuntimeError as e:\n",
    "#     print(f\"Note: {e}. Multiprocessing start method might have been already set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data Augmentation Transforms\n",
    "\n",
    "The following cells define custom PyTorch transforms used for data augmentation. `RandomChoice` applies one randomly selected transform from a list, and `ThicknessTransform` simulates variations in stroke thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomChoice(torch.nn.Module):\n",
    "    \"\"\"Apply one randomly chosen transform from a list of transforms.\n",
    "    Args:\n",
    "        transforms (list of callable): List of transforms to choose from.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return random.choice(self.transforms)(img)\n",
    "\n",
    "class ThicknessTransform(torch.nn.Module):\n",
    "    \"\"\"Apply morphological operations to change stroke thickness.\n",
    "    It randomly chooses between dilation (thicker) or erosion (thinner).\n",
    "    Args:\n",
    "        kernel_size (int): Size of the kernel for morphological operations (default: 3).\n",
    "        iterations (int): Number of times to apply the operation (default: 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, iterations=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to OpenCV format (numpy array)\n",
    "        img_cv = np.array(img)\n",
    "        \n",
    "        # Ensure image is grayscale for morphological operations\n",
    "        if len(img_cv.shape) == 3 and img_cv.shape[2] == 3:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2GRAY)\n",
    "        elif len(img_cv.shape) == 3 and img_cv.shape[2] == 1: # Already grayscale but 3-channel\n",
    "             img_cv = img_cv[:, :, 0]\n",
    "        \n",
    "        kernel = np.ones((self.kernel_size, self.kernel_size), np.uint8)\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            # Dilation (thicker)\n",
    "            processed_img = cv2.dilate(img_cv, kernel, iterations=self.iterations)\n",
    "        else:\n",
    "            # Erosion (thinner)\n",
    "            processed_img = cv2.erode(img_cv, kernel, iterations=self.iterations)\n",
    "        \n",
    "        # Convert back to PIL Image\n",
    "        return Image.fromarray(processed_img, mode='L') # 'L' for grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting Data Pipeline\n",
    "\n",
    "The `HandwritingDataPipeline` class manages the loading, augmentation, and splitting of the image dataset. \n",
    "\n",
    "Key features:\n",
    "- **Purpose**: Encapsulates all data handling steps, from reading images from disk to preparing them in batches for training, validation, and testing.\n",
    "- **Parameters**:\n",
    "    - `data_root`: The root directory where the dataset is stored. Images should be organized into subdirectories, where each subdirectory name corresponds to a class label (e.g., 'A', 'B', '0', '1').\n",
    "    - `image_size`: The target size (height, width) to which all images will be resized.\n",
    "    - `batch_size`: The number of images per batch for the DataLoaders.\n",
    "    - `do_transform`: A boolean flag to enable or disable data augmentation. Augmentation is typically enabled for the training set to improve model generalization.\n",
    "- **Data Split**: The pipeline splits the dataset into training, validation, and test sets (typically 70%/15%/15% split). This is crucial for evaluating the model's performance on unseen data and for hyperparameter tuning.\n",
    "- **Normalization**: Image pixel values are normalized (mean and standard deviation) based on ImageNet statistics, which is a common practice, especially when using pretrained models. For grayscale images, the mean and std are often applied across all three channels if the pretrained model expects 3-channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataPipeline:\n",
    "    def __init__(self, data_root, image_size=(64, 64), batch_size=32, do_transform=True, test_split=0.15, val_split=0.15):\n",
    "        self.data_root = data_root\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.do_transform = do_transform\n",
    "        self.test_split = test_split\n",
    "        self.val_split = val_split # Val split is from the remaining data after test_split\n",
    "\n",
    "        # Normalization parameters (ImageNet defaults, suitable for transfer learning)\n",
    "        # For grayscale, these are often applied as (mean, mean, mean) and (std, std, std)\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self._setup_transforms()\n",
    "        self._load_and_split_datasets()\n",
    "\n",
    "    def _setup_transforms(self):\n",
    "        if self.do_transform:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1), # Ensure grayscale\n",
    "                RandomChoice([\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10),\n",
    "                    transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "                ]),\n",
    "                ThicknessTransform(kernel_size=random.choice([1,3]), iterations=random.choice([1,2])),\n",
    "                transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3), # Still useful for grayscale\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), # Repeat grayscale channel for models expecting 3 channels\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "                self.normalize\n",
    "            ])\n",
    "\n",
    "        self.val_test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "            self.normalize\n",
    "        ])\n",
    "\n",
    "    def _load_and_split_datasets(self):\n",
    "        full_dataset = datasets.ImageFolder(root=self.data_root)\n",
    "        self.class_names = full_dataset.classes\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "        total_size = len(full_dataset)\n",
    "        test_size = int(total_size * self.test_split)\n",
    "        remaining_size = total_size - test_size\n",
    "        val_size = int(remaining_size * (self.val_split / (1.0 - self.test_split))) # val_split is a fraction of the original total\n",
    "        train_size = remaining_size - val_size\n",
    "\n",
    "        # Check if sizes are valid\n",
    "        if train_size <= 0 or val_size <=0 or test_size <=0:\n",
    "            print(f\"Warning: Dataset too small for current split ratios. Total: {total_size}\")\n",
    "            # Fallback to simpler split if calculated sizes are problematic\n",
    "            if total_size < 3:\n",
    "                train_set, val_set, test_set = full_dataset, full_dataset, full_dataset # Use all for all if tiny\n",
    "            else:\n",
    "                # Simplified split for small datasets\n",
    "                train_size = max(1, int(total_size * 0.7))\n",
    "                val_size = max(1, int(total_size * 0.15))\n",
    "                test_size = total_size - train_size - val_size\n",
    "                if test_size <= 0: # Ensure test_size is at least 1 if possible\n",
    "                    test_size = 1\n",
    "                    val_size = total_size - train_size - test_size\n",
    "                    if val_size <=0:\n",
    "                        val_size = 1\n",
    "                        train_size = total_size - val_size - test_size\n",
    "                \n",
    "        print(f\"Attempting to split: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "        try:\n",
    "            train_temp_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size + val_size, test_size],\n",
    "                                                                          generator=torch.Generator().manual_seed(42))\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(train_temp_dataset, [train_size, val_size],\n",
    "                                                                       generator=torch.Generator().manual_seed(42))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dataset splitting: {e}. Adjusting split sizes or check dataset.\")\n",
    "            # Fallback: if split fails, assign datasets to avoid crashing, though this is not ideal.\n",
    "            # This might happen if dataset is extremely small. User should be warned.\n",
    "            print(\"Using full dataset for train/val/test due to splitting error. THIS IS NOT RECOMMENDED FOR ACTUAL TRAINING.\")\n",
    "            train_dataset, val_dataset, test_dataset = full_dataset, full_dataset, full_dataset\n",
    "\n",
    "        # Assign transforms\n",
    "        # We need to wrap these datasets to apply transforms\n",
    "        self.train_dataset = TransformedDataset(train_dataset, transform=self.train_transform)\n",
    "        self.val_dataset = TransformedDataset(val_dataset, transform=self.val_test_transform)\n",
    "        self.test_dataset = TransformedDataset(test_dataset, transform=self.val_test_transform)\n",
    "        \n",
    "        self.sizes = {'train': len(self.train_dataset), 'val': len(self.val_dataset), 'test': len(self.test_dataset)}\n",
    "\n",
    "    def get_loaders(self, shuffle_train=True, shuffle_val=False, shuffle_test=False):\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=shuffle_train, num_workers=0) # num_workers=0 for notebooks generally safer\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=shuffle_val, num_workers=0)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=shuffle_test, num_workers=0)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self.class_names\n",
    "\n",
    "# Helper class to apply transforms to subsets from random_split\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize data pipeline with root: ./content/augmented_images/augmented_images1\n",
      "If this path is incorrect or the dataset is not structured as expected (ImageFolder format), this cell will error.\n",
      "Attempting to split: Train=9548, Val=2046, Test=2046\n",
      "\n",
      "Data pipeline initialized successfully.\n",
      "Dataset split sizes: {'train': 9548, 'val': 2046, 'test': 2046}\n",
      "Number of classes: 62\n",
      "Class labels: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A_caps', 'B_caps', 'C_caps', 'D_caps', 'E_caps', 'F_caps', 'G_caps', 'H_caps', 'I_caps', 'J_caps', 'K_caps', 'L_caps', 'M_caps', 'N_caps', 'O_caps', 'P_caps', 'Q_caps', 'R_caps', 'S_caps', 'T_caps', 'U_caps', 'V_caps', 'W_caps', 'X_caps', 'Y_caps', 'Z_caps', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Train loader: <torch.utils.data.dataloader.DataLoader object at 0x31703bd50>, Val loader: <torch.utils.data.dataloader.DataLoader object at 0x316140950>, Test loader: <torch.utils.data.dataloader.DataLoader object at 0x317100550>\n"
     ]
    }
   ],
   "source": [
    "# Data Pipeline Initialization and Loader Retrieval Example\n",
    "\n",
    "# IMPORTANT: Replace this path with the actual path to your dataset's root folder.\n",
    "# The dataset should be structured with subfolders for each class (e.g., data_root/A, data_root/B, ...).\n",
    "data_root_example = \"./content/augmented_images/augmented_images1\" # <<< USER: CHANGE THIS PATH\n",
    "\n",
    "print(f\"Attempting to initialize data pipeline with root: {data_root_example}\")\n",
    "print(\"If this path is incorrect or the dataset is not structured as expected (ImageFolder format), this cell will error.\")\n",
    "\n",
    "# Check if the placeholder path exists, if not, this cell will likely error later but we can warn now.\n",
    "if not os.path.exists(data_root_example):\n",
    "    print(f\"\\nWARNING: The directory '{data_root_example}' does not exist. \\nPlease ensure your dataset is available at this path or update 'data_root_example'.\")\n",
    "    print(\"Skipping pipeline initialization and loader retrieval for now as the path is invalid.\")\n",
    "    # Assign None to loaders to prevent subsequent cells from crashing immediately if they use these variables\n",
    "    # Users will need to fix the path and re-run for those cells to work.\n",
    "    train_loader_example, val_loader_example, test_loader_example, dataset_sizes_example, num_classes_example = None, None, None, None, None\n",
    "    example_pipeline = None\n",
    "else:\n",
    "    try:\n",
    "        example_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=16, do_transform=True)\n",
    "        train_loader_example, val_loader_example, test_loader_example = example_pipeline.get_loaders()\n",
    "        dataset_sizes_example = example_pipeline.sizes\n",
    "        num_classes_example = example_pipeline.num_classes\n",
    "\n",
    "        print(f\"\\nData pipeline initialized successfully.\")\n",
    "        print(f\"Dataset split sizes: {dataset_sizes_example}\")\n",
    "        print(f\"Number of classes: {num_classes_example}\")\n",
    "        print(f\"Class labels: {example_pipeline.get_class_labels()}\")\n",
    "        print(f\"Train loader: {train_loader_example}, Val loader: {val_loader_example}, Test loader: {test_loader_example}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR initializing data pipeline or getting loaders: {e}\")\n",
    "        print(\"Please check the 'data_root_example' path and the dataset structure.\")\n",
    "        train_loader_example, val_loader_example, test_loader_example, dataset_sizes_example, num_classes_example = None, None, None, None, None\n",
    "        example_pipeline = None\n",
    "\n",
    "# Note to user:\n",
    "# For the subsequent cells (like displaying augmented images or training models) to run,\n",
    "# the 'data_root_example' path must be correctly set and the dataset must be present and structured correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Augmented Images\n",
    "\n",
    "This function helps visualize the effect of the data augmentation techniques applied to the training images. It fetches a batch of images from the training loader and displays a few of them, each with multiple augmented versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying augmented images... Note: 'Base' and 'Aug' may look similar if augmentation is subtle or loader doesn't re-augment per call as expected.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x1200 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_augmented_images(data_loader, class_names, num_images=5, num_augmentations=3):\n",
    "    \"\"\"Displays original and augmented images from the train_loader.\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader for the training set.\n",
    "        class_names (list): List of class names.\n",
    "        num_images (int): Number of unique images to display.\n",
    "        num_augmentations (int): Number of augmented versions to show per image (if using augmentation in loader).\n",
    "                                 If loader doesn't re-augment on each call, this will show similar images.\n",
    "                                 For this notebook, it will show different augmentations if the loader is set up with diverse transforms.\n",
    "    \"\"\"\n",
    "    if data_loader is None:\n",
    "        print(\"Data loader is None. Cannot display images. Please ensure the data pipeline was initialized correctly.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(num_augmentations * 3, num_images * 3))\n",
    "    \n",
    "    # Temporarily set a batch size for display if needed, or rely on loader's batch size\n",
    "    # This function assumes the loader can provide at least num_images\n",
    "    try:\n",
    "        images_so_far = 0\n",
    "        for i in range(num_augmentations):\n",
    "            # Get a batch of images. Iterating the loader will give new batches (potentially augmented differently)\n",
    "            try:\n",
    "                inputs, classes = next(iter(data_loader))\n",
    "            except StopIteration:\n",
    "                print(\"Could not fetch a new batch from data_loader. Maybe dataset is too small or loader exhausted?\")\n",
    "                return\n",
    "            \n",
    "            # Ensure we have enough images in the batch\n",
    "            if len(inputs) < num_images and i == 0:\n",
    "                print(f\"Warning: Batch size ({len(inputs)}) is less than num_images ({num_images}). Displaying available images.\")\n",
    "                # num_images = len(inputs) # this would change plotting layout, better to inform user\n",
    "\n",
    "            for j in range(min(num_images, len(inputs))): # Iterate up to num_images or available images in batch\n",
    "                if i == 0: # For the first set of augmentations, also store original-like images\n",
    "                    ax = plt.subplot(num_images, num_augmentations + 1, j * (num_augmentations + 1) + 1)\n",
    "                    ax.set_title(f\"Base: {class_names[classes[j]]}\")\n",
    "                    img = inputs[j].numpy().transpose((1, 2, 0))\n",
    "                    mean = np.array([0.485, 0.456, 0.406])\n",
    "                    std = np.array([0.229, 0.224, 0.225])\n",
    "                    img = std * img + mean # Unnormalize\n",
    "                    img = np.clip(img, 0, 1)\n",
    "                    plt.imshow(img)\n",
    "                    plt.axis('off')\n",
    "                \n",
    "                ax = plt.subplot(num_images, num_augmentations + 1, j * (num_augmentations + 1) + i + 2)\n",
    "                ax.set_title(f\"Aug: {class_names[classes[j]]}\")\n",
    "                img = inputs[j].numpy().transpose((1, 2, 0))\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean # Unnormalize\n",
    "                img = np.clip(img, 0, 1)\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display images: {e}. Ensure train_loader is valid and contains data.\")\n",
    "\n",
    "# Example usage (ensure train_loader_example and example_pipeline are valid from the previous cell)\n",
    "if train_loader_example and example_pipeline:\n",
    "    print(\"Displaying augmented images... Note: 'Base' and 'Aug' may look similar if augmentation is subtle or loader doesn't re-augment per call as expected.\")\n",
    "    display_augmented_images(train_loader_example, class_names=example_pipeline.get_class_labels(), num_images=4, num_augmentations=3)\n",
    "else:\n",
    "    print(\"Skipping display_augmented_images example as train_loader_example or example_pipeline is not available. Please fix the data path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions: Custom CNNs\n",
    "\n",
    "Below are definitions for two custom Convolutional Neural Network (CNN) architectures:\n",
    "- `LetterCNN64`: A basic CNN model designed for 64x64 pixel input images.\n",
    "- `ImprovedLetterCNN`: An enhanced version of `LetterCNN64` that incorporates Batch Normalization for stable training and Dropout for regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterCNN64(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LetterCNN64, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) # Input 3 channels (RGB-like after transform)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input size: 64x64 -> After conv1 (padding=1): 64x64 -> After pool1: 32x32\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input size: 32x32 -> After conv2 (padding=1): 32x32 -> After pool2: 16x16\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input size: 16x16 -> After conv3 (padding=1): 16x16 -> After pool3: 8x8\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Flattened size: 128 channels * 8 * 8 = 8192\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8) # Flatten the tensor\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImprovedLetterCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ImprovedLetterCNN, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 64x64 -> 32x32\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 32x32 -> 16x16\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 16x16 -> 8x8\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 8x8 -> 4x4\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Flattened size: 256 channels * 4 * 4 = 4096\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5) # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.dropout1(self.relu_fc1(self.bn_fc1(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu_fc2(self.bn_fc2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition: VGG19 Transfer Learning\n",
    "\n",
    "This section defines `VGG19HandwritingModel`, which leverages a pre-trained VGG19 model for transfer learning. \n",
    "- **Transfer Learning**: Instead of training a model from scratch, we use the VGG19 architecture with weights pre-trained on the large ImageNet dataset. These learned features are often effective for various computer vision tasks.\n",
    "- **Adaptation for Grayscale**: The first convolutional layer of VGG19 is modified to accept single-channel (grayscale) input images, as our handwriting dataset is primarily grayscale. The weights of this layer can be initialized by averaging the original weights across the three color channels or by other strategies.\n",
    "- **Custom Classifier**: The original VGG19 classifier (fully connected layers) is replaced with a new classifier suited to the number of classes in our handwriting dataset. This new classifier will be trained on our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19HandwritingModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, pretrained=True):\n",
    "        super(VGG19HandwritingModel, self).__init__()\n",
    "        self.device = device\n",
    "        # Load a pretrained VGG19 model\n",
    "        vgg19 = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "\n",
    "        # Modify the first convolutional layer to accept 1 input channel (grayscale)\n",
    "        # instead of 3 (RGB). We do this by averaging the weights of the original first conv layer.\n",
    "        original_first_conv = vgg19.features[0]\n",
    "        new_first_conv = nn.Conv2d(1, original_first_conv.out_channels, \n",
    "                                     kernel_size=original_first_conv.kernel_size, \n",
    "                                     stride=original_first_conv.stride, \n",
    "                                     padding=original_first_conv.padding,\n",
    "                                     dilation=original_first_conv.dilation,\n",
    "                                     groups=original_first_conv.groups,\n",
    "                                     bias=(original_first_conv.bias is not None))\n",
    "        \n",
    "        if pretrained:\n",
    "            # Average the weights across the input channels (RGB) for the new grayscale channel\n",
    "            new_first_conv.weight.data = torch.mean(original_first_conv.weight.data, dim=1, keepdim=True)\n",
    "            if original_first_conv.bias is not None:\n",
    "                new_first_conv.bias.data = original_first_conv.bias.data\n",
    "        \n",
    "        # Replace the first conv layer in the features module\n",
    "        vgg19.features[0] = new_first_conv\n",
    "        \n",
    "        # The input transformation pipeline already converts grayscale to 3-channel pseudo-RGB\n",
    "        # to match VGG's original expectation. So, direct modification of vgg19.features[0] to take 1 channel\n",
    "        # is an alternative if we change the ToTensor() + normalize pipeline to output 1 channel directly.\n",
    "        # For this notebook, we assume the DataPipeline prepares 3-channel images (by repeating grayscale).\n",
    "        # Thus, we will use the original VGG first layer that expects 3 channels.\n",
    "        # If we strictly wanted to use the modified single-channel input VGG, the data pipeline's Lambda\n",
    "        # that repeats channels should be removed and this model's first layer should be the `new_first_conv` for 1 channel.\n",
    "        # For simplicity and consistency with common transfer learning where input channels are matched, \n",
    "        # we will stick to the 3-channel input for VGG provided by the data pipeline.\n",
    "        # THEREFORE, the above modification to new_first_conv is more of a demonstration.\n",
    "        # We will use the standard VGG first layer expecting 3 channels, as our pipeline provides that.\n",
    "        # If data pipeline was changed to output 1 channel, then the modified layer would be essential.\n",
    "        # Let's revert to the original vgg19 features if the pipeline provides 3 channels.\n",
    "        # For this notebook, the pipeline's transforms.Lambda(lambda x: x.repeat(3,1,1)) ensures 3 channels.\n",
    "        # So, no change to VGG's first layer is strictly necessary here if that lambda is used.\n",
    "        # However, if one *wanted* to feed 1-channel images directly, they'd uncomment the above and modify the pipeline.\n",
    "        # For clarity, let's use the standard VGG expecting 3 channels as per the pipeline.\n",
    "        \n",
    "        # Re-load VGG19 but this time we are sure our pipeline gives 3 channels.\n",
    "        vgg19 = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        self.features = vgg19.features\n",
    "\n",
    "        # Freeze feature parameters if using pretrained model (common practice for transfer learning)\n",
    "        if pretrained:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier with a new one appropriate for num_classes\n",
    "        # VGG19's classifier input features: 512 * 7 * 7 (if input is 224x224)\n",
    "        # Since our input is 64x64, the output feature map size from self.features will be different.\n",
    "        # VGG19 with 64x64 input: after 5 maxpools (64 -> 32 -> 16 -> 8 -> 4 -> 2)\n",
    "        # So, the output from features will be (batch_size, 512, 2, 2)\n",
    "        num_features_output = 512 * 2 * 2 \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features_output, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The data pipeline ensures x is (batch, 3, H, W)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Utility Functions: Training, Loading, and Testing\n",
    "\n",
    "These functions are essential for the model development lifecycle:\n",
    "- `train_model`: Handles the training loop, including forward pass, loss calculation, backpropagation, optimizer steps, and learning rate scheduling. It also includes logic for validating the model periodically and saving the best performing model checkpoint as well as the final model.\n",
    "- `load_model`: Utility to load saved model checkpoints (weights and optimizer state) to resume training or for inference.\n",
    "- `test_model`: Evaluates the trained model on the test dataset to assess its generalization performance on unseen data, reporting accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, save_dir='model_checkpoints'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Created directory: {save_dir}\")\n",
    "        \n",
    "    start_time = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Store history\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            if dataloader is None or len(dataloader.dataset) == 0:\n",
    "                print(f\"Skipping {phase} phase as dataloader is None or dataset is empty.\")\n",
    "                if phase == 'val' and best_acc == 0: # if val loader is bad, cannot determine best model based on val_acc\n",
    "                     # Save model based on training performance or just save last if no val set\n",
    "                     pass # Or consider other metrics if val is unavailable\n",
    "                continue\n",
    "\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                total_samples += inputs.size(0)\n",
    "            \n",
    "            if total_samples == 0: # Avoid division by zero if dataset was empty\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                print(f\"No samples found for {phase} phase in epoch {epoch+1}.\")\n",
    "            else:\n",
    "                epoch_loss = running_loss / total_samples\n",
    "                epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item()) # .item() to get Python number\n",
    "            else: # phase == 'val'\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                if scheduler:\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(epoch_loss)\n",
    "                    # For other schedulers like CosineAnnealingLR, step is usually called after optimizer.step()\n",
    "                    # For this structure, it's fine here if not ReduceLROnPlateau\n",
    "                    elif phase == 'train' and scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                         pass # Handled below after optimizer step for most schedulers\n",
    "\n",
    "                if epoch_acc > best_acc and total_samples > 0 : # ensure val set was processed\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch + 1\n",
    "                    # Save best model checkpoint\n",
    "                    best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "                    torch.save({\n",
    "                        'epoch': best_epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': epoch_loss,\n",
    "                        'accuracy': best_acc.item(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
    "                    }, best_model_path)\n",
    "                    print(f\"Best model saved to {best_model_path} (Epoch {best_epoch}, Val Acc: {best_acc:.4f})\")\n",
    "        \n",
    "        if phase == 'train' and scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step() # Step for schedulers like CosineAnnealing, StepLR etc.\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f} at epoch {best_epoch}')\n",
    "\n",
    "    # Load best model weights back\n",
    "    if best_acc > 0: # only load if a best model was found (i.e. val phase ran)\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(save_dir, 'final_model.pth')\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': history['val_loss'][-1] if history['val_loss'] else history['train_loss'][-1],\n",
    "        'accuracy': history['val_acc'][-1] if history['val_acc'] else history['train_acc'][-1],\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def load_model(model, optimizer, checkpoint_path, scheduler=None):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint path {checkpoint_path} does not exist. Returning initial model.\")\n",
    "        return model, optimizer, scheduler, 0, 0.0, 0.0\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) # map_location ensures tensors are loaded to the correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state only if optimizer is provided and state exists in checkpoint\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        try:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not load optimizer state: {e}. Optimizer will be reinitialized.\")\n",
    "    \n",
    "    # Load scheduler state only if scheduler is provided and state exists in checkpoint\n",
    "    if scheduler is not None and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:\n",
    "        try:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        except Exception as e: # Catching general exception as state loading can have various issues\n",
    "            print(f\"Warning: Could not load scheduler state: {e}. Scheduler may be reinitialized or use default state.\")\n",
    "            \n",
    "    start_epoch = checkpoint.get('epoch', 0)\n",
    "    loss = checkpoint.get('loss', 0.0)\n",
    "    accuracy = checkpoint.get('accuracy', 0.0)\n",
    "    \n",
    "    print(f\"Model loaded from {checkpoint_path}. Epoch: {start_epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return model, optimizer, scheduler, start_epoch, loss, accuracy\n",
    "\n",
    "def test_model(model, test_loader, criterion=None):\n",
    "    if test_loader is None or len(test_loader.dataset) == 0:\n",
    "        print(\"Test loader is None or dataset is empty. Skipping testing.\")\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    model.eval() # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss() # Default criterion if not provided\n",
    "\n",
    "    with torch.no_grad(): # No need to track gradients during testing\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        print(\"No samples found in the test set.\")\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "    return test_loss, test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Training a Custom CNN (`ImprovedLetterCNN`)\n",
    "\n",
    "In this experiment, we will train the `ImprovedLetterCNN` model defined earlier. This model includes improvements like Batch Normalization and Dropout. We will use data augmentation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 1: Training ImprovedLetterCNN ---\n",
    "print(\"--- Experiment 1: Training ImprovedLetterCNN ---\")\n",
    "\n",
    "# Ensure data_root_example and device are defined from earlier cells.\n",
    "# If example_pipeline was successfully initialized, we can use its properties.\n",
    "if example_pipeline is not None and num_classes_example is not None:\n",
    "    save_dir_cnn = 'model_checkpoints_cnn'\n",
    "    if not os.path.exists(save_dir_cnn):\n",
    "        os.makedirs(save_dir_cnn)\n",
    "\n",
    "    # Re-initialize pipeline for this experiment if needed, or use existing one\n",
    "    # For clarity, let's assume we might want different settings, so re-init or use specific vars\n",
    "    # This assumes data_root_example is correctly set by the user.\n",
    "    print(f\"Using data_root: {data_root_example} for CNN experiment.\")\n",
    "    cnn_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=32, do_transform=True)\n",
    "    train_loader_cnn, val_loader_cnn, test_loader_cnn = cnn_pipeline.get_loaders()\n",
    "    num_classes_cnn = cnn_pipeline.num_classes\n",
    "    \n",
    "    if num_classes_cnn > 0 and len(train_loader_cnn.dataset) > 0:\n",
    "        print(f\"Number of classes for CNN: {num_classes_cnn}\")\n",
    "        print(f\"Train samples: {len(train_loader_cnn.dataset)}, Val samples: {len(val_loader_cnn.dataset)}, Test samples: {len(test_loader_cnn.dataset)}\")\n",
    "\n",
    "        model_cnn = ImprovedLetterCNN(num_classes_cnn).to(device)\n",
    "        criterion_cnn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer and Scheduler Setup\n",
    "        optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.0005) # Adjusted LR from 0.001 to 0.0005\n",
    "        scheduler_cnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_cnn, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "        # --- Other Scheduler Examples (commented out) ---\n",
    "        # scheduler_cnn_step = torch.optim.lr_scheduler.StepLR(optimizer_cnn, step_size=7, gamma=0.1)\n",
    "        # # Note: For StepLR, T_max and eta_min are not applicable. StepLR reduces LR by gamma every step_size epochs.\n",
    "\n",
    "        # scheduler_cnn_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn, T_max=20, eta_min=1e-6) # T_max = num_epochs\n",
    "        # # Note: CosineAnnealingLR gradually decreases LR following a cosine curve.\n",
    "\n",
    "        # num_epochs_cnn = 20 # Define num_epochs if using schedulers that require it like OneCycleLR or CosineAnnealingLR\n",
    "        # scheduler_cnn_onecycle = torch.optim.lr_scheduler.OneCycleLR(optimizer_cnn, max_lr=0.001, epochs=num_epochs_cnn, steps_per_epoch=len(train_loader_cnn))\n",
    "        # # Note: OneCycleLR varies LR, increasing then decreasing it over the epochs. max_lr should typically be determined with an LR range test.\n",
    "        # --- End of Other Scheduler Examples ---\n",
    "\n",
    "        num_epochs_cnn_train = 20 # Set number of epochs for this experiment\n",
    "        print(f\"Starting training for ImprovedLetterCNN for {num_epochs_cnn_train} epochs...\")\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model_cnn, history_cnn = train_model(model_cnn, train_loader_cnn, val_loader_cnn, \n",
    "                                                   criterion_cnn, optimizer_cnn, scheduler_cnn, \n",
    "                                                   num_epochs=num_epochs_cnn_train, save_dir=save_dir_cnn)\n",
    "        \n",
    "        print(\"\\nTesting the final trained ImprovedLetterCNN model on the test set:\")\n",
    "        test_model(trained_model_cnn, test_loader_cnn, criterion_cnn)\n",
    "\n",
    "        print(\"\\nLoading the best saved ImprovedLetterCNN model and testing it on the test set:\")\n",
    "        # Initialize a new model instance for loading\n",
    "        best_model_cnn_instance = ImprovedLetterCNN(num_classes_cnn).to(device)\n",
    "        # Create a dummy optimizer for load_model function, its state will be overwritten if saved in checkpoint\n",
    "        # Or, if you need to resume training with this optimizer, re-initialize it as before.\n",
    "        dummy_optimizer_cnn = optim.Adam(best_model_cnn_instance.parameters()) \n",
    "        best_cnn_model_loaded, _, _, _, _, _ = load_model(best_model_cnn_instance, \n",
    "                                                       dummy_optimizer_cnn, \n",
    "                                                       os.path.join(save_dir_cnn, 'best_model.pth'))\n",
    "        test_model(best_cnn_model_loaded, test_loader_cnn, criterion_cnn)\n",
    "    else:\n",
    "        print(\"Skipping Experiment 1: CNN training, due to invalid number of classes or empty train loader.\")\n",
    "        print(f\"  Number of classes: {num_classes_cnn if 'num_classes_cnn' in locals() else 'Not determined'}\")\n",
    "        print(f\"  Train loader dataset length: {len(train_loader_cnn.dataset) if 'train_loader_cnn' in locals() and hasattr(train_loader_cnn, 'dataset') else 'Not determined'}\")\n",
    "else:\n",
    "    print(\"Skipping Experiment 1: CNN training, as the data pipeline was not initialized successfully in the earlier cell.\")\n",
    "    print(\"Please ensure 'data_root_example' is set correctly and the dataset is accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Training with Transfer Learning (`VGG19HandwritingModel`)\n",
    "\n",
    "This experiment explores transfer learning using the `VGG19HandwritingModel`. We will initialize the model with weights pre-trained on ImageNet and fine-tune it on our handwriting dataset. We will also demonstrate how to set up the optimizer with different learning rates for the pre-trained feature extractor and the newly initialized classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 2: Training VGG19HandwritingModel ---\n",
    "print(\"--- Experiment 2: Training VGG19HandwritingModel ---\")\n",
    "\n",
    "if example_pipeline is not None and num_classes_example is not None:\n",
    "    save_dir_vgg = 'model_checkpoints_vgg'\n",
    "    if not os.path.exists(save_dir_vgg):\n",
    "        os.makedirs(save_dir_vgg)\n",
    "\n",
    "    print(f\"Using data_root: {data_root_example} for VGG experiment.\")\n",
    "    # It's good practice to have a separate pipeline instance if batch_size or other params differ\n",
    "    # Or re-use cnn_pipeline if settings are identical\n",
    "    vgg_pipeline = HandwritingDataPipeline(data_root=data_root_example, image_size=(64,64), batch_size=32, do_transform=True)\n",
    "    train_loader_vgg, val_loader_vgg, test_loader_vgg = vgg_pipeline.get_loaders()\n",
    "    num_classes_vgg = vgg_pipeline.num_classes\n",
    "    num_epochs_vgg = 20 # Define number of epochs for VGG training\n",
    "\n",
    "    if num_classes_vgg > 0 and len(train_loader_vgg.dataset) > 0:\n",
    "        print(f\"Number of classes for VGG: {num_classes_vgg}\")\n",
    "        print(f\"Train samples: {len(train_loader_vgg.dataset)}, Val samples: {len(val_loader_vgg.dataset)}, Test samples: {len(test_loader_vgg.dataset)}\")\n",
    "\n",
    "        # Model Initialization\n",
    "        # Option 1: Use pretrained VGG19\n",
    "        use_pretrained_vgg = True # Set to False to train VGG from scratch\n",
    "        model_vgg = VGG19HandwritingModel(num_classes=num_classes_vgg, device=device, pretrained=use_pretrained_vgg).to(device)\n",
    "        print(f\"VGG19 Model initialized {'with pretrained ImageNet weights' if use_pretrained_vgg else 'from scratch'}.\")\n",
    "\n",
    "        criterion_vgg = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer and Scheduler Setup for VGG\n",
    "        if use_pretrained_vgg:\n",
    "            # Different learning rates for feature extractor (lower LR) and classifier (higher LR)\n",
    "            optimizer_vgg = optim.Adam([\n",
    "                {'params': model_vgg.features.parameters(), 'lr': 1e-5}, # Lower LR for frozen or slowly unfrozen features\n",
    "                {'params': model_vgg.classifier.parameters(), 'lr': 1e-4} # Higher LR for the new classifier part\n",
    "            ], weight_decay=1e-4) # Added weight decay\n",
    "            # Scheduler for fine-tuning pretrained model\n",
    "            scheduler_vgg = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vgg, T_max=num_epochs_vgg, eta_min=1e-6)\n",
    "            print(\"Optimizer set up for pretrained VGG model with differential learning rates.\")\n",
    "        else:\n",
    "            # Training VGG from scratch (typically requires more epochs and careful LR tuning)\n",
    "            optimizer_vgg = optim.Adam(model_vgg.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "            # OneCycleLR is often good for training from scratch\n",
    "            scheduler_vgg = torch.optim.lr_scheduler.OneCycleLR(optimizer_vgg, \n",
    "                                                                max_lr=1e-3, \n",
    "                                                                epochs=num_epochs_vgg, \n",
    "                                                                steps_per_epoch=len(train_loader_vgg))\n",
    "            print(\"Optimizer set up for VGG model training from scratch.\")\n",
    "\n",
    "        print(f\"Starting training for VGG19HandwritingModel for {num_epochs_vgg} epochs...\")\n",
    "        trained_model_vgg, history_vgg = train_model(model_vgg, train_loader_vgg, val_loader_vgg, \n",
    "                                                   criterion_vgg, optimizer_vgg, scheduler_vgg, \n",
    "                                                   num_epochs=num_epochs_vgg, save_dir=save_dir_vgg)\n",
    "\n",
    "        print(\"\\nTesting the final trained VGG19 model on the test set:\")\n",
    "        test_model(trained_model_vgg, test_loader_vgg, criterion_vgg)\n",
    "\n",
    "        print(\"\\nLoading the best saved VGG19 model and testing it on the test set:\")\n",
    "        best_model_vgg_instance = VGG19HandwritingModel(num_classes=num_classes_vgg, device=device, pretrained=False).to(device) # `pretrained` here only affects init, weights are loaded next\n",
    "        dummy_optimizer_vgg = optim.Adam(best_model_vgg_instance.parameters())\n",
    "        best_vgg_model_loaded, _, _, _, _, _ = load_model(best_model_vgg_instance, \n",
    "                                                       dummy_optimizer_vgg, \n",
    "                                                       os.path.join(save_dir_vgg, 'best_model.pth'))\n",
    "        test_model(best_vgg_model_loaded, test_loader_vgg, criterion_vgg)\n",
    "    else:\n",
    "        print(\"Skipping Experiment 2: VGG training, due to invalid number of classes or empty train loader.\")\n",
    "        print(f\"  Number of classes: {num_classes_vgg if 'num_classes_vgg' in locals() else 'Not determined'}\")\n",
    "        print(f\"  Train loader dataset length: {len(train_loader_vgg.dataset) if 'train_loader_vgg' in locals() and hasattr(train_loader_vgg, 'dataset') else 'Not determined'}\")\n",
    "else:\n",
    "    print(\"Skipping Experiment 2: VGG training, as the data pipeline was not initialized successfully.\")\n",
    "    print(\"Please ensure 'data_root_example' is set correctly and the dataset is accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on a Single Image\n",
    "\n",
    "This section shows how to load a trained model (either the custom CNN or the VGG model) and use it to predict the character from a single image. You'll need to provide the path to your trained model and the image you want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(data_root):\n",
    "    \"\"\"Gets class labels from the folder names in data_root.\"\"\"\n",
    "    if not os.path.exists(data_root):\n",
    "        print(f\"Error: Data root directory '{data_root}' not found for getting class labels.\")\n",
    "        return []\n",
    "    # Assuming ImageFolder structure where subdirectories are class names\n",
    "    try:\n",
    "        dataset = datasets.ImageFolder(root=data_root)\n",
    "        return dataset.classes\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset from '{data_root}' to get class labels: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_path, image_size=(64, 64)):\n",
    "    \"\"\"Loads an image, converts to grayscale, resizes, and prepares it for model inference.\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image path '{image_path}' not found.\")\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('L') # Convert to grayscale\n",
    "        \n",
    "        # Define transformations similar to validation/test transform but without dataset context\n",
    "        # Normalization values should be consistent with training\n",
    "        normalize_inference = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                  std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        inference_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(), # This will convert grayscale PIL image (H,W) to (1,H,W) tensor\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), # Repeat for 3 channels\n",
    "            normalize_inference\n",
    "        ])\n",
    "        \n",
    "        img_tensor = inference_transform(img)\n",
    "        return img_tensor.unsqueeze(0) # Add batch dimension -> (1, 3, H, W)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform Inference --- \n",
    "print(\"--- Inference Section ---\")\n",
    "\n",
    "# USER ACTION REQUIRED:\n",
    "# 1. Set `inference_model_path` to the path of your trained model checkpoint (e.g., best_model.pth from CNN or VGG experiment).\n",
    "# 2. Set `data_root_for_labels` to the *same data_root* used during training to ensure class labels are mapped correctly.\n",
    "# 3. Set `image_path_for_inference` to the path of the image you want to classify.\n",
    "\n",
    "inference_model_path = os.path.join('model_checkpoints_cnn', 'best_model.pth') # Example: using best CNN model\n",
    "# Or for VGG: inference_model_path = os.path.join('model_checkpoints_vgg', 'best_model.pth')\n",
    "\n",
    "data_root_for_labels = data_root_example # Use the same data_root as defined in cell 6 (or your training data path)\n",
    "\n",
    "# Example image path - replace with your own image.\n",
    "# You might need to upload an image to your Colab environment or provide a full path if running locally.\n",
    "# image_path_for_inference = \"path/to/your/character/image.png\" \n",
    "image_path_for_inference = \"\" # Intentionally blank: USER MUST PROVIDE A PATH\n",
    "\n",
    "print(f\"Using model: {inference_model_path}\")\n",
    "print(f\"Using data root for labels: {data_root_for_labels}\")\n",
    "\n",
    "if not os.path.exists(inference_model_path):\n",
    "    print(f\"ERROR: Model checkpoint '{inference_model_path}' not found. Please train a model or provide a valid path.\")\n",
    "elif not image_path_for_inference or not os.path.exists(image_path_for_inference):\n",
    "    print(f\"INFO: 'image_path_for_inference' is not set or the file does not exist ('{image_path_for_inference}').\")\n",
    "    print(\"Please provide a valid image path to perform inference.\")\n",
    "    # You could create a dummy image here for demonstration if you have cv2/numpy and know the dataset structure\n",
    "    # For example, to create a dummy 'A.png':\n",
    "    # if not os.path.exists(\"example_char.png\"):\n",
    "    #     dummy_img_arr = np.zeros((64,64,1), dtype=np.uint8)\n",
    "    #     cv2.putText(dummy_img_arr, 'A', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255), 3)\n",
    "    #     cv2.imwrite(\"example_char.png\", dummy_img_arr)\n",
    "    #     image_path_for_inference = \"example_char.png\"\n",
    "    #     print(\"Created a dummy 'example_char.png'. Re-run this cell with this path or your own.\")\n",
    "    # else:\n",
    "    #     image_path_for_inference = \"example_char.png\"\n",
    "    #     print(f\"Using existing 'example_char.png'. Re-run cell if you want to use your own image.\")\n",
    "else:\n",
    "    class_labels = get_class_labels(data_root_for_labels)\n",
    "    if not class_labels:\n",
    "        print(\"ERROR: Could not retrieve class labels. Ensure 'data_root_for_labels' is correct.\")\n",
    "    else:\n",
    "        num_classes_inference = len(class_labels)\n",
    "        print(f\"Number of classes for inference: {num_classes_inference}, Labels: {class_labels}\")\n",
    "\n",
    "        # Initialize the model architecture\n",
    "        # IMPORTANT: This must match the architecture of the saved model in 'inference_model_path'\n",
    "        # If loading a VGG model, use: VGG19HandwritingModel(num_classes=num_classes_inference, device=device, pretrained=False).to(device)\n",
    "        # The `pretrained=False` here is fine because we are loading weights from our checkpoint.\n",
    "        inference_model_arch = ImprovedLetterCNN(num_classes_inference).to(device) # Assuming CNN model\n",
    "        # If using VGG, uncomment below and comment out the ImprovedLetterCNN line:\n",
    "        # inference_model_arch = VGG19HandwritingModel(num_classes=num_classes_inference, device=device, pretrained=False).to(device)\n",
    "        \n",
    "        # Create a dummy optimizer for load_model. Its state isn't used if only inferring.\n",
    "        dummy_optimizer_inference = optim.Adam(inference_model_arch.parameters())\n",
    "\n",
    "        # Load the trained model weights\n",
    "        loaded_inference_model, _, _, _, _, _ = load_model(inference_model_arch, \n",
    "                                                          dummy_optimizer_inference, \n",
    "                                                          inference_model_path)\n",
    "        loaded_inference_model.eval() # Set model to evaluation mode\n",
    "\n",
    "        # Prepare the input image\n",
    "        input_tensor = prepare_image(image_path_for_inference).to(device)\n",
    "\n",
    "        if input_tensor is not None:\n",
    "            with torch.no_grad(): # Disable gradient calculations for inference\n",
    "                outputs = loaded_inference_model(input_tensor)\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                confidence, predicted_class_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                predicted_label = class_labels[predicted_class_idx.item()]\n",
    "                confidence_percent = confidence.item() * 100\n",
    "                \n",
    "                print(f\"\\nImage: {image_path_for_inference}\")\n",
    "                print(f\"Predicted Class: {predicted_label}\")\n",
    "                print(f\"Confidence: {confidence_percent:.2f}%\")\n",
    "\n",
    "                # Display the image (optional)\n",
    "                try:\n",
    "                    img_display = Image.open(image_path_for_inference)\n",
    "                    plt.imshow(img_display)\n",
    "                    plt.title(f\"Predicted: {predicted_label} ({confidence_percent:.2f}%)\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not display the image: {e}\")\n",
    "        else:\n",
    "            print(\"Could not prepare image for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive walkthrough of a handwritten character recognition task using PyTorch. We covered:\n",
    "- Setting up a data pipeline with image loading, transformations, and augmentations.\n",
    "- Defining and comparing custom CNN architectures (`LetterCNN64`, `ImprovedLetterCNN`).\n",
    "- Implementing transfer learning with a pre-trained VGG19 model (`VGG19HandwritingModel`), including modifying it for the specific task.\n",
    "- Core functions for training, model checkpointing (saving best and final models), loading models, and evaluating them on a test set.\n",
    "- Running two distinct training experiments: one for the custom `ImprovedLetterCNN` and another for the `VGG19HandwritingModel`.\n",
    "- A section demonstrating how to perform inference on a single image using a trained model.\n",
    "\n",
    "This framework can be extended and adapted for various image classification tasks. For further improvements, one could explore more advanced architectures, different augmentation strategies, hyperparameter optimization techniques, or larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio (quietly)\n",
    "!pip install gradio -q\n",
    "import gradio as gr\n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demos with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: Single Character Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure class_labels are loaded or defined globally for the notebook if not already\n",
    "# Example: data_root_for_labels = \"./content/augmented_images/augmented_images1\" \n",
    "# class_labels = get_class_labels(data_root_for_labels)\n",
    "# num_classes = len(class_labels)\n",
    "\n",
    "# Placeholder for where models are saved (ensure these paths are correct)\n",
    "MODEL_PATH_CNN = os.path.join('model_checkpoints_cnn', 'best_model.pth')\n",
    "MODEL_PATH_VGG = os.path.join('model_checkpoints_vgg', 'best_model.pth') # Assuming VGG model is also saved as best_model.pth in its own dir\n",
    "\n",
    "def predict_single_character(image_input, model_choice):\n",
    "    if image_input is None:\n",
    "        return \"No image provided\", \"0.0%\"\n",
    "\n",
    "    # Image can be PIL from upload or NumPy from Sketchpad\n",
    "    if isinstance(image_input, np.ndarray):\n",
    "        # For Sketchpad: input is usually (height, width, 3) or (height, width)\n",
    "        # Convert to PIL for consistent processing\n",
    "        if image_input.ndim == 3 and image_input.shape[2] == 3: # RGB from sketchpad\n",
    "            image_pil = Image.fromarray(image_input, 'RGB').convert('L')\n",
    "        elif image_input.ndim == 2: # Grayscale from sketchpad (less likely for default sketchpad)\n",
    "            image_pil = Image.fromarray(image_input, 'L')\n",
    "        else: \n",
    "            image_pil = Image.fromarray(image_input).convert('L')\n",
    "\n",
    "\n",
    "    elif isinstance(image_input, Image.Image):\n",
    "        image_pil = image_input.convert('L') # Ensure PIL and grayscale\n",
    "    else:\n",
    "        return \"Invalid image input\", \"0.0%\"\n",
    "\n",
    "    # Get class labels (ensure this is defined and accessible)\n",
    "    # This path might need to be adjusted by the user.\n",
    "    data_root_for_labels = \"./content/augmented_images/augmented_images1\" \n",
    "    if not os.path.exists(data_root_for_labels):\n",
    "        return \"Error: data_root_for_labels path for class names does not exist. Please check the path in the notebook.\", \"\"\n",
    "    \n",
    "    class_labels = get_class_labels(data_root_for_labels)\n",
    "    if not class_labels: # Handle case where class_labels might be empty\n",
    "        return \"Error: Could not load class labels. Ensure data_root is correct and contains data.\", \"\"\n",
    "    num_classes = len(class_labels)\n",
    "\n",
    "    # Preprocess the image\n",
    "    # The model expects 3 channels after ToTensor and Normalize in the training pipeline\n",
    "    # For Gradio, we use a Normalize that expects single channel and then repeat it for the model\n",
    "    # However, the provided training pipeline's normalize expects 3 channels.\n",
    "    # Let's align with the training pipeline's val_test_transform more closely.\n",
    "    gradio_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(), # Converts to (1, H, W)\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), # Converts to (3, H, W)\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Use same normalization as training\n",
    "    ])\n",
    "    img_tensor = gradio_transform(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Load selected model\n",
    "    if model_choice == 'ImprovedCNN':\n",
    "        model_path = MODEL_PATH_CNN\n",
    "        model = ImprovedLetterCNN(num_classes).to(device)\n",
    "        if not os.path.exists(model_path): return f\"Model checkpoint not found: {model_path}\", \"0.0%\"\n",
    "    elif model_choice == 'VGG19':\n",
    "        model_path = MODEL_PATH_VGG\n",
    "        model = VGG19HandwritingModel(num_classes=num_classes, device=device, pretrained=False).to(device)\n",
    "        if not os.path.exists(model_path): return f\"Model checkpoint not found: {model_path}\", \"0.0%\"\n",
    "    else:\n",
    "        return \"Invalid model choice\", \"0.0%\"\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        # Check if the checkpoint contains 'model_state_dict'\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else: # Assume the checkpoint itself is the state_dict (older saving format)\n",
    "            model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        return f\"Error loading model: {str(e)}\", \"0.0%\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "        predicted_char = class_labels[predicted_idx.item()]\n",
    "    \n",
    "    return predicted_char, f\"{confidence.item()*100:.2f}%\"\n",
    "\n",
    "iface_single = gr.Interface(\n",
    "    fn=predict_single_character,\n",
    "    inputs=[\n",
    "        gr.Sketchpad(label=\"Draw a Character Here\", type=\"numpy\", image_mode=\"L\", invert_colors=True, shape=(128,128)), # Increased shape for better drawing\n",
    "        gr.Dropdown(choices=['ImprovedCNN', 'VGG19'], label=\"Choose Model\", value='ImprovedCNN')\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Label(label=\"Prediction\"),\n",
    "        gr.Label(label=\"Confidence\")\n",
    "    ],\n",
    "    title=\"Single Handwritten Character Recognition\",\n",
    "    description=\"Draw a character (ensure it's centered and reasonably large), then choose a model to predict it. Ensure model checkpoints exist by running training cells first.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Multiple Character Recognition (Word/Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters(image_pil):\n",
    "    # Convert PIL image to OpenCV format\n",
    "    image_np = np.array(image_pil.convert('L')) # Grayscale\n",
    "    \n",
    "    # Preprocessing for segmentation: \n",
    "    # Input images for segmentation are typically black text on white background.\n",
    "    # THRESH_BINARY_INV will make the text white (objects) and background black.\n",
    "    thresh = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    bounding_boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    \n",
    "    min_h, max_h = 10, image_np.shape[0] * 0.95 \n",
    "    min_w, max_w = 5, image_np.shape[1] * 0.9  \n",
    "    aspect_ratio_min, aspect_ratio_max = 0.1, 3.0\n",
    "\n",
    "    filtered_boxes = []\n",
    "    for x, y, w, h in bounding_boxes:\n",
    "        if min_h < h < max_h and min_w < w < max_w:\n",
    "            aspect_ratio = w / float(h) if h > 0 else 0\n",
    "            if aspect_ratio_min < aspect_ratio < aspect_ratio_max:\n",
    "                # Filter out boxes that are likely part of other larger boxes (simple check)\n",
    "                is_nested = False\n",
    "                for x2, y2, w2, h2 in bounding_boxes:\n",
    "                    if (x > x2 and y > y2 and x+w < x2+w2 and y+h < y2+h2) and (w*h < 0.5 * w2*h2):\n",
    "                        is_nested = True\n",
    "                        break\n",
    "                if not is_nested:\n",
    "                     filtered_boxes.append((x, y, w, h))\n",
    "            \n",
    "    bounding_boxes = sorted(filtered_boxes, key=lambda b: b[0])\n",
    "    \n",
    "    cropped_characters = []\n",
    "    img_for_boxes_np = np.array(image_pil.convert('RGB'))\n",
    "\n",
    "    for x, y, w, h in bounding_boxes:\n",
    "        # Crop from the original grayscale image_np (black text on white bg)\n",
    "        char_img_np = image_np[y:y+h, x:x+w]\n",
    "        \n",
    "        # Add padding (white background for model input)\n",
    "        pad_size = int(max(w,h) * 0.20) \n",
    "        # Pad with white (255) as model expects black char on white bg before ToTensor\n",
    "        char_img_np_padded = cv2.copyMakeBorder(char_img_np, pad_size, pad_size, pad_size, pad_size, cv2.BORDER_CONSTANT, value=255)\n",
    "        \n",
    "        char_img_pil = Image.fromarray(char_img_np_padded).convert('L')\n",
    "        cropped_characters.append(char_img_pil)\n",
    "        \n",
    "        cv2.rectangle(img_for_boxes_np, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    return Image.fromarray(img_for_boxes_np), cropped_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiple_characters(image_input, model_choice):\n",
    "    if image_input is None:\n",
    "        return None, \"No image provided\"\n",
    "\n",
    "    if isinstance(image_input, np.ndarray):\n",
    "        image_pil = Image.fromarray(image_input).convert('L')\n",
    "    elif isinstance(image_input, Image.Image):\n",
    "        image_pil = image_input.convert('L') # Ensure grayscale for segmentation\n",
    "    else:\n",
    "        return None, \"Invalid image input\"\n",
    "\n",
    "    image_with_boxes, char_images_pil = segment_characters(image_pil)\n",
    "    \n",
    "    if not char_images_pil:\n",
    "        return image_with_boxes, \"No characters detected or segmented properly\"\n",
    "\n",
    "    data_root_for_labels = \"./content/augmented_images/augmented_images1\"\n",
    "    if not os.path.exists(data_root_for_labels):\n",
    "        return image_with_boxes, \"Error: data_root_for_labels path for class names does not exist.\"\n",
    "\n",
    "    class_labels = get_class_labels(data_root_for_labels)\n",
    "    if not class_labels:\n",
    "        return image_with_boxes, \"Error: Could not load class labels.\"\n",
    "    num_classes = len(class_labels)\n",
    "    \n",
    "    gradio_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    if model_choice == 'ImprovedCNN':\n",
    "        model_path = MODEL_PATH_CNN\n",
    "        model = ImprovedLetterCNN(num_classes).to(device)\n",
    "        if not os.path.exists(model_path): return image_with_boxes, f\"CNN Model checkpoint not found: {model_path}\"\n",
    "    elif model_choice == 'VGG19':\n",
    "        model_path = MODEL_PATH_VGG\n",
    "        model = VGG19HandwritingModel(num_classes=num_classes, device=device, pretrained=False).to(device)\n",
    "        if not os.path.exists(model_path): return image_with_boxes, f\"VGG Model checkpoint not found: {model_path}\"\n",
    "    else:\n",
    "        return image_with_boxes, \"Invalid model choice\"\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        return image_with_boxes, f\"Error loading model: {str(e)}\"\n",
    "\n",
    "    predictions_text = \"\"\n",
    "    with torch.no_grad():\n",
    "        for char_pil in char_images_pil:\n",
    "            img_tensor = gradio_transform(char_pil).unsqueeze(0).to(device)\n",
    "            outputs = model(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "            predicted_char = class_labels[predicted_idx.item()]\n",
    "            predictions_text += f\"{predicted_char} ({confidence.item()*100:.1f}%) \"\n",
    "            \n",
    "    return image_with_boxes, predictions_text.strip()\n",
    "\n",
    "iface_multi = gr.Interface(\n",
    "    fn=predict_multiple_characters,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload an Image of a Word/Sentence (black text on white bg recommended for segmentation)\"),\n",
    "        gr.Dropdown(choices=['ImprovedCNN', 'VGG19'], label=\"Choose Model\", value='ImprovedCNN')\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Image(label=\"Detected Characters\"),\n",
    "        gr.Textbox(label=\"Recognized Text & Confidences\", lines=3)\n",
    "    ],\n",
    "    title=\"Multiple Handwritten Character Recognition\",\n",
    "    description=\"Upload an image containing multiple characters (ideally black text on a light, clean background). The system will attempt to segment and recognize them. Ensure models are trained.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.TabbedInterface([iface_single, iface_multi], [\"Single Character\", \"Multiple Characters\"]).launch(share=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
