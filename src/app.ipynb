{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45299173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Import Libraries and Set Up Environment ---\n",
    "\"\"\"\n",
    "# Handwritten Character Recognition: Interactive Gradio Application\n",
    "\n",
    "This notebook builds an interactive web application for handwritten character recognition using Gradio.\n",
    "It allows users to recognize characters through various input methods and visualize the results.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "from glob import glob\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import utility modules\n",
    "from src.models_util import get_model, get_model_info\n",
    "from src.inference_utils import prepare_single_image, process_numpy_image\n",
    "from src.inference_utils import extract_characters_from_image, benchmark_inference_speed\n",
    "\n",
    "# Gradio for UI\n",
    "import gradio as gr\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories for saving results\n",
    "os.makedirs(\"gradio_results\", exist_ok=True)\n",
    "os.makedirs(\"gradio_results/uploads\", exist_ok=True)\n",
    "os.makedirs(\"gradio_results/drawings\", exist_ok=True)\n",
    "os.makedirs(\"gradio_results/captures\", exist_ok=True)\n",
    "os.makedirs(\"gradio_results/batch\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Model Loading Functions ---\n",
    "\"\"\"\n",
    "## Model Loading Functions\n",
    "\n",
    "These functions handle loading trained models from checkpoints.\n",
    "They include options for different model architectures and configurations.\n",
    "\"\"\"\n",
    "\n",
    "def load_class_names(class_names_path):\n",
    "    \"\"\"\n",
    "    Load class names from a text file.\n",
    "    \n",
    "    Args:\n",
    "        class_names_path: Path to the class names file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of class names\n",
    "    \"\"\"\n",
    "    print(f\"Loading class names from: {class_names_path}\")\n",
    "    \n",
    "    if not os.path.exists(class_names_path):\n",
    "        print(f\"ERROR: Class names file '{class_names_path}' not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(class_names_path, 'r') as f:\n",
    "            class_names = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        print(f\"Loaded {len(class_names)} class names\")\n",
    "        return class_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading class names: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_available_models(model_checkpoints_dir=\"../model_checkpoints\"):\n",
    "    \"\"\"\n",
    "    Find available trained models in the checkpoints directory.\n",
    "    \n",
    "    Args:\n",
    "        model_checkpoints_dir: Directory containing model checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of available models\n",
    "    \"\"\"\n",
    "    print(f\"Searching for available models in: {model_checkpoints_dir}\")\n",
    "    \n",
    "    if not os.path.exists(model_checkpoints_dir):\n",
    "        print(f\"ERROR: Model checkpoints directory '{model_checkpoints_dir}' not found\")\n",
    "        return {}\n",
    "    \n",
    "    available_models = {}\n",
    "    \n",
    "    # List subdirectories (each should be a model type)\n",
    "    model_dirs = [d for d in os.listdir(model_checkpoints_dir) \n",
    "                if os.path.isdir(os.path.join(model_checkpoints_dir, d))]\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        model_path = os.path.join(model_checkpoints_dir, model_dir)\n",
    "        \n",
    "        # Check for model checkpoints\n",
    "        checkpoint_files = []\n",
    "        for ext in ['*.pth', '*.pt']:\n",
    "            checkpoint_files.extend(glob(os.path.join(model_path, ext)))\n",
    "        \n",
    "        if checkpoint_files:\n",
    "            # Check for class_names.txt\n",
    "            class_names_path = os.path.join(model_path, 'class_names.txt')\n",
    "            has_class_names = os.path.exists(class_names_path)\n",
    "            \n",
    "            # Check for model_info.txt\n",
    "            model_info_path = os.path.join(model_path, 'model_info.txt')\n",
    "            has_model_info = os.path.exists(model_info_path)\n",
    "            \n",
    "            available_models[model_dir] = {\n",
    "                'checkpoint_files': sorted(checkpoint_files),\n",
    "                'class_names_path': class_names_path if has_class_names else None,\n",
    "                'model_info_path': model_info_path if has_model_info else None,\n",
    "                'has_class_names': has_class_names,\n",
    "                'has_model_info': has_model_info\n",
    "            }\n",
    "    \n",
    "    if available_models:\n",
    "        print(f\"Found {len(available_models)} available models:\")\n",
    "        for model_name, info in available_models.items():\n",
    "            print(f\"  - {model_name}:\")\n",
    "            print(f\"    - Checkpoints: {len(info['checkpoint_files'])}\")\n",
    "            print(f\"    - Has class names: {info['has_class_names']}\")\n",
    "            print(f\"    - Has model info: {info['has_model_info']}\")\n",
    "    else:\n",
    "        print(\"No trained models found.\")\n",
    "    \n",
    "    return available_models\n",
    "\n",
    "def load_model_checkpoint(checkpoint_path, model_name, num_classes, device=device):\n",
    "    \"\"\"\n",
    "    Load a model from a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        model_name: Name of the model architecture\n",
    "        num_classes: Number of output classes\n",
    "        device: Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Loaded model\n",
    "    \"\"\"\n",
    "    print(f\"Loading model checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"ERROR: Checkpoint file '{checkpoint_path}' not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model architecture\n",
    "        model = get_model(model_name, num_classes, device, pretrained=False)\n",
    "        \n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Extract state dict\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "        else:\n",
    "            # Handle case where checkpoint is just the state dict\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        # Load state dict into model\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Extract metadata if available\n",
    "        metadata = {}\n",
    "        for key in ['epoch', 'accuracy', 'val_acc', 'loss']:\n",
    "            if key in checkpoint:\n",
    "                metadata[key] = checkpoint[key]\n",
    "        \n",
    "        if metadata:\n",
    "            print(f\"Checkpoint metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model checkpoint: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_default_class_names():\n",
    "    \"\"\"Get default class names when no class_names.txt is available.\"\"\"\n",
    "    # Default class names: digits + uppercase letters + lowercase letters\n",
    "    default_names = [str(i) for i in range(10)]  # 0-9\n",
    "    default_names += [chr(i) for i in range(65, 91)]  # A-Z\n",
    "    default_names += [chr(i) for i in range(97, 123)]  # a-z\n",
    "    return default_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15655299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Image Processing Functions ---\n",
    "\"\"\"\n",
    "## Image Processing Functions\n",
    "\n",
    "These functions handle preprocessing of input images for character recognition.\n",
    "They include operations for resizing, thresholding, and character segmentation.\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_image(image, resize=True, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocess an image for inference.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (numpy array or PIL Image)\n",
    "        resize: Whether to resize the image\n",
    "        target_size: Target size for resizing\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Preprocessed image\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image if needed\n",
    "    if isinstance(image, np.ndarray):\n",
    "        if len(image.shape) == 3 and image.shape[2] == 4:  # RGBA\n",
    "            image = Image.fromarray(image).convert('RGB')\n",
    "        else:\n",
    "            image = Image.fromarray(image)\n",
    "    \n",
    "    # Ensure grayscale\n",
    "    gray_image = image.convert('L')\n",
    "    \n",
    "    # Resize if needed\n",
    "    if resize:\n",
    "        gray_image = gray_image.resize(target_size, Image.LANCZOS)\n",
    "    \n",
    "    return gray_image\n",
    "\n",
    "def extract_characters(image, min_area=50, spacing=24):\n",
    "    \"\"\"\n",
    "    Extract individual characters from an image containing text.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (numpy array or PIL Image)\n",
    "        min_area: Minimum contour area to consider as a character\n",
    "        spacing: Spacing around characters when normalizing\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (extracted_chars, visualization_image)\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image_np = image.copy()\n",
    "    \n",
    "    # Ensure grayscale\n",
    "    if len(image_np.shape) == 3:\n",
    "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image_np.copy()\n",
    "    \n",
    "    # Apply binary thresholding (black text on white background -> white text on black for findContours)\n",
    "    _, binary_inv = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_inv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter by area and sort left-to-right\n",
    "    valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    valid_contours = sorted(valid_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
    "    \n",
    "    # Create visualization image\n",
    "    vis_image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.drawContours(vis_image, valid_contours, -1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Extract and normalize individual characters\n",
    "    extracted_chars = []\n",
    "    \n",
    "    for i, cnt in enumerate(valid_contours):\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        \n",
    "        # Draw bounding box with index\n",
    "        cv2.rectangle(vis_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(vis_image, str(i+1), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        # Extract character ROI\n",
    "        char_roi = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # Normalize size while maintaining aspect ratio\n",
    "        target_size = 64 - spacing * 2\n",
    "        \n",
    "        if w > h:\n",
    "            new_w = target_size\n",
    "            new_h = int(h * new_w / w)\n",
    "        else:\n",
    "            new_h = target_size\n",
    "            new_w = int(w * new_h / h)\n",
    "        \n",
    "        # Resize\n",
    "        resized_char = cv2.resize(char_roi, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Create padded image (white background)\n",
    "        padded_char = np.ones((64, 64), dtype=np.uint8) * 255\n",
    "        \n",
    "        # Calculate position to place resized image (centered)\n",
    "        x_offset = (64 - new_w) // 2\n",
    "        y_offset = (64 - new_h) // 2\n",
    "        \n",
    "        # Place resized image\n",
    "        padded_char[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_char\n",
    "        \n",
    "        # Add to extracted characters\n",
    "        extracted_chars.append({\n",
    "            'image': padded_char,\n",
    "            'bbox': (x, y, w, h),\n",
    "            'index': i\n",
    "        })\n",
    "    \n",
    "    return extracted_chars, vis_image\n",
    "\n",
    "def recognize_characters(model, char_images, class_names, device=device):\n",
    "    \"\"\"\n",
    "    Recognize characters using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        char_images: List of character images\n",
    "        class_names: List of class names\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        list: Recognition results for each character\n",
    "    \"\"\"\n",
    "    if not char_images:\n",
    "        return []\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    # Define transform for model input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    for char_info in char_images:\n",
    "        # Convert to PIL image\n",
    "        char_pil = Image.fromarray(char_info['image'])\n",
    "        \n",
    "        # Apply transform\n",
    "        char_tensor = transform(char_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(char_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "            \n",
    "            predicted_class = class_names[predicted_idx.item()]\n",
    "            confidence_score = confidence.item()\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'character': predicted_class,\n",
    "            'confidence': confidence_score,\n",
    "            'bbox': char_info['bbox'],\n",
    "            'index': char_info['index']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_visualization(original_image, char_results, char_images=None):\n",
    "    \"\"\"\n",
    "    Create a visualization of the recognition results.\n",
    "    \n",
    "    Args:\n",
    "        original_image: Original input image\n",
    "        char_results: Character recognition results\n",
    "        char_images: Extracted character images (optional)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (result_image, extracted_grid)\n",
    "    \"\"\"\n",
    "    # Convert original image to RGB for visualization\n",
    "    if isinstance(original_image, Image.Image):\n",
    "        original_np = np.array(original_image)\n",
    "        if len(original_np.shape) == 2:  # Grayscale\n",
    "            vis_image = cv2.cvtColor(original_np, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            vis_image = original_np.copy()\n",
    "    else:\n",
    "        if len(original_image.shape) == 2:  # Grayscale\n",
    "            vis_image = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            vis_image = original_image.copy()\n",
    "    \n",
    "    # Draw bounding boxes and predictions\n",
    "    for result in char_results:\n",
    "        x, y, w, h = result['bbox']\n",
    "        char = result['character']\n",
    "        conf = result['confidence']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(vis_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw prediction text\n",
    "        text = f\"{char} ({conf:.2f})\"\n",
    "        cv2.putText(vis_image, text, (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "    \n",
    "    # Create a grid of extracted characters if available\n",
    "    extracted_grid = None\n",
    "    if char_images is not None and len(char_images) > 0:\n",
    "        # Determine grid size\n",
    "        n_chars = len(char_images)\n",
    "        grid_cols = min(8, n_chars)\n",
    "        grid_rows = (n_chars + grid_cols - 1) // grid_cols\n",
    "        \n",
    "        # Create grid\n",
    "        cell_size = 64\n",
    "        grid_width = grid_cols * cell_size\n",
    "        grid_height = grid_rows * cell_size\n",
    "        grid = np.ones((grid_height, grid_width), dtype=np.uint8) * 255\n",
    "        \n",
    "        # Place images in grid\n",
    "        for i, char_info in enumerate(char_images):\n",
    "            if i >= grid_rows * grid_cols:\n",
    "                break\n",
    "                \n",
    "            row = i // grid_cols\n",
    "            col = i % grid_cols\n",
    "            \n",
    "            y_start = row * cell_size\n",
    "            x_start = col * cell_size\n",
    "            \n",
    "            grid[y_start:y_start+cell_size, x_start:x_start+cell_size] = char_info['image']\n",
    "        \n",
    "        # Convert to RGB\n",
    "        extracted_grid = cv2.cvtColor(grid, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Add character labels\n",
    "        for i, result in enumerate(char_results):\n",
    "            if i >= grid_rows * grid_cols:\n",
    "                break\n",
    "                \n",
    "            row = i // grid_cols\n",
    "            col = i % grid_cols\n",
    "            \n",
    "            x_text = col * cell_size + 5\n",
    "            y_text = row * cell_size + 15\n",
    "            \n",
    "            cv2.putText(extracted_grid, result['character'], (x_text, y_text), \n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    \n",
    "    return vis_image, extracted_grid\n",
    "\n",
    "def enhance_image(image, enhancement_type):\n",
    "    \"\"\"\n",
    "    Apply various image enhancements for better character recognition.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (numpy array or PIL Image)\n",
    "        enhancement_type: Type of enhancement to apply\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Enhanced image\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image_np = image.copy()\n",
    "    \n",
    "    # Ensure grayscale\n",
    "    if len(image_np.shape) == 3:\n",
    "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image_np.copy()\n",
    "    \n",
    "    if enhancement_type == \"none\":\n",
    "        # No enhancement\n",
    "        return gray\n",
    "    \n",
    "    elif enhancement_type == \"threshold\":\n",
    "        # Otsu's thresholding\n",
    "        _, enhanced = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return enhanced\n",
    "    \n",
    "    elif enhancement_type == \"adaptive\":\n",
    "        # Adaptive thresholding\n",
    "        enhanced = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "        return enhanced\n",
    "    \n",
    "    elif enhancement_type == \"contrast\":\n",
    "        # Contrast enhancement\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced = clahe.apply(gray)\n",
    "        return enhanced\n",
    "    \n",
    "    elif enhancement_type == \"denoise\":\n",
    "        # Denoising\n",
    "        enhanced = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "        return enhanced\n",
    "    \n",
    "    elif enhancement_type == \"sharpen\":\n",
    "        # Sharpening\n",
    "        kernel = np.array([[-1, -1, -1],\n",
    "                         [-1,  9, -1],\n",
    "                         [-1, -1, -1]])\n",
    "        enhanced = cv2.filter2D(gray, -1, kernel)\n",
    "        return enhanced\n",
    "    \n",
    "    return gray  # Default fallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Gradio Interface Components ---\n",
    "\"\"\"\n",
    "## Gradio Interface Components\n",
    "\n",
    "These functions build the individual components of the Gradio interface.\n",
    "Each component handles a specific aspect of the user interaction.\n",
    "\"\"\"\n",
    "\n",
    "def create_model_dropdown():\n",
    "    \"\"\"Create a dropdown for model selection.\"\"\"\n",
    "    # Find available models\n",
    "    available_models = find_available_models()\n",
    "    \n",
    "    if not available_models:\n",
    "        # Create a default dropdown with placeholder if no models found\n",
    "        return gr.Dropdown(\n",
    "            choices=[\"No models found - Please train models first\"],\n",
    "            value=\"No models found - Please train models first\",\n",
    "            label=\"Model Selection\",\n",
    "            info=\"No trained models were found. Please run the training notebook first.\"\n",
    "        )\n",
    "    \n",
    "    # Create a list of choices from available models\n",
    "    model_choices = list(available_models.keys())\n",
    "    \n",
    "    # Add descriptive labels\n",
    "    model_labels = []\n",
    "    for model_name in model_choices:\n",
    "        # Try to get model info if available\n",
    "        model_info_path = available_models[model_name].get('model_info_path')\n",
    "        if model_info_path and os.path.exists(model_info_path):\n",
    "            try:\n",
    "                with open(model_info_path, 'r') as f:\n",
    "                    info_text = f.read()\n",
    "                    # Extract a short description\n",
    "                    desc_line = [line for line in info_text.split('\\n') if line.startswith('Description:')]\n",
    "                    if desc_line:\n",
    "                        description = desc_line[0].replace('Description:', '').strip()\n",
    "                        model_labels.append(f\"{model_name} - {description}\")\n",
    "                    else:\n",
    "                        model_labels.append(model_name)\n",
    "            except:\n",
    "                model_labels.append(model_name)\n",
    "        else:\n",
    "            model_labels.append(model_name)\n",
    "    \n",
    "    # Create mapping of labels to model names\n",
    "    label_to_model = {label: model for label, model in zip(model_labels, model_choices)}\n",
    "    \n",
    "    return gr.Dropdown(\n",
    "        choices=model_labels,\n",
    "        value=model_labels[0] if model_labels else None,\n",
    "        label=\"Model Selection\",\n",
    "        info=\"Select a trained model to use for recognition\",\n",
    "    ), label_to_model\n",
    "\n",
    "def create_model_info_box(label_to_model):\n",
    "    \"\"\"Create an info box for displaying model details.\"\"\"\n",
    "    # Function to load and display model info\n",
    "    def load_model_info(model_label):\n",
    "        if not model_label or \"No models found\" in model_label:\n",
    "            return \"No model selected or no models available.\"\n",
    "        \n",
    "        # Extract model name from label\n",
    "        model_name = label_to_model.get(model_label, model_label)\n",
    "        if not model_name:\n",
    "            return \"Invalid model selection.\"\n",
    "        \n",
    "        # Find available models to get info path\n",
    "        available_models = find_available_models()\n",
    "        if model_name not in available_models:\n",
    "            return f\"Model '{model_name}' not found in available models.\"\n",
    "        \n",
    "        model_info = available_models[model_name]\n",
    "        \n",
    "        # Check if model info file exists\n",
    "        if not model_info.get('has_model_info'):\n",
    "            # No info file, provide basic details\n",
    "            checkpoint_files = model_info.get('checkpoint_files', [])\n",
    "            class_names_path = model_info.get('class_names_path')\n",
    "            \n",
    "            # Count classes if available\n",
    "            num_classes = \"Unknown\"\n",
    "            if class_names_path and os.path.exists(class_names_path):\n",
    "                try:\n",
    "                    with open(class_names_path, 'r') as f:\n",
    "                        class_names = [line.strip() for line in f if line.strip()]\n",
    "                        num_classes = len(class_names)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            info_text = f\"Model: {model_name}\\n\"\n",
    "            info_text += f\"Number of classes: {num_classes}\\n\"\n",
    "            info_text += f\"Available checkpoints: {len(checkpoint_files)}\\n\"\n",
    "            \n",
    "            if checkpoint_files:\n",
    "                # Try to extract some info from checkpoint\n",
    "                latest_checkpoint = checkpoint_files[-1]\n",
    "                try:\n",
    "                    checkpoint = torch.load(latest_checkpoint, map_location='cpu')\n",
    "                    if isinstance(checkpoint, dict):\n",
    "                        if 'epoch' in checkpoint:\n",
    "                            info_text += f\"Trained epochs: {checkpoint['epoch']}\\n\"\n",
    "                        if 'accuracy' in checkpoint or 'val_acc' in checkpoint:\n",
    "                            acc = checkpoint.get('accuracy', checkpoint.get('val_acc', 'Unknown'))\n",
    "                            info_text += f\"Validation accuracy: {acc}\\n\"\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return info_text\n",
    "        else:\n",
    "            # Read from info file\n",
    "            info_path = model_info.get('model_info_path')\n",
    "            try:\n",
    "                with open(info_path, 'r') as f:\n",
    "                    info_text = f.read()\n",
    "                return info_text\n",
    "            except:\n",
    "                return f\"Error reading model info for '{model_name}'.\"\n",
    "    \n",
    "    return gr.Textbox(\n",
    "        value=\"Select a model to see details.\",\n",
    "        label=\"Model Information\",\n",
    "        lines=6,\n",
    "        interactive=False\n",
    "    ), load_model_info\n",
    "\n",
    "def create_settings_accordion():\n",
    "    \"\"\"Create accordion with settings for character recognition.\"\"\"\n",
    "    with gr.Accordion(\"Recognition Settings\", open=False) as settings_accordion:\n",
    "        with gr.Row():\n",
    "            checkpoint_type = gr.Radio(\n",
    "                choices=[\"best\", \"final\"],\n",
    "                value=\"best\",\n",
    "                label=\"Checkpoint Type\",\n",
    "                info=\"Which model checkpoint to use\"\n",
    "            )\n",
    "            min_area = gr.Slider(\n",
    "                minimum=10,\n",
    "                maximum=200,\n",
    "                value=50,\n",
    "                step=5,\n",
    "                label=\"Minimum Character Area\",\n",
    "                info=\"Minimum area (in pixels) to consider as a character\"\n",
    "            )\n",
    "        \n",
    "        with gr.Row():\n",
    "            enhancement = gr.Dropdown(\n",
    "                choices=[\"none\", \"threshold\", \"adaptive\", \"contrast\", \"denoise\", \"sharpen\"],\n",
    "                value=\"none\",\n",
    "                label=\"Image Enhancement\",\n",
    "                info=\"Apply preprocessing to enhance image quality\"\n",
    "            )\n",
    "            spacing = gr.Slider(\n",
    "                minimum=0,\n",
    "                maximum=32,\n",
    "                value=24,\n",
    "                step=4,\n",
    "                label=\"Character Spacing\",\n",
    "                info=\"Spacing around characters when normalizing (pixels)\"\n",
    "            )\n",
    "    \n",
    "    return settings_accordion, checkpoint_type, min_area, enhancement, spacing\n",
    "\n",
    "def create_debug_accordion():\n",
    "    \"\"\"Create accordion with debug information and visualization options.\"\"\"\n",
    "    with gr.Accordion(\"Debug & Visualization\", open=False) as debug_accordion:\n",
    "        with gr.Row():\n",
    "            show_steps = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"Show Processing Steps\",\n",
    "                info=\"Display intermediate processing steps\"\n",
    "            )\n",
    "            show_confidence = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"Show Confidence Scores\",\n",
    "                info=\"Display confidence scores for each character\"\n",
    "            )\n",
    "        \n",
    "        with gr.Row():\n",
    "            debug_info = gr.Textbox(\n",
    "                value=\"Debug information will appear here.\",\n",
    "                label=\"Debug Information\",\n",
    "                lines=5,\n",
    "                interactive=False\n",
    "            )\n",
    "    \n",
    "    return debug_accordion, show_steps, show_confidence, debug_info\n",
    "\n",
    "def create_single_image_tab(label_to_model, load_model_info, settings):\n",
    "    \"\"\"Create tab for single image recognition (upload and draw).\"\"\"\n",
    "    checkpoint_type, min_area, enhancement, spacing = settings\n",
    "    \n",
    "    with gr.Tab(\"Upload Image\") as upload_tab:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                image_input = gr.Image(\n",
    "                    type=\"numpy\",\n",
    "                    label=\"Upload Image\",\n",
    "                    tool=\"select\"\n",
    "                )\n",
    "                upload_button = gr.Button(\"Recognize Characters\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Column(scale=5):\n",
    "                with gr.Row():\n",
    "                    result_image = gr.Image(\n",
    "                        type=\"numpy\",\n",
    "                        label=\"Recognition Result\"\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    extracted_grid = gr.Image(\n",
    "                        type=\"numpy\",\n",
    "                        label=\"Extracted Characters\"\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    result_text = gr.Textbox(\n",
    "                        value=\"\",\n",
    "                        label=\"Recognized Text\",\n",
    "                        lines=1\n",
    "                    )\n",
    "    \n",
    "    with gr.Tab(\"Draw Text\") as draw_tab:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                canvas_input = gr.Image(\n",
    "                    type=\"numpy\",\n",
    "                    label=\"Draw Text\",\n",
    "                    tool=\"sketch\",\n",
    "                    height=180,\n",
    "                    brush_radius=4,\n",
    "                    source=\"canvas\",\n",
    "                    shape=(180, 600, 3),\n",
    "                    invert_colors=False\n",
    "                )\n",
    "                draw_button = gr.Button(\"Recognize Characters\", variant=\"primary\")\n",
    "                clear_button = gr.Button(\"Clear Canvas\")\n",
    "            \n",
    "            with gr.Column(scale=5):\n",
    "                with gr.Row():\n",
    "                    draw_result_image = gr.Image(\n",
    "                        type=\"numpy\",\n",
    "                        label=\"Recognition Result\"\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    draw_extracted_grid = gr.Image(\n",
    "                        type=\"numpy\",\n",
    "                        label=\"Extracted Characters\"\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    draw_result_text = gr.Textbox(\n",
    "                        value=\"\",\n",
    "                        label=\"Recognized Text\",\n",
    "                        lines=1\n",
    "                    )\n",
    "    \n",
    "    with gr.Tab(\"Camera Capture\") as camera_tab:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                camera_input = gr.Image(\n",
    "                    type=\"numpy\",\n",
    "                    label=\"Camera Capture\",\n",
    "                    sources=[\"webcam\", \"upload\"],\n",
    "                )\n",
    "                camera_button = gr.Button(\"Recognize Characters\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Column(scale=5):\n",
    "                with gr.Row():\n",
    "                    camera_result_image = gr.Image(\n",
    "                        type=\"numpy\",\n",
    "                        label=\"Recognition Result\"\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    camera_extracted_grid = gr.Image(\n",
    "                        type=\"numpy\",\n",
    "                        label=\"Extracted Characters\"\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    camera_result_text = gr.Textbox(\n",
    "                        value=\"\",\n",
    "                        label=\"Recognized Text\",\n",
    "                        lines=1\n",
    "                    )\n",
    "    \n",
    "    return {\n",
    "        'upload': {\n",
    "            'tab': upload_tab,\n",
    "            'input': image_input,\n",
    "            'button': upload_button,\n",
    "            'result_image': result_image,\n",
    "            'extracted_grid': extracted_grid,\n",
    "            'result_text': result_text\n",
    "        },\n",
    "        'draw': {\n",
    "            'tab': draw_tab,\n",
    "            'input': canvas_input,\n",
    "            'button': draw_button,\n",
    "            'clear_button': clear_button,\n",
    "            'result_image': draw_result_image,\n",
    "            'extracted_grid': draw_extracted_grid,\n",
    "            'result_text': draw_result_text\n",
    "        },\n",
    "        'camera': {\n",
    "            'tab': camera_tab,\n",
    "            'input': camera_input,\n",
    "            'button': camera_button,\n",
    "            'result_image': camera_result_image,\n",
    "            'extracted_grid': camera_extracted_grid,\n",
    "            'result_text': camera_result_text\n",
    "        }\n",
    "    }\n",
    "\n",
    "def create_batch_processing_tab(label_to_model, load_model_info, settings):\n",
    "    \"\"\"Create tab for batch processing of multiple images.\"\"\"\n",
    "    checkpoint_type, min_area, enhancement, spacing = settings\n",
    "    \n",
    "    with gr.Tab(\"Batch Processing\") as batch_tab:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                batch_input = gr.File(\n",
    "                    file_types=[\"image\"],\n",
    "                    file_count=\"multiple\",\n",
    "                    label=\"Upload Multiple Images\"\n",
    "                )\n",
    "                batch_button = gr.Button(\"Process Batch\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Column(scale=4):\n",
    "                batch_results = gr.Gallery(\n",
    "                    label=\"Batch Results\",\n",
    "                    columns=3,\n",
    "                    height=\"auto\",\n",
    "                    object_fit=\"contain\"\n",
    "                )\n",
    "        \n",
    "        with gr.Row():\n",
    "            batch_output = gr.Dataframe(\n",
    "                headers=[\"Filename\", \"Recognized Text\", \"Confidence\"],\n",
    "                label=\"Batch Recognition Results\",\n",
    "                wrap=True,\n",
    "                row_count=10,\n",
    "                col_count=(3, \"fixed\")\n",
    "            )\n",
    "            \n",
    "        with gr.Row():\n",
    "            batch_download = gr.File(\n",
    "                label=\"Download Results (CSV)\",\n",
    "                file_types=[\".csv\"],\n",
    "                interactive=False\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        'tab': batch_tab,\n",
    "        'input': batch_input,\n",
    "        'button': batch_button,\n",
    "        'results': batch_results,\n",
    "        'output': batch_output,\n",
    "        'download': batch_download\n",
    "    }\n",
    "\n",
    "def create_examples_section():\n",
    "    \"\"\"Create examples section with sample images for demonstration.\"\"\"\n",
    "    example_dir = \"example_images\"\n",
    "    \n",
    "    # Check if examples directory exists\n",
    "    if not os.path.exists(example_dir):\n",
    "        os.makedirs(example_dir, exist_ok=True)\n",
    "        \n",
    "        # Create some example images if none exist\n",
    "        try:\n",
    "            # Example 1: Simple text\n",
    "            example1 = np.ones((100, 300), dtype=np.uint8) * 255\n",
    "            cv2.putText(example1, \"Hello123\", (50, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 2)\n",
    "            cv2.imwrite(os.path.join(example_dir, \"example1.png\"), example1)\n",
    "            \n",
    "            # Example 2: Mixed characters\n",
    "            example2 = np.ones((100, 300), dtype=np.uint8) * 255\n",
    "            cv2.putText(example2, \"Abc123\", (50, 60), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1.5, (0, 0, 0), 2)\n",
    "            cv2.imwrite(os.path.join(example_dir, \"example2.png\"), example2)\n",
    "            \n",
    "            # Example 3: Spaced characters\n",
    "            example3 = np.ones((100, 300), dtype=np.uint8) * 255\n",
    "            cv2.putText(example3, \"A B C 1 2 3\", (20, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 2)\n",
    "            cv2.imwrite(os.path.join(example_dir, \"example3.png\"), example3)\n",
    "        except:\n",
    "            print(\"Could not create example images\")\n",
    "    \n",
    "    # Find existing examples\n",
    "    example_images = []\n",
    "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
    "        example_images.extend(glob(os.path.join(example_dir, ext)))\n",
    "    \n",
    "    return gr.Examples(\n",
    "        examples=example_images,\n",
    "        inputs=[\"image\"],\n",
    "        label=\"Example Images\",\n",
    "        examples_per_page=5\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Main Recognition Functions ---\n",
    "\"\"\"\n",
    "## Main Recognition Functions\n",
    "\n",
    "These functions handle the core recognition logic for different input methods.\n",
    "They coordinate loading models, processing images, and generating results.\n",
    "\"\"\"\n",
    "\n",
    "def recognize_image(image, model_label, checkpoint_type, min_area, enhancement_type, spacing, \n",
    "                  show_steps, show_confidence, debug_info, label_to_model):\n",
    "    \"\"\"\n",
    "    Process an uploaded image and recognize characters.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image from Gradio\n",
    "        model_label: Selected model from dropdown\n",
    "        checkpoint_type: Type of checkpoint to use\n",
    "        min_area: Minimum contour area to consider\n",
    "        enhancement_type: Type of image enhancement to apply\n",
    "        spacing: Character spacing for normalization\n",
    "        show_steps: Whether to show processing steps\n",
    "        show_confidence: Whether to show confidence scores\n",
    "        debug_info: Debug info text box\n",
    "        label_to_model: Mapping from labels to model names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (result_image, extracted_grid, result_text)\n",
    "    \"\"\"\n",
    "    debug_text = \"Processing image...\\n\"\n",
    "    \n",
    "    # Check if an image was provided\n",
    "    if image is None:\n",
    "        debug_text += \"ERROR: No image provided.\"\n",
    "        return None, None, \"\", debug_text\n",
    "    \n",
    "    # Extract model name from label\n",
    "    model_name = label_to_model.get(model_label, model_label)\n",
    "    debug_text += f\"Using model: {model_name}\\n\"\n",
    "    \n",
    "    try:\n",
    "        # Find available models\n",
    "        available_models = find_available_models()\n",
    "        \n",
    "        if model_name not in available_models:\n",
    "            debug_text += f\"ERROR: Model '{model_name}' not found.\"\n",
    "            return None, None, \"\", debug_text\n",
    "        \n",
    "        model_info = available_models[model_name]\n",
    "        \n",
    "        # Determine checkpoint path\n",
    "        checkpoint_path = None\n",
    "        \n",
    "        if checkpoint_type == \"best\":\n",
    "            # Look for best_model.pth\n",
    "            best_paths = [cp for cp in model_info['checkpoint_files'] if 'best' in os.path.basename(cp).lower()]\n",
    "            if best_paths:\n",
    "                checkpoint_path = best_paths[0]\n",
    "            else:\n",
    "                debug_text += \"WARNING: No 'best' checkpoint found, using first available checkpoint.\\n\"\n",
    "                checkpoint_path = model_info['checkpoint_files'][0]\n",
    "        else:  # \"final\"\n",
    "            # Look for final_model.pth\n",
    "            final_paths = [cp for cp in model_info['checkpoint_files'] if 'final' in os.path.basename(cp).lower()]\n",
    "            if final_paths:\n",
    "                checkpoint_path = final_paths[0]\n",
    "            else:\n",
    "                debug_text += \"WARNING: No 'final' checkpoint found, using first available checkpoint.\\n\"\n",
    "                checkpoint_path = model_info['checkpoint_files'][0]\n",
    "        \n",
    "        # Load class names\n",
    "        if model_info['has_class_names']:\n",
    "            class_names = load_class_names(model_info['class_names_path'])\n",
    "        else:\n",
    "            debug_text += \"WARNING: No class_names.txt found, using default class names.\\n\"\n",
    "            class_names = get_default_class_names()\n",
    "        \n",
    "        if class_names is None:\n",
    "            debug_text += \"ERROR: Failed to load class names.\"\n",
    "            return None, None, \"\", debug_text\n",
    "        \n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model_checkpoint(checkpoint_path, model_name, num_classes, device)\n",
    "        \n",
    "        if model is None:\n",
    "            debug_text += \"ERROR: Failed to load model.\"\n",
    "            return None, None, \"\", debug_text\n",
    "        \n",
    "        # Preprocess image\n",
    "        debug_text += \"Preprocessing image...\\n\"\n",
    "        \n",
    "        # Apply enhancement if requested\n",
    "        if enhancement_type != \"none\":\n",
    "            debug_text += f\"Applying {enhancement_type} enhancement...\\n\"\n",
    "            enhanced_image = enhance_image(image, enhancement_type)\n",
    "        else:\n",
    "            enhanced_image = image\n",
    "        \n",
    "        # Extract characters\n",
    "        debug_text += \"Extracting characters...\\n\"\n",
    "        char_images, vis_image = extract_characters(enhanced_image, min_area=min_area, spacing=spacing)\n",
    "        \n",
    "        if not char_images:\n",
    "            debug_text += \"WARNING: No characters detected in the image.\"\n",
    "            return vis_image, None, \"\", debug_text\n",
    "        \n",
    "        debug_text += f\"Found {len(char_images)} potential characters.\\n\"\n",
    "        \n",
    "        # Recognize characters\n",
    "        debug_text += \"Recognizing characters...\\n\"\n",
    "        char_results = recognize_characters(model, char_images, class_names, device)\n",
    "        \n",
    "        # Create result text\n",
    "        result_text = ''.join(result['character'] for result in char_results)\n",
    "        \n",
    "        # Create visualization\n",
    "        debug_text += \"Creating visualization...\\n\"\n",
    "        result_image, extracted_grid = create_visualization(enhanced_image, char_results, char_images)\n",
    "        \n",
    "        debug_text += f\"Recognition complete. Detected text: {result_text}\\n\"\n",
    "        \n",
    "        return result_image, extracted_grid, result_text, debug_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        debug_text += f\"ERROR: {str(e)}\\n\"\n",
    "        debug_text += traceback.format_exc()\n",
    "        return None, None, \"\", debug_text\n",
    "\n",
    "def process_batch_images(batch_files, model_label, checkpoint_type, min_area, enhancement_type, \n",
    "                        spacing, label_to_model):\n",
    "    \"\"\"\n",
    "    Process a batch of images for character recognition.\n",
    "    \n",
    "    Args:\n",
    "        batch_files: List of uploaded files\n",
    "        model_label: Selected model from dropdown\n",
    "        checkpoint_type: Type of checkpoint to use\n",
    "        min_area: Minimum contour area to consider\n",
    "        enhancement_type: Type of image enhancement to apply\n",
    "        spacing: Character spacing for normalization\n",
    "        label_to_model: Mapping from labels to model names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (gallery_items, results_df, csv_file)\n",
    "    \"\"\"\n",
    "    if not batch_files:\n",
    "        return [], None, None\n",
    "    \n",
    "    # Extract model name from label\n",
    "    model_name = label_to_model.get(model_label, model_label)\n",
    "    \n",
    "    try:\n",
    "        # Find available models\n",
    "        available_models = find_available_models()\n",
    "        \n",
    "        if model_name not in available_models:\n",
    "            return [], None, None\n",
    "        \n",
    "        model_info = available_models[model_name]\n",
    "        \n",
    "        # Determine checkpoint path\n",
    "        checkpoint_path = None\n",
    "        \n",
    "        if checkpoint_type == \"best\":\n",
    "            # Look for best_model.pth\n",
    "            best_paths = [cp for cp in model_info['checkpoint_files'] if 'best' in os.path.basename(cp).lower()]\n",
    "            if best_paths:\n",
    "                checkpoint_path = best_paths[0]\n",
    "            else:\n",
    "                checkpoint_path = model_info['checkpoint_files'][0]\n",
    "        else:  # \"final\"\n",
    "            # Look for final_model.pth\n",
    "            final_paths = [cp for cp in model_info['checkpoint_files'] if 'final' in os.path.basename(cp).lower()]\n",
    "            if final_paths:\n",
    "                checkpoint_path = final_paths[0]\n",
    "            else:\n",
    "                checkpoint_path = model_info['checkpoint_files'][0]\n",
    "        \n",
    "        # Load class names\n",
    "        if model_info['has_class_names']:\n",
    "            class_names = load_class_names(model_info['class_names_path'])\n",
    "        else:\n",
    "            class_names = get_default_class_names()\n",
    "        \n",
    "        if class_names is None:\n",
    "            return [], None, None\n",
    "        \n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model_checkpoint(checkpoint_path, model_name, num_classes, device)\n",
    "        \n",
    "        if model is None:\n",
    "            return [], None, None\n",
    "        \n",
    "        # Process each image\n",
    "        gallery_items = []\n",
    "        results = []\n",
    "        \n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                # Load image\n",
    "                image = Image.open(file.name)\n",
    "                filename = os.path.basename(file.name)\n",
    "                \n",
    "                # Apply enhancement if requested\n",
    "                if enhancement_type != \"none\":\n",
    "                    enhanced_image = enhance_image(image, enhancement_type)\n",
    "                else:\n",
    "                    enhanced_image = image\n",
    "                \n",
    "                # Extract characters\n",
    "                char_images, vis_image = extract_characters(enhanced_image, min_area=min_area, spacing=spacing)\n",
    "                \n",
    "                if not char_images:\n",
    "                    # No characters detected\n",
    "                    results.append({\n",
    "                        'filename': filename,\n",
    "                        'text': \"No characters detected\",\n",
    "                        'confidence': 0.0\n",
    "                    })\n",
    "                    \n",
    "                    # Add to gallery\n",
    "                    gallery_items.append((file.name, vis_image))\n",
    "                    continue\n",
    "                \n",
    "                # Recognize characters\n",
    "                char_results = recognize_characters(model, char_images, class_names, device)\n",
    "                \n",
    "                # Create result text\n",
    "                result_text = ''.join(result['character'] for result in char_results)\n",
    "                \n",
    "                # Calculate average confidence\n",
    "                avg_confidence = sum(result['confidence'] for result in char_results) / len(char_results)\n",
    "                \n",
    "                # Create visualization\n",
    "                result_image, _ = create_visualization(enhanced_image, char_results, char_images)\n",
    "                \n",
    "                # Add to results\n",
    "                results.append({\n",
    "                    'filename': filename,\n",
    "                    'text': result_text,\n",
    "                    'confidence': avg_confidence\n",
    "                })\n",
    "                \n",
    "                # Add to gallery\n",
    "                gallery_items.append((file.name, result_image))\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Add error result\n",
    "                results.append({\n",
    "                    'filename': os.path.basename(file.name),\n",
    "                    'text': f\"Error: {str(e)}\",\n",
    "                    'confidence': 0.0\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame for display\n",
    "        results_df = [(r['filename'], r['text'], f\"{r['confidence']:.2f}\") for r in results]\n",
    "        \n",
    "        # Create CSV file\n",
    "        csv_content = \"Filename,Recognized Text,Confidence\\n\"\n",
    "        for r in results:\n",
    "            csv_content += f\"\\\"{r['filename']}\\\",\\\"{r['text']}\\\",{r['confidence']:.4f}\\n\"\n",
    "        \n",
    "        csv_file = os.path.join(\"gradio_results\", \"batch\", \"batch_results.csv\")\n",
    "        os.makedirs(os.path.dirname(csv_file), exist_ok=True)\n",
    "        \n",
    "        with open(csv_file, 'w') as f:\n",
    "            f.write(csv_content)\n",
    "        \n",
    "        return gallery_items, results_df, csv_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in batch processing: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return [], None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Build Complete Gradio Interface ---\n",
    "\"\"\"\n",
    "## Build Complete Gradio Interface\n",
    "\n",
    "This section assembles all the components into a complete interactive application.\n",
    "It defines the layout, sets up event handlers, and launches the interface.\n",
    "\"\"\"\n",
    "\n",
    "def build_interface():\n",
    "    \"\"\"Build and launch the complete Gradio interface.\"\"\"\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Handwritten Character Recognition\") as app:\n",
    "        gr.Markdown(\"# Handwritten Character Recognition\")\n",
    "        gr.Markdown(\"\"\"\n",
    "        This application recognizes handwritten characters using deep learning models. \n",
    "        You can upload an image, draw text, or capture from your camera.\n",
    "        \n",
    "        Select a model from the dropdown and use the tabs to access different input methods.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create model selection components\n",
    "        model_dropdown, label_to_model = create_model_dropdown()\n",
    "        model_info, load_model_info = create_model_info_box(label_to_model)\n",
    "        \n",
    "        # Create settings accordion\n",
    "        settings_accordion, checkpoint_type, min_area, enhancement, spacing = create_settings_accordion()\n",
    "        \n",
    "        # Create debug accordion\n",
    "        debug_accordion, show_steps, show_confidence, debug_info = create_debug_accordion()\n",
    "        \n",
    "        # Create tabs for different input methods\n",
    "        with gr.Tabs():\n",
    "            # Single image tabs (upload, draw, camera)\n",
    "            single_tabs = create_single_image_tab(\n",
    "                label_to_model,\n",
    "                load_model_info,\n",
    "                (checkpoint_type, min_area, enhancement, spacing)\n",
    "            )\n",
    "            \n",
    "            # Batch processing tab\n",
    "            batch_tab = create_batch_processing_tab(\n",
    "                label_to_model,\n",
    "                load_model_info,\n",
    "                (checkpoint_type, min_area, enhancement, spacing)\n",
    "            )\n",
    "        \n",
    "        # Add examples section\n",
    "        examples = create_examples_section()\n",
    "        \n",
    "        # Set up event handlers\n",
    "        \n",
    "        # Update model info when selection changes\n",
    "        model_dropdown.change(\n",
    "            fn=load_model_info,\n",
    "            inputs=[model_dropdown],\n",
    "            outputs=[model_info]\n",
    "        )\n",
    "        \n",
    "        # Upload tab\n",
    "        single_tabs['upload']['button'].click(\n",
    "            fn=recognize_image,\n",
    "            inputs=[\n",
    "                single_tabs['upload']['input'],\n",
    "                model_dropdown,\n",
    "                checkpoint_type,\n",
    "                min_area,\n",
    "                enhancement,\n",
    "                spacing,\n",
    "                show_steps,\n",
    "                show_confidence,\n",
    "                debug_info,\n",
    "                gr.State(label_to_model)\n",
    "            ],\n",
    "            outputs=[\n",
    "                single_tabs['upload']['result_image'],\n",
    "                single_tabs['upload']['extracted_grid'],\n",
    "                single_tabs['upload']['result_text'],\n",
    "                debug_info\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Draw tab\n",
    "        single_tabs['draw']['button'].click(\n",
    "            fn=recognize_image,\n",
    "            inputs=[\n",
    "                single_tabs['draw']['input'],\n",
    "                model_dropdown,\n",
    "                checkpoint_type,\n",
    "                min_area,\n",
    "                enhancement,\n",
    "                spacing,\n",
    "                show_steps,\n",
    "                show_confidence,\n",
    "                debug_info,\n",
    "                gr.State(label_to_model)\n",
    "            ],\n",
    "            outputs=[\n",
    "                single_tabs['draw']['result_image'],\n",
    "                single_tabs['draw']['extracted_grid'],\n",
    "                single_tabs['draw']['result_text'],\n",
    "                debug_info\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Clear drawing canvas\n",
    "        single_tabs['draw']['clear_button'].click(\n",
    "            fn=lambda: (None, None, \"\", \"Canvas cleared.\"),\n",
    "            inputs=[],\n",
    "            outputs=[\n",
    "                single_tabs['draw']['result_image'],\n",
    "                single_tabs['draw']['extracted_grid'],\n",
    "                single_tabs['draw']['result_text'],\n",
    "                debug_info\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Camera tab\n",
    "        single_tabs['camera']['button'].click(\n",
    "            fn=recognize_image,\n",
    "            inputs=[\n",
    "                single_tabs['camera']['input'],\n",
    "                model_dropdown,\n",
    "                checkpoint_type,\n",
    "                min_area,\n",
    "                enhancement,\n",
    "                spacing,\n",
    "                show_steps,\n",
    "                show_confidence,\n",
    "                debug_info,\n",
    "                gr.State(label_to_model)\n",
    "            ],\n",
    "            outputs=[\n",
    "                single_tabs['camera']['result_image'],\n",
    "                single_tabs['camera']['extracted_grid'],\n",
    "                single_tabs['camera']['result_text'],\n",
    "                debug_info\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Batch tab\n",
    "        batch_tab['button'].click(\n",
    "            fn=process_batch_images,\n",
    "            inputs=[\n",
    "                batch_tab['input'],\n",
    "                model_dropdown,\n",
    "                checkpoint_type,\n",
    "                min_area,\n",
    "                enhancement,\n",
    "                spacing,\n",
    "                gr.State(label_to_model)\n",
    "            ],\n",
    "            outputs=[\n",
    "                batch_tab['results'],\n",
    "                batch_tab['output'],\n",
    "                batch_tab['download']\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Connect examples to upload tab\n",
    "        examples.dataset.click(\n",
    "            fn=lambda x: x,\n",
    "            inputs=[examples.components[0]],\n",
    "            outputs=[single_tabs['upload']['input']]\n",
    "        )\n",
    "        \n",
    "        # Add CSS for styling\n",
    "        gr.Markdown(\"\"\"\n",
    "        <style>\n",
    "        .gradio-container {\n",
    "            max-width: 1200px !important;\n",
    "            margin-left: auto !important;\n",
    "            margin-right: auto !important;\n",
    "        }\n",
    "        \n",
    "        .gr-button.gr-button-lg.gr-button-primary {\n",
    "            background-color: #2980b9 !important;\n",
    "        }\n",
    "        \n",
    "        .gr-button.gr-button-lg.gr-button-secondary {\n",
    "            background-color: #95a5a6 !important;\n",
    "        }\n",
    "        \n",
    "        .footer {\n",
    "            margin-top: 20px;\n",
    "            text-align: center;\n",
    "            font-size: 0.8em;\n",
    "            color: #7f8c8d;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Add footer\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div class=\"footer\">\n",
    "        <p>Handwritten Character Recognition Application</p>\n",
    "        <p>Created with Gradio, PyTorch, and OpenCV</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    \n",
    "    return app\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de929f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Launch Gradio Application ---\n",
    "\"\"\"\n",
    "## Launch Gradio Application\n",
    "\n",
    "This section launches the Gradio application with appropriate settings.\n",
    "It handles startup configurations and provides a public share link if requested.\n",
    "\"\"\"\n",
    "\n",
    "def launch_application(share=True, debug=False):\n",
    "    \"\"\"\n",
    "    Launch the Gradio application.\n",
    "    \n",
    "    Args:\n",
    "        share: Whether to create a public share link\n",
    "        debug: Whether to enable debug mode\n",
    "    \"\"\"\n",
    "    # Find available models first\n",
    "    available_models = find_available_models()\n",
    "    \n",
    "    if not available_models:\n",
    "        print(\"WARNING: No trained models found. Please run the training notebook first.\")\n",
    "        print(\"The application will start, but you won't be able to perform recognition.\")\n",
    "    else:\n",
    "        print(f\"Found {len(available_models)} trained models:\")\n",
    "        for model_name in available_models:\n",
    "            print(f\"  - {model_name}\")\n",
    "    \n",
    "    # Build the interface\n",
    "    app = build_interface()\n",
    "    \n",
    "    # Launch with specified settings\n",
    "    app.launch(\n",
    "        share=share,\n",
    "        debug=debug,\n",
    "        server_name=\"0.0.0.0\",  # Listen on all network interfaces\n",
    "        server_port=7860,\n",
    "        inbrowser=True,\n",
    "        show_api=False,\n",
    "        max_threads=40,\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch with share=True to generate a public URL\n",
    "    launch_application(share=True, debug=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a19ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Run Gradio Application ---\n",
    "\"\"\"\n",
    "## Run Gradio Application\n",
    "\n",
    "This is where you run the Gradio application.\n",
    "Uncomment and modify the code below to launch the application with your desired settings.\n",
    "\"\"\"\n",
    "\n",
    "# Launch the application\n",
    "# launch_application(share=True)  # Set share=True to get a public URL\n",
    "\n",
    "print(\"This notebook is ready to launch the Gradio application for handwritten character recognition.\")\n",
    "print(\"Execute this cell to start the application.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
