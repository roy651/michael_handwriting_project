{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9aeff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Import Libraries and Set Up Environment ---\n",
    "\"\"\"\n",
    "# Handwritten Character Recognition: Training Notebook\n",
    "\n",
    "This notebook focuses on training various model architectures for handwritten character recognition.\n",
    "It demonstrates how to set up a training pipeline, train different models, and save checkpoints.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import utility modules\n",
    "from data_utils_file import HandwritingDataPipeline, get_class_labels_from_directory\n",
    "from src.models_util import get_model, get_model_info\n",
    "from training_utils_file import train_model, load_checkpoint, test_model, plot_training_history, setup_training_components\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories for saving models and visualizations\n",
    "os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"training_plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Data Pipeline Configuration ---\n",
    "\"\"\"\n",
    "## Data Pipeline Configuration\n",
    "\n",
    "Configure the data pipeline for loading and preprocessing the dataset. This pipeline handles:\n",
    "- Loading images from disk\n",
    "- Applying transformations and augmentations\n",
    "- Splitting the dataset into training, validation, and test sets\n",
    "- Creating DataLoaders for batch processing\n",
    "\"\"\"\n",
    "\n",
    "def setup_data_pipeline(data_root, image_size=(64, 64), batch_size=32, \n",
    "                        train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
    "                        seed=42, device=device, do_transform=True, \n",
    "                        normalization_type='imagenet'):\n",
    "    \"\"\"Set up the data pipeline with the specified configuration.\"\"\"\n",
    "    print(f\"Setting up data pipeline with root: {data_root}\")\n",
    "    \n",
    "    try:\n",
    "        pipeline = HandwritingDataPipeline(\n",
    "            data_root=data_root,\n",
    "            image_size=image_size,\n",
    "            batch_size=batch_size,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "            seed=seed,\n",
    "            device=device,\n",
    "            do_transform=do_transform,\n",
    "            normalization_type=normalization_type\n",
    "        )\n",
    "        \n",
    "        train_loader, val_loader, test_loader = pipeline.get_loaders()\n",
    "        sizes = pipeline.get_sizes()\n",
    "        class_names = pipeline.get_class_labels()\n",
    "        \n",
    "        print(f\"Dataset successfully loaded with {len(class_names)} classes\")\n",
    "        print(f\"Split sizes: Train={sizes['train']}, Val={sizes['val']}, Test={sizes['test']}\")\n",
    "        \n",
    "        return pipeline, train_loader, val_loader, test_loader, class_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up data pipeline: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Example usage (commented out, replace with actual data path):\n",
    "# DATA_ROOT = \"./datasets/handwritten-english/augmented_images1\"\n",
    "# pipeline, train_loader, val_loader, test_loader, class_names = setup_data_pipeline(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Visualize Augmented Images ---\n",
    "\"\"\"\n",
    "## Visualize Augmented Images\n",
    "\n",
    "Visualize the effect of data augmentation on training images. This helps understand\n",
    "how the augmentation pipeline transforms the images, which can improve model generalization.\n",
    "\"\"\"\n",
    "\n",
    "def display_augmented_images(data_loader, num_images=5, num_augmentations=3):\n",
    "    \"\"\"\n",
    "    Display original images and their augmented versions.\n",
    "    \n",
    "    Args:\n",
    "        data_loader: DataLoader for the training set\n",
    "        num_images: Number of unique images to display\n",
    "        num_augmentations: Number of augmented versions to show per image\n",
    "    \"\"\"\n",
    "    if data_loader is None or not hasattr(data_loader, 'dataset'):\n",
    "        print(\"Data loader is not available or not properly initialized.\")\n",
    "        return\n",
    "    \n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(data_loader))\n",
    "    \n",
    "    # Get class names\n",
    "    if hasattr(data_loader.dataset, 'subset') and hasattr(data_loader.dataset.subset, 'dataset'):\n",
    "        try:\n",
    "            dataset = data_loader.dataset.subset.dataset\n",
    "            if hasattr(dataset, 'class_to_idx'):\n",
    "                idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "                class_names = [idx_to_class.get(i, f\"Class {i}\") for i in range(len(idx_to_class))]\n",
    "            else:\n",
    "                class_names = [f\"Class {i}\" for i in range(10)]  # Fallback\n",
    "        except:\n",
    "            class_names = [f\"Class {i}\" for i in range(10)]  # Fallback\n",
    "    else:\n",
    "        class_names = [f\"Class {i}\" for i in range(10)]  # Fallback\n",
    "    \n",
    "    # Limit to the requested number of images\n",
    "    num_images = min(num_images, len(images))\n",
    "    \n",
    "    fig = plt.figure(figsize=(num_augmentations * 3, num_images * 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Get the original image and label\n",
    "        img = images[i].cpu()\n",
    "        label = labels[i].item()\n",
    "        class_name = class_names[label] if label < len(class_names) else f\"Class {label}\"\n",
    "        \n",
    "        # Display the original image\n",
    "        ax = plt.subplot(num_images, num_augmentations + 1, i * (num_augmentations + 1) + 1)\n",
    "        img_display = img.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        # Unnormalize the image for display\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_display = std * img_display + mean\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "        \n",
    "        plt.imshow(img_display)\n",
    "        ax.set_title(f'Original: {class_name}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # For each augmentation\n",
    "        for j in range(num_augmentations):\n",
    "            # Apply the transform again to get a different augmentation\n",
    "            # This assumes we can access the original image and transform\n",
    "            if hasattr(data_loader.dataset, 'transform'):\n",
    "                transform = data_loader.dataset.transform\n",
    "                \n",
    "                # Try to get the original PIL image\n",
    "                try:\n",
    "                    if hasattr(data_loader.dataset, 'subset') and hasattr(data_loader.dataset.subset, 'dataset'):\n",
    "                        # This is a complex structure like TransformedDataset -> Subset -> ImageFolder\n",
    "                        subset = data_loader.dataset.subset\n",
    "                        index = subset.indices[i]  # Get the real index in the original dataset\n",
    "                        original_pil, _ = subset.dataset[index]\n",
    "                        \n",
    "                        # Apply transform\n",
    "                        augmented = transform(original_pil)\n",
    "                        \n",
    "                        # Display the augmented image\n",
    "                        ax = plt.subplot(num_images, num_augmentations + 1, i * (num_augmentations + 1) + j + 2)\n",
    "                        aug_display = augmented.cpu().numpy().transpose((1, 2, 0))\n",
    "                        \n",
    "                        # Unnormalize\n",
    "                        aug_display = std * aug_display + mean\n",
    "                        aug_display = np.clip(aug_display, 0, 1)\n",
    "                        \n",
    "                        plt.imshow(aug_display)\n",
    "                        ax.set_title(f'Aug {j+1}: {class_name}')\n",
    "                        ax.axis('off')\n",
    "                    else:\n",
    "                        print(\"Cannot access original images for augmentation display.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error displaying augmentation: {e}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Cannot access the transform for augmentation display.\")\n",
    "                break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_plots/augmented_images.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (depends on the data pipeline being initialized):\n",
    "# display_augmented_images(train_loader, num_images=4, num_augmentations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Model Training Configuration ---\n",
    "\"\"\"\n",
    "## Model Training Configuration\n",
    "\n",
    "Configure and train different model architectures. This section demonstrates:\n",
    "- Setting up different model architectures\n",
    "- Configuring optimizers and learning rate schedulers\n",
    "- Training models with the configured pipeline\n",
    "- Saving model checkpoints\n",
    "\"\"\"\n",
    "\n",
    "def train_and_evaluate_model(model_name, train_loader, val_loader, test_loader, \n",
    "                           num_classes, num_epochs=25, learning_rate=0.001,\n",
    "                           optimizer_name='adam', scheduler_name='cosine',\n",
    "                           save_dir='model_checkpoints', pretrained=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model with the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model architecture to use\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        test_loader: DataLoader for test data\n",
    "        num_classes: Number of output classes\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        optimizer_name: Name of optimizer to use\n",
    "        scheduler_name: Name of learning rate scheduler to use\n",
    "        save_dir: Directory to save model checkpoints\n",
    "        pretrained: Whether to use pretrained weights (for transfer learning models)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained_model, history)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name} model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get model info\n",
    "    model_info = get_model_info(model_name)\n",
    "    print(f\"Model: {model_info['name']}\")\n",
    "    print(f\"Description: {model_info['description']}\")\n",
    "    \n",
    "    # Create model-specific directory\n",
    "    model_dir = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        model = get_model(model_name, num_classes, device, pretrained)\n",
    "        print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer_config = {\n",
    "            'name': optimizer_name,\n",
    "            'lr': learning_rate,\n",
    "            'weight_decay': 1e-4 if model_name.startswith('vgg') else 0\n",
    "        }\n",
    "        \n",
    "        scheduler_config = None\n",
    "        if scheduler_name:\n",
    "            if scheduler_name == 'cosine':\n",
    "                scheduler_config = {\n",
    "                    'name': 'cosine',\n",
    "                    'T_max': num_epochs,\n",
    "                    'eta_min': 1e-6\n",
    "                }\n",
    "            elif scheduler_name == 'plateau':\n",
    "                scheduler_config = {\n",
    "                    'name': 'plateau',\n",
    "                    'mode': 'min',\n",
    "                    'factor': 0.1,\n",
    "                    'patience': 3,\n",
    "                    'verbose': True\n",
    "                }\n",
    "            elif scheduler_name == 'onecycle':\n",
    "                scheduler_config = {\n",
    "                    'name': 'onecycle',\n",
    "                    'max_lr': learning_rate * 10,\n",
    "                    'epochs': num_epochs,\n",
    "                    'steps_per_epoch': len(train_loader)\n",
    "                }\n",
    "        \n",
    "        # Setup training components\n",
    "        criterion, optimizer, scheduler = setup_training_components(\n",
    "            model, optimizer_config, scheduler_config, device\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        trained_model, history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device,\n",
    "            save_dir=model_dir,\n",
    "            model_name=model_name,\n",
    "            verbose=True,\n",
    "            save_every_n_epochs=5\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time/60:.2f} minutes\")\n",
    "        \n",
    "        # Plot training history\n",
    "        print(\"Plotting training history...\")\n",
    "        plot_training_history(\n",
    "            history, \n",
    "            save_path=os.path.join(model_dir, f\"{model_name}_training_history.png\")\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        test_loss, test_acc = test_model(\n",
    "            model=trained_model,\n",
    "            test_loader=test_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        # Save final results\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'num_epochs': num_epochs,\n",
    "            'training_time': training_time,\n",
    "            'final_train_loss': history['train_loss'][-1],\n",
    "            'final_train_acc': history['train_acc'][-1],\n",
    "            'final_val_loss': history['val_loss'][-1] if history['val_loss'] else None,\n",
    "            'final_val_acc': history['val_acc'][-1] if history['val_acc'] else None,\n",
    "            'test_loss': test_loss,\n",
    "            'test_acc': test_acc\n",
    "        }\n",
    "        \n",
    "        # Save results to file\n",
    "        import json\n",
    "        with open(os.path.join(model_dir, f\"{model_name}_results.json\"), 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        return trained_model, history\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Example usage (depends on the data pipeline being initialized):\n",
    "# model, history = train_and_evaluate_model(\n",
    "#     model_name='improved_cnn',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     num_classes=len(class_names),\n",
    "#     num_epochs=20,\n",
    "#     learning_rate=0.001,\n",
    "#     optimizer_name='adam',\n",
    "#     scheduler_name='cosine'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Train Multiple Models ---\n",
    "\"\"\"\n",
    "## Train Multiple Models\n",
    "\n",
    "This section demonstrates how to train multiple model architectures with different\n",
    "configurations to compare their performance.\n",
    "\"\"\"\n",
    "\n",
    "def train_multiple_models(train_loader, val_loader, test_loader, class_names, \n",
    "                         configs=None, save_dir='model_checkpoints'):\n",
    "    \"\"\"\n",
    "    Train multiple models with different configurations.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        test_loader: DataLoader for test data\n",
    "        class_names: List of class names\n",
    "        configs: List of model configurations\n",
    "        save_dir: Directory to save model checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for all models\n",
    "    \"\"\"\n",
    "    if configs is None:\n",
    "        configs = [\n",
    "            {\n",
    "                'model_name': 'basic_cnn',\n",
    "                'num_epochs': 15,\n",
    "                'learning_rate': 0.001,\n",
    "                'optimizer_name': 'adam',\n",
    "                'scheduler_name': 'cosine',\n",
    "                'pretrained': False\n",
    "            },\n",
    "            {\n",
    "                'model_name': 'improved_cnn',\n",
    "                'num_epochs': 20,\n",
    "                'learning_rate': 0.001,\n",
    "                'optimizer_name': 'adam',\n",
    "                'scheduler_name': 'cosine',\n",
    "                'pretrained': False\n",
    "            },\n",
    "            {\n",
    "                'model_name': 'vgg19',\n",
    "                'num_epochs': 10,\n",
    "                'learning_rate': 0.0001,\n",
    "                'optimizer_name': 'adam',\n",
    "                'scheduler_name': 'plateau',\n",
    "                'pretrained': True\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    num_classes = len(class_names)\n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        model_name = config['model_name']\n",
    "        print(f\"\\n\\nTraining model: {model_name}\")\n",
    "        \n",
    "        model, history = train_and_evaluate_model(\n",
    "            model_name=model_name,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_classes=num_classes,\n",
    "            num_epochs=config.get('num_epochs', 20),\n",
    "            learning_rate=config.get('learning_rate', 0.001),\n",
    "            optimizer_name=config.get('optimizer_name', 'adam'),\n",
    "            scheduler_name=config.get('scheduler_name', 'cosine'),\n",
    "            save_dir=save_dir,\n",
    "            pretrained=config.get('pretrained', False)\n",
    "        )\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"Model Comparison Summary\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result['history'] is not None:\n",
    "            config = result['config']\n",
    "            history = result['history']\n",
    "            \n",
    "            best_val_acc = max(history['val_acc']) if history['val_acc'] else 0\n",
    "            final_train_acc = history['train_acc'][-1] if history['train_acc'] else 0\n",
    "            \n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"  Config: {config}\")\n",
    "            print(f\"  Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "            print(f\"  Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "            print(\"-\"*50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (depends on the data pipeline being initialized):\n",
    "# results = train_multiple_models(train_loader, val_loader, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3daa574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Fine-tuning and Advanced Training ---\n",
    "\"\"\"\n",
    "## Fine-tuning and Advanced Training\n",
    "\n",
    "This section demonstrates more advanced training techniques:\n",
    "- Fine-tuning pretrained models\n",
    "- Freezing and unfreezing layers\n",
    "- Learning rate scheduling strategies\n",
    "\"\"\"\n",
    "\n",
    "def freeze_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freeze the first num_layers_to_freeze layers of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        num_layers_to_freeze: Number of layers to freeze\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'features') and isinstance(model.features, nn.Sequential):\n",
    "        # Count actual layers (Conv2d, Linear, etc.) if features is a sequential block\n",
    "        layer_idx = 0\n",
    "        for child in model.features.children():\n",
    "            if isinstance(child, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)):\n",
    "                if layer_idx < num_layers_to_freeze:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = False\n",
    "                layer_idx += 1\n",
    "        print(f\"Froze {min(num_layers_to_freeze, layer_idx)} layers in model.features.\")\n",
    "    else:\n",
    "        # Fallback for generic parameters\n",
    "        params = list(model.parameters())\n",
    "        actual_layers_to_freeze = min(num_layers_to_freeze, len(params))\n",
    "        for i, param in enumerate(params):\n",
    "            if i < actual_layers_to_freeze:\n",
    "                param.requires_grad = False\n",
    "        print(f\"Froze first {actual_layers_to_freeze} parameter groups of the model.\")\n",
    "\n",
    "def fine_tune_model(model_name, train_loader, val_loader, test_loader, class_names,\n",
    "                   initial_epochs=5, fine_tune_epochs=15, save_dir='model_checkpoints',\n",
    "                   freeze_layers_count=None, unfreeze_after_epoch=None):\n",
    "    \"\"\"\n",
    "    Fine-tune a pretrained model with a two-phase training approach.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model architecture to use\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        test_loader: DataLoader for test data\n",
    "        class_names: List of class names\n",
    "        initial_epochs: Number of epochs for initial training phase\n",
    "        fine_tune_epochs: Number of epochs for fine-tuning phase\n",
    "        save_dir: Directory to save model checkpoints\n",
    "        freeze_layers_count: Number of layers to freeze initially\n",
    "        unfreeze_after_epoch: Epoch after which to unfreeze all layers\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (fine_tuned_model, history)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fine-tuning {model_name} model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model_dir = os.path.join(save_dir, f\"{model_name}_finetuned\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    try:\n",
    "        # Initialize model with pretrained weights\n",
    "        model = get_model(model_name, num_classes, device, pretrained=True)\n",
    "        print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Phase 1: Train with frozen layers (if specified)\n",
    "        if freeze_layers_count is not None and freeze_layers_count > 0:\n",
    "            print(f\"\\nPhase 1: Training with {freeze_layers_count} frozen layers for {initial_epochs} epochs\")\n",
    "            freeze_layers(model, freeze_layers_count)\n",
    "            \n",
    "            # Setup optimizer and scheduler for phase 1\n",
    "            optimizer_config = {\n",
    "                'name': 'adam',\n",
    "                'lr': 0.0001,  # Lower learning rate for pretrained features\n",
    "                'weight_decay': 1e-4\n",
    "            }\n",
    "            \n",
    "            scheduler_config = {\n",
    "                'name': 'cosine',\n",
    "                'T_max': initial_epochs,\n",
    "                'eta_min': 1e-6\n",
    "            }\n",
    "            \n",
    "            criterion, optimizer, scheduler = setup_training_components(\n",
    "                model, optimizer_config, scheduler_config, device\n",
    "            )\n",
    "            \n",
    "            # Train with frozen layers\n",
    "            model, phase1_history = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_epochs=initial_epochs,\n",
    "                device=device,\n",
    "                save_dir=model_dir,\n",
    "                model_name=f\"{model_name}_phase1\",\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Phase 1 completed. Best validation accuracy: {max(phase1_history['val_acc']):.4f}\")\n",
    "        \n",
    "        # Phase 2: Fine-tune all layers\n",
    "        print(f\"\\nPhase 2: Fine-tuning all layers for {fine_tune_epochs} epochs\")\n",
    "        \n",
    "        # Unfreeze all layers\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Setup optimizer and scheduler for phase 2\n",
    "        optimizer_config = {\n",
    "            'name': 'adam',\n",
    "            'lr': 0.00005,  # Even lower learning rate for fine-tuning\n",
    "            'weight_decay': 1e-4\n",
    "        }\n",
    "        \n",
    "        scheduler_config = {\n",
    "            'name': 'cosine',\n",
    "            'T_max': fine_tune_epochs,\n",
    "            'eta_min': 1e-7\n",
    "        }\n",
    "        \n",
    "        criterion, optimizer, scheduler = setup_training_components(\n",
    "            model, optimizer_config, scheduler_config, device\n",
    "        )\n",
    "        \n",
    "        # Fine-tune the model\n",
    "        fine_tuned_model, phase2_history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=fine_tune_epochs,\n",
    "            device=device,\n",
    "            save_dir=model_dir,\n",
    "            model_name=f\"{model_name}_finetuned\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Combine histories from both phases\n",
    "        if freeze_layers_count is not None and freeze_layers_count > 0:\n",
    "            combined_history = {\n",
    "                'train_loss': phase1_history['train_loss'] + phase2_history['train_loss'],\n",
    "                'train_acc': phase1_history['train_acc'] + phase2_history['train_acc'],\n",
    "                'val_loss': phase1_history['val_loss'] + phase2_history['val_loss'],\n",
    "                'val_acc': phase1_history['val_acc'] + phase2_history['val_acc'],\n",
    "                'learning_rates': phase1_history['learning_rates'] + phase2_history['learning_rates']\n",
    "            }\n",
    "        else:\n",
    "            combined_history = phase2_history\n",
    "        \n",
    "        # Plot combined training history\n",
    "        plot_training_history(\n",
    "            combined_history, \n",
    "            save_path=os.path.join(model_dir, f\"{model_name}_finetuned_history.png\")\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"\\nEvaluating fine-tuned model on test set...\")\n",
    "        test_loss, test_acc = test_model(\n",
    "            model=fine_tuned_model,\n",
    "            test_loader=test_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        # Save final results\n",
    "        results = {\n",
    "            'model_name': f\"{model_name}_finetuned\",\n",
    "            'initial_epochs': initial_epochs if freeze_layers_count is not None else 0,\n",
    "            'fine_tune_epochs': fine_tune_epochs,\n",
    "            'frozen_layers': freeze_layers_count,\n",
    "            'test_loss': test_loss,\n",
    "            'test_acc': test_acc,\n",
    "            'best_val_acc': max(combined_history['val_acc']) if combined_history['val_acc'] else 0\n",
    "        }\n",
    "        \n",
    "        # Save results to file\n",
    "        import json\n",
    "        with open(os.path.join(model_dir, f\"{model_name}_finetuned_results.json\"), 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        return fine_tuned_model, combined_history\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model fine-tuning: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Example usage (depends on the data pipeline being initialized):\n",
    "# fine_tuned_model, history = fine_tune_model(\n",
    "#     model_name='vgg19',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     class_names=class_names,\n",
    "#     initial_epochs=5,\n",
    "#     fine_tune_epochs=15,\n",
    "#     freeze_layers_count=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Complete Training Pipeline Example ---\n",
    "\"\"\"\n",
    "## Complete Training Pipeline Example\n",
    "\n",
    "This section demonstrates a complete training pipeline from data loading to model evaluation.\n",
    "Follow this example to train models on your own dataset.\n",
    "\"\"\"\n",
    "\n",
    "def run_complete_training_pipeline(data_root, image_size=(64, 64), batch_size=32,\n",
    "                                  model_name='improved_cnn', num_epochs=20,\n",
    "                                  learning_rate=0.001, optimizer_name='adam',\n",
    "                                  scheduler_name='cosine', pretrained=False,\n",
    "                                  save_dir='model_checkpoints'):\n",
    "    \"\"\"\n",
    "    Run a complete training pipeline from data loading to model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        data_root: Path to the dataset root directory\n",
    "        image_size: Input image size\n",
    "        batch_size: Batch size for training\n",
    "        model_name: Model architecture to use\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        optimizer_name: Optimizer to use\n",
    "        scheduler_name: Learning rate scheduler to use\n",
    "        pretrained: Whether to use pretrained weights\n",
    "        save_dir: Directory to save model checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_model, history, class_names)\n",
    "    \"\"\"\n",
    "    print(f\"Running complete training pipeline for {model_name} model\")\n",
    "    \n",
    "    # Step 1: Set up data pipeline\n",
    "    print(\"\\nStep 1: Setting up data pipeline...\")\n",
    "    pipeline, train_loader, val_loader, test_loader, class_names = setup_data_pipeline(\n",
    "        data_root=data_root,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        do_transform=True\n",
    "    )\n",
    "    \n",
    "    if pipeline is None:\n",
    "        print(\"Failed to set up data pipeline. Aborting.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Step 2: Visualize augmented images\n",
    "    print(\"\\nStep 2: Visualizing augmented images...\")\n",
    "    display_augmented_images(train_loader, num_images=3, num_augmentations=3)\n",
    "    \n",
    "    # Step 3: Train the model\n",
    "    print(\"\\nStep 3: Training the model...\")\n",
    "    trained_model, history = train_and_evaluate_model(\n",
    "        model_name=model_name,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_classes=len(class_names),\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer_name=optimizer_name,\n",
    "        scheduler_name=scheduler_name,\n",
    "        save_dir=save_dir,\n",
    "        pretrained=pretrained\n",
    "    )\n",
    "    \n",
    "    if trained_model is None:\n",
    "        print(\"Failed to train the model. Aborting.\")\n",
    "        return None, None, class_names\n",
    "    \n",
    "    # Step 4: Save model architecture info\n",
    "    print(\"\\nStep 4: Saving model architecture info...\")\n",
    "    model_dir = os.path.join(save_dir, model_name)\n",
    "    \n",
    "    # Save class names\n",
    "    with open(os.path.join(model_dir, 'class_names.txt'), 'w') as f:\n",
    "        for name in class_names:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    \n",
    "    # Save model architecture info\n",
    "    model_info = get_model_info(model_name)\n",
    "    with open(os.path.join(model_dir, 'model_info.txt'), 'w') as f:\n",
    "        f.write(f\"Model: {model_info['name']}\\n\")\n",
    "        f.write(f\"Description: {model_info['description']}\\n\")\n",
    "        f.write(f\"Parameters: {model_info['parameters']}\\n\")\n",
    "        f.write(f\"Training Time: {model_info['training_time']}\\n\")\n",
    "        f.write(f\"Expected Accuracy: {model_info['accuracy']}\\n\")\n",
    "    \n",
    "    print(f\"Training pipeline completed successfully. Model saved to {model_dir}\")\n",
    "    return trained_model, history, class_names\n",
    "\n",
    "# Usage example (uncomment and modify with your actual data path):\n",
    "\"\"\"\n",
    "DATA_ROOT = \"./your_dataset_path\"\n",
    "trained_model, history, class_names = run_complete_training_pipeline(\n",
    "    data_root=DATA_ROOT,\n",
    "    model_name='improved_cnn',\n",
    "    num_epochs=20\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Run Training (User Code) ---\n",
    "\"\"\"\n",
    "## Run Training\n",
    "\n",
    "This is where you run the actual training pipeline with your dataset.\n",
    "Uncomment and modify the code below to train models on your own dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Define your dataset path\n",
    "# DATA_ROOT = \"./datasets/handwritten-english/augmented_images1\"\n",
    "\n",
    "# Option 1: Run the complete pipeline for a single model\n",
    "\"\"\"\n",
    "trained_model, history, class_names = run_complete_training_pipeline(\n",
    "    data_root=DATA_ROOT,\n",
    "    model_name='improved_cnn',\n",
    "    num_epochs=20,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    optimizer_name='adam',\n",
    "    scheduler_name='cosine'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Option 2: Train multiple models\n",
    "\"\"\"\n",
    "# First set up the data pipeline\n",
    "pipeline, train_loader, val_loader, test_loader, class_names = setup_data_pipeline(\n",
    "    data_root=DATA_ROOT,\n",
    "    batch_size=32,\n",
    "    do_transform=True\n",
    ")\n",
    "\n",
    "# Define configurations for multiple models\n",
    "configs = [\n",
    "    {\n",
    "        'model_name': 'basic_cnn',\n",
    "        'num_epochs': 15,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer_name': 'adam',\n",
    "        'scheduler_name': 'cosine',\n",
    "        'pretrained': False\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'improved_cnn',\n",
    "        'num_epochs': 20,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer_name': 'adam',\n",
    "        'scheduler_name': 'cosine',\n",
    "        'pretrained': False\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'vgg19',\n",
    "        'num_epochs': 10,\n",
    "        'learning_rate': 0.0001,\n",
    "        'optimizer_name': 'adam',\n",
    "        'scheduler_name': 'plateau',\n",
    "        'pretrained': True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train all models\n",
    "results = train_multiple_models(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    class_names=class_names,\n",
    "    configs=configs\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: Fine-tune a pretrained model\n",
    "\"\"\"\n",
    "# First set up the data pipeline\n",
    "pipeline, train_loader, val_loader, test_loader, class_names = setup_data_pipeline(\n",
    "    data_root=DATA_ROOT,\n",
    "    batch_size=32,\n",
    "    do_transform=True\n",
    ")\n",
    "\n",
    "# Fine-tune a VGG19 model\n",
    "fine_tuned_model, history = fine_tune_model(\n",
    "    model_name='vgg19',\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    class_names=class_names,\n",
    "    initial_epochs=5,\n",
    "    fine_tune_epochs=15,\n",
    "    freeze_layers_count=20\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"This notebook is ready for training handwritten character recognition models.\")\n",
    "print(\"Uncomment one of the training options above and run this cell to start training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
