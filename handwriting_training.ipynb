{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ex6pQh3WcjGN",
        "outputId": "783427f3-8d44-4dcd-dae2-6bc3ac1e9015"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdKb9M3lQKT4",
        "outputId": "0166b667-828d-40c3-91b5-3ccda5a625a3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.models as models\n",
        "import matplotlib.patches as patches\n",
        "import gradio as gr\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k2WoPJiYwtx"
      },
      "outputs": [],
      "source": [
        "### ONLY FOR MAC!!!\n",
        "#import torch.multiprocessing as mp\n",
        "#if __name__ == '__main__':\n",
        "   # mp.set_start_method('spawn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QXV3adCkqGt"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# # Upload the kaggle.json file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Move kaggle.json to the correct directory\n",
        "# os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "\n",
        "# os.chmod('/root/.kaggle/kaggle.json', 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S976ss4vqZpG"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'username'\n",
        "os.environ['KAGGLE_KEY'] = 'key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rhIos0OBYBaz",
        "outputId": "da9767ed-ae6e-44dd-8908-11ccc21d38bd"
      },
      "outputs": [],
      "source": [
        "# RUN ON NEW ENVIRONMENT ONLY!!!\n",
        "!kaggle datasets download sujaymann/handwritten-english-characters-and-digits\n",
        "!unzip handwritten-english-characters-and-digits.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qobSUm-CYy_S"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RandomChoice(torch.nn.Module):\n",
        "    \"\"\"Randomly applies one of the given transforms with given probability\"\"\"\n",
        "    def __init__(self, transforms, p=0.5):\n",
        "        super().__init__()\n",
        "        self.transforms = transforms\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        if random.random() < self.p:\n",
        "            transform = random.choice(self.transforms)\n",
        "            return transform(img)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dwavLZcZ1SW"
      },
      "outputs": [],
      "source": [
        "class ThicknessTransform:\n",
        "    def __init__(self, kernel_range=(1, 3), p=0.5):\n",
        "        self.kernel_range = kernel_range\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if torch.rand(1) < self.p:\n",
        "            # Convert PIL image to numpy array\n",
        "            img_np = np.array(img)\n",
        "\n",
        "            # Randomly choose operation (erosion or dilation)\n",
        "            operation = np.random.choice(['erode', 'dilate'])\n",
        "\n",
        "            # Random kernel size\n",
        "            kernel_size = np.random.randint(self.kernel_range[0], self.kernel_range[1] + 1)\n",
        "            kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "\n",
        "            # Apply morphological operation\n",
        "            if operation == 'erode':\n",
        "                processed = cv2.erode(img_np, kernel, iterations=1)\n",
        "            else:\n",
        "                processed = cv2.dilate(img_np, kernel, iterations=1)\n",
        "\n",
        "            # Convert back to PIL\n",
        "            return Image.fromarray(processed)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B8qvoQQZ3Ri"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HandwritingDataPipeline:\n",
        "    def __init__(self, data_root, image_size=64, batch_size=32,\n",
        "                 train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
        "\t\t         seed=42, device='mps', do_transform=False):\n",
        "        self.device = device\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "\n",
        "        # Set random seeds\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        if do_transform:\n",
        "            # Define transforms with enhanced augmentations\n",
        "            self.transform_train = transforms.Compose([\n",
        "                transforms.Grayscale(num_output_channels=1),\n",
        "                transforms.Resize((64, 64)),\n",
        "\n",
        "                # Thickness variations through morphological operations\n",
        "                ThicknessTransform(kernel_range=(1, 3), p=0.3),\n",
        "\n",
        "                # Blur variations to simulate different pen pressures\n",
        "                transforms.RandomApply([\n",
        "                    transforms.GaussianBlur(3, sigma=(0.1, 0.2))\n",
        "                ], p=0.3),\n",
        "\n",
        "                # Conservative rotations\n",
        "                transforms.RandomRotation(15, fill=255),\n",
        "\n",
        "                # Gentle affine transformations\n",
        "                transforms.RandomAffine(\n",
        "                    degrees=0,\n",
        "                    translate=(0.1, 0.1),\n",
        "                    scale=(0.8, 1.2),\n",
        "                    shear=10,\n",
        "                    fill=255\n",
        "                ),\n",
        "\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)),\n",
        "                transforms.RandomErasing(p=0.2, scale=(0.02, 0.03), value=1.0)\n",
        "            ])\n",
        "        else:\n",
        "            self.transform_train = transforms.Compose([\n",
        "                transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed\n",
        "                transforms.Resize((image_size, image_size)),  # Resize to standard size\n",
        "                transforms.ToTensor(),  # Convert to tensor\n",
        "                transforms.Normalize((0.5,), (0.5,))  # Normalize between -1 and 1\n",
        "            ])\n",
        "\n",
        "        self.transform_eval = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        # Load and split dataset\n",
        "        self.setup_datasets(data_root, train_ratio, val_ratio, test_ratio)\n",
        "\n",
        "    def setup_datasets(self, data_root, train_ratio, val_ratio, test_ratio):\n",
        "        \"\"\"Set up and split datasets with appropriate transforms\"\"\"\n",
        "        # Same as before...\n",
        "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9, \"Ratios must sum to 1\"\n",
        "\n",
        "        full_dataset = datasets.ImageFolder(root=data_root, transform=self.transform_eval)\n",
        "\n",
        "        total_size = len(full_dataset)\n",
        "        train_size = int(train_ratio * total_size)\n",
        "        val_size = int(val_ratio * total_size)\n",
        "        test_size = total_size - train_size - val_size\n",
        "\n",
        "        lengths = [train_size, val_size, test_size]\n",
        "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "            full_dataset, lengths, generator=torch.Generator().manual_seed(self.seed)\n",
        "        )\n",
        "\n",
        "        train_dataset.dataset.transform = self.transform_train\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True if self.device != 'cpu' else False\n",
        "        )\n",
        "\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        self.sizes = {\n",
        "            'train': train_size,\n",
        "            'val': val_size,\n",
        "            'test': test_size,\n",
        "            'total': total_size\n",
        "        }\n",
        "\n",
        "    def get_loaders(self):\n",
        "        \"\"\"Return all three DataLoaders\"\"\"\n",
        "        return self.train_loader, self.val_loader, self.test_loader\n",
        "\n",
        "    def get_sizes(self):\n",
        "        \"\"\"Return the sizes of all splits\"\"\"\n",
        "        return self.sizes\n",
        "\n",
        "# Usage:\n",
        "# pipeline = HandwritingDataPipeline(\n",
        "#     data_root=\"./content/augmented_images/augmented_images1\",\n",
        "#     image_size=64,\n",
        "#     batch_size=32,\n",
        "#     train_ratio=0.7,\n",
        "#     val_ratio=0.15,\n",
        "#     test_ratio=0.15\n",
        "# )\n",
        "\n",
        "# train_loader, val_loader, test_loader = pipeline.get_loaders()\n",
        "# sizes = pipeline.get_sizes()\n",
        "\n",
        "# print(f\"Dataset splits: {sizes}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc34p4JEaBzg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def display_augmented_images(train_loader, num_images=5, num_augmentations=3):\n",
        "    # Get class names from the dataset\n",
        "    class_to_idx = train_loader.dataset.dataset.class_to_idx\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "    # Get random indices from the dataset\n",
        "    dataset_size = len(train_loader.dataset)\n",
        "    random_indices = torch.randperm(dataset_size)[:num_images]\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 3*num_images))\n",
        "\n",
        "    for i, dataset_idx in enumerate(random_indices):\n",
        "        # Get the original image and its true label\n",
        "        original_path = train_loader.dataset.dataset.imgs[train_loader.dataset.indices[dataset_idx]][0]\n",
        "        true_label = train_loader.dataset.dataset.imgs[train_loader.dataset.indices[dataset_idx]][1]\n",
        "        class_name = idx_to_class[true_label]\n",
        "\n",
        "        # Load and process original image\n",
        "        original_pil = Image.open(original_path).convert('L')\n",
        "        original_tensor = pipeline.transform_eval(original_pil)\n",
        "\n",
        "        # Show true original image\n",
        "        ax = plt.subplot(num_images, num_augmentations + 1, i*(num_augmentations + 1) + 1)\n",
        "        img_display = original_tensor.cpu().numpy()[0]\n",
        "        img_display = (img_display * 0.5) + 0.5  # Denormalize\n",
        "        ax.imshow(img_display, cmap='gray')\n",
        "        ax.set_title(f'Original (Class: {class_name})')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Show augmented versions\n",
        "        for j in range(num_augmentations):\n",
        "            aug_img = pipeline.transform_train(original_pil)\n",
        "\n",
        "            ax = plt.subplot(num_images, num_augmentations + 1, i*(num_augmentations + 1) + j + 2)\n",
        "            aug_display = aug_img.cpu().numpy()[0]\n",
        "            aug_display = (aug_display * 0.5) + 0.5  # Denormalize\n",
        "            ax.imshow(aug_display, cmap='gray')\n",
        "            ax.set_title(f'Augmented {j+1} ({class_name})')\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Usage:\n",
        "# display_augmented_images(train_loader, num_images=16, num_augmentations=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iD6NFxXaGAc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LetterCNN64(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(LetterCNN64, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Compute correct flattened size dynamically\n",
        "        self._to_linear = self._get_conv_output()\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _get_conv_output(self):\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 1, 64, 64)  # Updated input size to (1, 64, 64)\n",
        "            x = self.pool(F.relu(self.conv1(dummy_input)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = self.pool(F.relu(self.conv3(x)))\n",
        "            return x.numel()  # Correct flattened size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ImprovedLetterCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ImprovedLetterCNN, self).__init__()\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25)\n",
        "        )\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25)\n",
        "        )\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25)\n",
        "        )\n",
        "\n",
        "        # Compute the flattened size dynamically\n",
        "        self._to_linear = self._get_conv_output()\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def _get_conv_output(self):\n",
        "        # Helper function to calculate the flattened size\n",
        "        x = torch.zeros(1, 1, 64, 64)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        return x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VGG19HandwritingModel(nn.Module):\n",
        "    def __init__(self, num_classes, device, pretrained=True):\n",
        "        super(VGG19HandwritingModel, self).__init__()\n",
        "\n",
        "        # Load pretrained VGG19 and move to device\n",
        "        vgg19 = models.vgg19(weights=('DEFAULT' if pretrained else None))\n",
        "        vgg19 = vgg19.to(device)\n",
        "\n",
        "        # Modify first layer to accept grayscale images\n",
        "        self.features = vgg19.features\n",
        "        self.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1).to(device)\n",
        "\n",
        "        # Custom classifier for our task\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 2 * 2, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(512, num_classes)\n",
        "        ).to(device)\n",
        "\n",
        "        # Initialize weights for the new layers\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2k4T_KuaL4-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, save_dir='checkpoints'):\n",
        "    train_losses, val_losses = [], []\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_acc = 100 * correct / total\n",
        "\n",
        "        # Step the scheduler with validation loss\n",
        "        if scheduler is not None:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(val_loss)  # Pass validation loss as metric\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        # Print current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "            }, os.path.join(save_dir, 'best_model.pth'))\n",
        "            print(f\"Saved best model with validation accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'epoch': num_epochs-1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_losses[-1],\n",
        "        'val_loss': val_losses[-1],\n",
        "        'val_acc': val_acc,\n",
        "    }, os.path.join(save_dir, 'final_model.pth'))\n",
        "    print(f\"Saved final model at epoch {num_epochs}\")\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, val_losses, best_val_acc\n",
        "\n",
        "# Function to load a saved model\n",
        "def load_model(model, optimizer, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
        "    # checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    val_acc = checkpoint['val_acc']\n",
        "    print(f\"Loaded model from epoch {epoch+1} with validation accuracy: {val_acc:.2f}%\")\n",
        "    return model, optimizer, epoch, val_acc\n",
        "\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "def freeze_layers(model, num_layers_to_freeze):\n",
        "    for i, param in enumerate(model.features.parameters()):\n",
        "        if i < num_layers_to_freeze:\n",
        "            param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsCW5TpIJxyU",
        "outputId": "e6c348b9-96af-4d94-8e34-6ce6f922b260"
      },
      "outputs": [],
      "source": [
        "# model = VGG19HandwritingModel(62, \"cpu\")\n",
        "# model = load_model(model, optim.Adam(model.parameters(),  lr=1e-3,   weight_decay=1e-4) , \"/content/drive/MyDrive/model_checkpoints_vgg/best_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFadXBgBeVb-",
        "outputId": "4b7b7c83-93f5-4469-9d89-c83e35725f0d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "F6QLg_zeaPBX",
        "outputId": "ea5fccee-31c2-4936-e57f-0e8245238948"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "save_dir = 'model_checkpoints'\n",
        "num_epochs=20\n",
        "pretrained=False\n",
        "\n",
        "# Create the data pipeline\n",
        "pipeline = HandwritingDataPipeline(\n",
        "    data_root=\"/content/augmented_images/augmented_images1\",\n",
        "    image_size=64,\n",
        "    batch_size=32,\n",
        "    train_ratio=0.7,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15,\n",
        "    device=device,\n",
        "    do_transform=True\n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = pipeline.get_loaders()\n",
        "sizes = pipeline.get_sizes()\n",
        "print(f\"Dataset splits: {sizes}\")\n",
        "\n",
        "# display_augmented_images(train_loader, num_images=16, num_augmentations=8)\n",
        "\n",
        "# Get number of classes and initialize model\n",
        "num_classes = len(train_loader.dataset.dataset.classes)\n",
        "model = VGG19HandwritingModel(num_classes=num_classes, device=device, pretrained=pretrained)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "if pretrained:\n",
        "    # Training configuration\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.features.parameters(), 'lr': 1e-5},  # Lower learning rate for VGG features\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-4}  # Higher learning rate for new layers\n",
        "    ])\n",
        "\n",
        "    # Cosine annealing scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "else:\n",
        "    num_epochs=50\n",
        "    # Optimizer setup - single learning rate since we're training from scratch\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=1e-3,  # Higher initial learning rate since we're training from scratch\n",
        "        weight_decay=1e-4  # L2 regularization to prevent overfitting\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=1e-3,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.2,  # Percentage of training to reach max_lr\n",
        "        div_factor=10,  # Initial learning rate will be max_lr/div_factor\n",
        "        final_div_factor=1e4  # Final learning rate will be max_lr/final_div_factor\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1d8LOQEaour"
      },
      "outputs": [],
      "source": [
        "####### T R A I N #######\n",
        "### DO NOT RUN ##########\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing with different schedulers:\n",
        "# 1. Cosine Annealing\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "#     optimizer,\n",
        "#     T_max=num_epochs,\n",
        "#     eta_min=1e-6\n",
        "# )\n",
        "\n",
        "# 2. Step LR\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "#     optimizer,\n",
        "#     step_size=5,\n",
        "#     gamma=0.5\n",
        "# )\n",
        "\n",
        "# # 3. One Cycle LR\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "#     optimizer,\n",
        "#     max_lr=0.01,\n",
        "#     epochs=num_epochs,\n",
        "#     steps_per_epoch=len(train_loader)\n",
        "# )\n",
        "\n",
        "# 4. Reduce LR on Plateau:\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.000005)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#     optimizer,\n",
        "#     mode='min',      # Monitor loss (min) or metrics (max)\n",
        "#     factor=0.1,      # Reduction factor\n",
        "#     patience=3,      # Epochs to wait before reducing\n",
        "#     verbose=True,\n",
        "#     min_lr=1e-6\n",
        "# )\n",
        "\n",
        "# model, optimizer, epoch, val_acc = load_model(\n",
        "#     model,\n",
        "#     optimizer,\n",
        "#     os.path.join(save_dir, 'best_model.pth')\n",
        "# )\n",
        "\n",
        "# Train the model\n",
        "train_model(model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scheduler=scheduler,\n",
        "            num_epochs=num_epochs,\n",
        "            save_dir=save_dir\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "fxOmMXH3ckOA",
        "outputId": "dbe362b1-cba8-4434-d81b-2063c488b113"
      },
      "outputs": [],
      "source": [
        "# To load the best model:\n",
        "model, optimizer, epoch, val_acc = load_model(\n",
        "    model,\n",
        "    optimizer,\n",
        "    os.path.join('/content/drive/MyDrive/model_checkpoints_vgg', 'best_model.pth')\n",
        ")\n",
        "\n",
        "# Set the model to eval mode to use pre-calcl batch normalization layers\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7vSL-y0cPVr"
      },
      "outputs": [],
      "source": [
        "### OPTIONAL: RUN THE MODEL'S TEST - LONG RUNNING!!!\n",
        "test_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJy9Nungj5G3"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image_path):\n",
        "    \"\"\"Prepare image for classification.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image)\n",
        "    return image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def get_class_labels(data_root):\n",
        "    \"\"\"Get class labels from the data directory.\"\"\"\n",
        "    class_names = sorted(os.listdir(data_root))\n",
        "    # Remove any hidden files (like .DS_Store)\n",
        "    class_names = [c for c in class_names if not c.startswith('.')]\n",
        "    return class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KebxSaMbjpBs",
        "outputId": "d103ae3f-62fc-49bf-ae9a-4f7e557e9618"
      },
      "outputs": [],
      "source": [
        "# Get class labels and number of classes\n",
        "class_names = get_class_labels('/content/handwritten-english-characters-and-digits/combined_folder/train')\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "jhNc7oR6kZWO",
        "outputId": "65e10dc8-6b16-489d-9a89-daaa697fea91"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "image_path = '/content/drive/MyDrive/sample_letters/9.png'\n",
        "# image_path = '/content/drive/MyDrive/sample_letters/a.png'\n",
        "# image_path = '/content/drive/MyDrive/sample_letters/k.png'\n",
        "# image_path = '/content/drive/MyDrive/sample_letters/o.png'\n",
        "# image_path = '/content/drive/MyDrive/sample_letters/x.png'\n",
        "\n",
        "# Load and prepare image\n",
        "image = prepare_image(image_path)\n",
        "image = image.to(device)\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
        "    confidence = probabilities[0][predicted_class].item() * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nClassification Results:\")\n",
        "print(f\"Predicted class: {class_names[predicted_class]}\")\n",
        "print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "# Print top 3 predictions\n",
        "top_k = 3\n",
        "top_probs, top_classes = torch.topk(probabilities, top_k)\n",
        "print(f\"\\nTop {top_k} predictions:\")\n",
        "for i in range(top_k):\n",
        "    class_idx = top_classes[0][i].item()\n",
        "    prob = top_probs[0][i].item() * 100\n",
        "    print(f\"{class_names[class_idx]}: {prob:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QbxsLb8cmem",
        "outputId": "47f16375-4f1a-4990-bf7c-ee392d073ce5"
      },
      "outputs": [],
      "source": [
        "def load_model_inference(model, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # Optionally load other info if needed\n",
        "    epoch = checkpoint['epoch']\n",
        "    val_acc = checkpoint['val_acc']\n",
        "    print(f\"Loaded model from epoch {epoch+1} with validation accuracy: {val_acc:.2f}%\")\n",
        "    return model\n",
        "\n",
        "model = VGG19HandwritingModel(62, \"cpu\")\n",
        "# model = load_model_inference(model, \"/content/drive/MyDrive/model_checkpoints_vgg/best_model.pth\")\n",
        "model = load_model_inference(model, \"/content/drive/MyDrive/model_checkpoints_vgg/best_model_vgg_full_train.pth\")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok8h8U9geO1y"
      },
      "outputs": [],
      "source": [
        "def visualize_letter_extraction(image_path, letters_info):\n",
        "    \"\"\"\n",
        "    Visualize the extracted letters and their bounding boxes\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the original image\n",
        "        letters_info (list): List of tuples containing letter and bounding box\n",
        "    \"\"\"\n",
        "    # Read the original image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Create a copy for visualization\n",
        "    visual = image.copy()\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for letter, bbox in letters_info:\n",
        "        x, y, w, h = bbox\n",
        "        cv2.rectangle(visual, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(visual, letter, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "    # Convert to RGB for displaying with matplotlib\n",
        "    visual_rgb = cv2.cvtColor(visual, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create a figure to display the original and processed images\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Display original image\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display processed image with bounding boxes\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.imshow(visual_rgb)\n",
        "    plt.title('Detected Letters')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create a grid of extracted letters\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.suptitle('Extracted Individual Letters', fontsize=16)\n",
        "\n",
        "    num_letters = len(letters_info)\n",
        "    cols = min(10, num_letters)\n",
        "    rows = (num_letters + cols - 1) // cols\n",
        "\n",
        "    for i, (letter, bbox) in enumerate(letters_info):\n",
        "        x, y, w, h = bbox\n",
        "        letter_img = image[y:y+h, x:x+w]\n",
        "        letter_img_rgb = cv2.cvtColor(letter_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(letter_img_rgb)\n",
        "        plt.title(f\"{letter}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR3_nxIGDpHN"
      },
      "outputs": [],
      "source": [
        "'''def extract_letters(image_path, output_dir=\"extracted_letters\"):\n",
        "    \"\"\"\n",
        "    Extract individual letters from a handwritten text image\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image\n",
        "        output_dir (str): Directory to save extracted letters\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing letter and its bounding box (x, y, w, h)\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply thresholding to get binary image\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Configure Tesseract parameters for character recognition\n",
        "    custom_config = r'--oem 3 --psm 10'  # PSM 10 treats the image as a single character\n",
        "\n",
        "    # Find contours for possible letters\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Filter contours based on size to avoid noise\n",
        "    min_area = 50  # Adjust based on your image\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "\n",
        "    transfoms = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    # Process each contour\n",
        "    letters_info = []\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        # Get bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract the letter\n",
        "        letter_image = gray[y:y+h, x:x+w]\n",
        "\n",
        "\n",
        "        # Save the letter image\n",
        "        letter_path = os.path.join(output_dir, f\"letter_{i}.png\")\n",
        "        cv2.imwrite(letter_path, letter_image)\n",
        "\n",
        "        letter_image = Image.fromarray(letter_image)\n",
        "        letter_image = transfoms(letter_image)\n",
        "        # Use Tesseract to recognize the character\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "\n",
        "                outputs = model(image)\n",
        "                detected_char = torch.argmax(outputs, dim=1).item()\n",
        "\n",
        "\n",
        "\n",
        "            # If no character is detected, use a placeholder\n",
        "            if not detected_char:\n",
        "                detected_char = \"?\"\n",
        "\n",
        "            # Keep only the first character if multiple are detected\n",
        "            detected_char = detected_char[0] if len(detected_char) > 0 else \"?\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error recognizing letter {i}: {e}\")\n",
        "            detected_char = \"?\"\n",
        "\n",
        "        letters_info.append((detected_char, (x, y, w, h)))\n",
        "\n",
        "    return letters_info'''\n",
        "\n",
        "def extract_letters_old(image_path, model, idx_to_char, output_dir=\"extracted_letters\"):\n",
        "    \"\"\"\n",
        "    Extract individual letters from a handwritten text image and classify them.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        model (torch.nn.Module): Trained handwriting recognition model.\n",
        "        idx_to_char (dict): Mapping from index to character.\n",
        "        output_dir (str): Directory to save extracted letters.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing classified letter and its bounding box.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Read and process the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    letters_info = []\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        letter_image = gray[y:y+h, x:x+w]\n",
        "\n",
        "        # Convert to PIL image and apply transformations\n",
        "        letter_image = Image.fromarray(letter_image)\n",
        "        letter_tensor = transform(letter_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = idx_to_char.get(predicted_idx, \"?\")\n",
        "\n",
        "        # Save letter image\n",
        "        letter_path = os.path.join(output_dir, f\"letter_{i}.png\")\n",
        "        cv2.imwrite(letter_path, gray[y:y+h, x:x+w])\n",
        "\n",
        "        letters_info.append((detected_char, (x, y, w, h)))\n",
        "\n",
        "    return letters_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0keAOfaqeQ2J"
      },
      "outputs": [],
      "source": [
        "def extract_letters(image_path, model, class_list, output_dir=\"extracted_letters\"):\n",
        "    \"\"\"\n",
        "    Extract individual letters from a handwritten text image and classify them.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        model (torch.nn.Module): Trained handwriting recognition model.\n",
        "        idx_to_char (dict): Mapping from index to character.\n",
        "        output_dir (str): Directory to save extracted letters.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing classified letter and its bounding box.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Read and process the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    letters_info = []\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        letter_image = gray[y:y+h, x:x+w]\n",
        "\n",
        "        # Convert to PIL image and apply transformations\n",
        "        letter_image = Image.fromarray(letter_image)\n",
        "        letter_tensor = transform(letter_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "\n",
        "\n",
        "        # Save letter image\n",
        "        letter_path = os.path.join(output_dir, f\"letter_{i}.png\")\n",
        "        cv2.imwrite(letter_path, gray[y:y+h, x:x+w])\n",
        "\n",
        "        letters_info.append((detected_char, (x, y, w, h)))\n",
        "\n",
        "    return letters_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "collapsed": true,
        "id": "G6t1XvcLdp_3",
        "outputId": "7aafb785-5763-4120-9f86-d641324fc2a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def demo():\n",
        "    \"\"\"\n",
        "    Demonstrate the letter extraction on a sample image\n",
        "    \"\"\"\n",
        "    # Extract letters\n",
        "    print(\"Extracting letters...\")\n",
        "    # letters_info = extract_letters(image_path)\n",
        "\n",
        "    # idx_to_char = {0: \"A\", 1: \"B\", 2: \"C\", ..., 25: \"Z\"}  # Define this based on training\n",
        "    letters_info = extract_letters(\"/content/drive/MyDrive/sample_letters/image lets go.png\", model, class_names)\n",
        "\n",
        "    # Print detected letters\n",
        "    print(\"Detected letters:\")\n",
        "    for letter, bbox in letters_info:\n",
        "        print(f\"Letter: {letter}, Bounding box: {bbox}\")\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_letter_extraction(image_path, letters_info)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AKKOC0qfPhB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qVTa7sBfPdr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfllKqJ8fPax"
      },
      "outputs": [],
      "source": [
        "def extract_letters(image_path, model, class_list, output_dir=\"extracted_letters\", spacing=0):\n",
        "    \"\"\"\n",
        "    Extract individual letters from a handwritten text image and classify them.\n",
        "    Uses two different thresholding methods:\n",
        "    1. Inverted OTSU thresholding for contour detection\n",
        "    2. Non-inverted adaptive thresholding for model input images\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Read and process the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
        "\n",
        "    # Visualize original image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.savefig(f\"{output_dir}/1_original.png\")\n",
        "    plt.show()\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Visualize grayscale image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(gray, cmap='gray')\n",
        "    plt.title(\"Grayscale Image\")\n",
        "    plt.savefig(f\"{output_dir}/2_grayscale.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Apply OTSU thresholding (inverted) for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    # Visualize binary image for contour detection\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(binary_contours, cmap='gray')\n",
        "    plt.title(\"Binary Image for Contours (Inverted OTSU)\")\n",
        "    plt.savefig(f\"{output_dir}/3a_binary_contours.png\")\n",
        "    plt.show()\n",
        "\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # binary_model = cv2.adaptiveThreshold(\n",
        "    #     gray,\n",
        "    #     255,\n",
        "    #     cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "    #     cv2.THRESH_BINARY,\n",
        "    #     11,  # Block size\n",
        "    #     2    # C constant (subtracted from mean)\n",
        "    # )\n",
        "    # Visualize binary image for model input\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(binary_model, cmap='gray')\n",
        "    plt.title(\"Binary Image for Model Input (Non-inverted Adaptive)\")\n",
        "    plt.savefig(f\"{output_dir}/3b_binary_model.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Find contours on the inverted binary image (for contour detection)\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Visualize contours\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"All Contours Found: {len(contours)}\")\n",
        "    plt.savefig(f\"{output_dir}/4_all_contours.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    print(f\"Found {len(contours)} total contours, {len(letter_contours)} after filtering by min area {min_area}\")\n",
        "\n",
        "    # Visualize filtered contours\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Filtered Contours: {len(letter_contours)}\")\n",
        "    plt.savefig(f\"{output_dir}/5_filtered_contours.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Sort contours from left to right (for reading order)\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Create a figure to show all extracted letters\n",
        "    num_letters = len(letter_contours)\n",
        "    fig_rows = max(1, (num_letters + 4) // 5)  # 5 letters per row\n",
        "    fig_cols = min(5, num_letters)\n",
        "\n",
        "    plt.figure(figsize=(15, 3 * fig_rows))\n",
        "    plt.suptitle(\"Extracted Letters Before Processing\", fontsize=16)\n",
        "\n",
        "    letters_info = []\n",
        "    model_input_images = []  # Store images that will be fed to the model\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract letter from the ADAPTIVE THRESHOLDED binary image for model input\n",
        "        # This uses the non-inverted adaptive thresholding for cleaner results\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "\n",
        "        # Save original extracted letter\n",
        "        letter_path = os.path.join(output_dir, f\"letter_{i}_original.png\")\n",
        "        cv2.imwrite(letter_path, letter_image)\n",
        "\n",
        "        # Add to the visualization figure\n",
        "        if i < fig_rows * fig_cols:\n",
        "            plt.subplot(fig_rows, fig_cols, i + 1)\n",
        "            plt.imshow(letter_image, cmap='gray')\n",
        "            plt.title(f\"Letter {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Fix the second issue: Maintain aspect ratio during resize\n",
        "        # Calculate target size while maintaining aspect ratio\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        # Determine which dimension is larger\n",
        "        if w > h:\n",
        "            # Width is the dominant dimension\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            # Height is the dominant dimension\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image (255 is white for non-inverted binary)\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Save the padded image\n",
        "        padded_image.save(os.path.join(output_dir, f\"letter_{i}_padded.png\"))\n",
        "\n",
        "        # Create the final 64x64 image with the specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Save the tensor representation\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        # Denormalize for visualization (undo normalization)\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)  # Just take the first channel\n",
        "\n",
        "        # Save the final processed image that will be fed to the model\n",
        "        cv2.imwrite(os.path.join(output_dir, f\"letter_{i}_model_input.png\"), transformed_image)\n",
        "        model_input_images.append(transformed_image)\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "            confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "            print(f\"Letter {i}: Classified as '{detected_char}' with confidence {confidence:.2f}\")\n",
        "\n",
        "        letters_info.append((detected_char, (x, y, w, h)))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Make room for the suptitle\n",
        "    plt.savefig(f\"{output_dir}/6_extracted_letters.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize the final processed images that are fed to the model\n",
        "    plt.figure(figsize=(15, 3 * fig_rows))\n",
        "    plt.suptitle(\"Processed Images Fed to Model (64x64 with spacing)\", fontsize=16)\n",
        "\n",
        "    for i, img in enumerate(model_input_images):\n",
        "        if i < fig_rows * fig_cols:\n",
        "            ax = plt.subplot(fig_rows, fig_cols, i + 1)\n",
        "\n",
        "            # Display the grayscale image\n",
        "            plt.imshow(img, cmap='gray')\n",
        "\n",
        "            # Add a red border around the full 64x64 image\n",
        "            rect_outer = patches.Rectangle((0, 0), 63, 63, linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect_outer)\n",
        "\n",
        "            # Add a blue border around the inner area\n",
        "            inner_size = 64 - spacing - 1\n",
        "            spacing_half = spacing // 2\n",
        "            rect_inner = patches.Rectangle((spacing_half, spacing_half), inner_size, inner_size,\n",
        "                                          linewidth=1, edgecolor='b', facecolor='none')\n",
        "            ax.add_patch(rect_inner)\n",
        "\n",
        "            plt.title(f\"Letter {i}: {letters_info[i][0]}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(f\"{output_dir}/7_model_inputs_with_border.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Draw bounding boxes on original image\n",
        "    result_img = image.copy()\n",
        "    for i, (char, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, char, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Final Result with Classifications\")\n",
        "    plt.savefig(f\"{output_dir}/8_final_result.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return letters_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qb7LSxfafPWa",
        "outputId": "1ffc7d25-d8df-4964-8346-00676eb5923c"
      },
      "outputs": [],
      "source": [
        "def demo():\n",
        "    \"\"\"\n",
        "    Demonstrate the letter extraction on a sample image\n",
        "    \"\"\"\n",
        "    # Extract letters\n",
        "    print(\"Extracting letters...\")\n",
        "\n",
        "    image_path = \"/content/drive/MyDrive/sample_letters/here_we_go.jpg\"\n",
        "    image_path = \"/content/drive/MyDrive/sample_letters/20250331_193146.jpg\"\n",
        "    image_path = \"/content/drive/MyDrive/sample_letters/20250331_230539.jpg\"\n",
        "    image_path = \"/content/drive/MyDrive/sample_letters/‏‏לכידה.PNG\"\n",
        "\n",
        "    letters_info = extract_letters(image_path, model, class_names, spacing=24)\n",
        "\n",
        "    # Print detected letters\n",
        "    print(\"Detected letters:\")\n",
        "    for letter, bbox in letters_info:\n",
        "        print(f\"Letter: {letter}, Bounding box: {bbox}\")\n",
        "\n",
        "    # If you have a separate visualization function\n",
        "    # visualize_letter_extraction(image_path, letters_info)\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "TZ3xxyRQduOS",
        "outputId": "b4bb846e-134a-43c3-a172-8b6dc6e83e1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_drawn_image(image, model, class_list, spacing=24):\n",
        "    \"\"\"\n",
        "    Process a drawn image from Gradio and extract/classify letters\n",
        "    \"\"\"\n",
        "    output_dir = \"extracted_letters\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Convert gradio image (RGB numpy array) to BGR for OpenCV compatibility\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Create a list to store all the visualization images\n",
        "    visualization_images = []\n",
        "\n",
        "    # 1. Original Image\n",
        "    orig_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Original Image\", orig_img))\n",
        "\n",
        "    # 2. Grayscale conversion\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    visualization_images.append((\"Grayscale Image\", gray))\n",
        "\n",
        "    # 3a. Binary image for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Contours (Inverted OTSU)\", binary_contours))\n",
        "\n",
        "    # 3b. Binary image for model input\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Model Input\", binary_model))\n",
        "\n",
        "    # 4. Find contours\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"All Contours Found: {len(contours)}\", contour_img_rgb))\n",
        "\n",
        "    # 5. Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    filtered_contour_img_rgb = cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"Filtered Contours: {len(letter_contours)}\", filtered_contour_img_rgb))\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Process each letter\n",
        "    letters_info = []\n",
        "    letter_images = []\n",
        "    model_input_images = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract letter from binary image\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "        letter_images.append((f\"Letter {i}\", letter_image))\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Maintain aspect ratio during resize\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        if w > h:\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Create the final 64x64 image with specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Save the processed image for visualization\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)\n",
        "        model_input_images.append((f\"Letter {i} Model Input\", transformed_image))\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "            confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "\n",
        "        letters_info.append((detected_char, confidence, (x, y, w, h)))\n",
        "\n",
        "    # Create final result image with classifications\n",
        "    result_img = image.copy()\n",
        "    detected_text = \"\"\n",
        "\n",
        "    for i, (char, confidence, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, f\"{char} ({confidence:.2f})\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        detected_text += char\n",
        "\n",
        "    result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Final Result with Classifications\", result_img_rgb))\n",
        "\n",
        "    # Combine all letter images and model inputs into group images for better display\n",
        "    letter_grid = create_image_grid([img for _, img in letter_images], f\"Extracted Letters ({len(letter_images)})\")\n",
        "    model_input_grid = create_image_grid([img for _, img in model_input_images],\n",
        "                                         f\"Model Input Images ({len(model_input_images)})\")\n",
        "\n",
        "    if letter_grid is not None:\n",
        "        visualization_images.append((\"Extracted Letters\", letter_grid))\n",
        "\n",
        "    if model_input_grid is not None:\n",
        "        visualization_images.append((\"Model Input Images\", model_input_grid))\n",
        "\n",
        "    return visualization_images, detected_text\n",
        "\n",
        "def create_image_grid(images, title):\n",
        "    \"\"\"Create a grid of images for better visualization\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    num_images = len(images)\n",
        "    cols = min(5, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 2 * rows))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Add images to the grid\n",
        "    for i, img in enumerate(images):\n",
        "        if i < rows * cols:\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Image {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Convert matplotlib figure to image\n",
        "    fig = plt.gcf()\n",
        "    fig.canvas.draw()\n",
        "    grid_image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    grid_image = grid_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.close()\n",
        "\n",
        "    return grid_image\n",
        "\n",
        "def gradio_interface(model, class_list):\n",
        "    \"\"\"Create a Gradio interface for the letter extraction and classification\"\"\"\n",
        "\n",
        "    def process_image(image):\n",
        "        if image is None:\n",
        "            return [(\"No Image Provided\", np.zeros((100, 100, 3), dtype=np.uint8))], \"No image provided\"\n",
        "\n",
        "        # Process the drawn image\n",
        "        visualization_images, detected_text = process_drawn_image(image, model, class_list)\n",
        "\n",
        "        # Return only the images as a list for the gallery\n",
        "        return [img for _, img in visualization_images], detected_text\n",
        "\n",
        "    # Define function to handle the sketchpad output\n",
        "    def process_sketch(sketch):\n",
        "        # Convert the black and white sketch to a proper image\n",
        "        if sketch is None:\n",
        "            return [], \"No image drawn\"\n",
        "\n",
        "        # Make sure sketch is 3-channel RGB\n",
        "        if len(sketch.shape) == 2:\n",
        "            sketch = np.stack([sketch, sketch, sketch], axis=2)\n",
        "\n",
        "        return process_image(sketch)\n",
        "\n",
        "    # Create the interface with the compatible components\n",
        "    with gr.Blocks(title=\"Handwritten Letter Recognition\") as demo:\n",
        "        gr.Markdown(\"# Handwritten Letter Recognition\")\n",
        "        gr.Markdown(\"Draw letters or a sentence using your mouse, then click 'Process' to recognize the text.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # For older Gradio versions, use Sketchpad instead of Image with source=\"canvas\"\n",
        "                input_sketch = gr.Sketchpad( label=\"Draw Text Here\", height = 400, width = 600)\n",
        "\n",
        "                process_btn = gr.Button(\"Process\")\n",
        "                clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                output_text = gr.Textbox(label=\"Recognized Text\")\n",
        "                output_gallery = gr.Gallery(\n",
        "                    label=\"Processing Steps\",\n",
        "                    show_label=True,\n",
        "                    columns=2,\n",
        "                    rows=4,\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "\n",
        "        process_btn.click(fn=process_sketch, inputs=input_sketch, outputs=[output_gallery, output_text])\n",
        "        clear_btn.click(fn=lambda: None, inputs=None, outputs=input_sketch)\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## How It Works\n",
        "        1. Draw text using your mouse in the canvas\n",
        "        2. Click 'Process' to analyze the image\n",
        "        3. The system will:\n",
        "           - Convert the image to binary format\n",
        "           - Find letter contours\n",
        "           - Extract each letter\n",
        "           - Process for classification\n",
        "           - Predict the character for each letter\n",
        "        4. View all processing steps in the gallery\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "def launch_interface(model, class_list):\n",
        "    \"\"\"Launch the Gradio interface\"\"\"\n",
        "    interface = gradio_interface(model, class_list)\n",
        "    interface.launch(share=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming model and class_names are already defined in your code\n",
        "    launch_interface(model, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "7mNQZ7zildOo",
        "outputId": "1022f2b7-0c58-41e0-9924-2236f244dcdd"
      },
      "outputs": [],
      "source": [
        "def process_drawn_image(image, model, class_list, spacing=24):\n",
        "    \"\"\"\n",
        "    Process a drawn image from Gradio and extract/classify letters\n",
        "    \"\"\"\n",
        "    output_dir = \"extracted_letters\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Convert gradio image (RGB numpy array) to BGR for OpenCV compatibility\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Create a list to store all the visualization images\n",
        "    visualization_images = []\n",
        "\n",
        "    # 1. Original Image\n",
        "    orig_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Original Image\", orig_img))\n",
        "\n",
        "    # 2. Grayscale conversion\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    visualization_images.append((\"Grayscale Image\", gray))\n",
        "\n",
        "    # 3a. Binary image for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Contours (Inverted OTSU)\", binary_contours))\n",
        "\n",
        "    # 3b. Binary image for model input\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Model Input\", binary_model))\n",
        "\n",
        "    # 4. Find contours\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"All Contours Found: {len(contours)}\", contour_img_rgb))\n",
        "\n",
        "    # 5. Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    filtered_contour_img_rgb = cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"Filtered Contours: {len(letter_contours)}\", filtered_contour_img_rgb))\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Process each letter\n",
        "    letters_info = []\n",
        "    letter_images = []\n",
        "    model_input_images = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract letter from binary image\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "        letter_images.append((f\"Letter {i}\", letter_image))\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Maintain aspect ratio during resize\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        if w > h:\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Create the final 64x64 image with specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Save the processed image for visualization\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)\n",
        "        model_input_images.append((f\"Letter {i} Model Input\", transformed_image))\n",
        "\n",
        "        # Classify the letter using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(letter_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            detected_char = class_list[predicted_idx]\n",
        "            confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "\n",
        "        letters_info.append((detected_char, confidence, (x, y, w, h)))\n",
        "\n",
        "    # Create final result image with classifications\n",
        "    result_img = image.copy()\n",
        "    detected_text = \"\"\n",
        "\n",
        "    for i, (char, confidence, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, f\"{char} ({confidence:.2f})\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        detected_text += char\n",
        "\n",
        "    result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Final Result with Classifications\", result_img_rgb))\n",
        "\n",
        "    # Combine all letter images and model inputs into group images for better display\n",
        "    letter_grid = create_image_grid([img for _, img in letter_images], f\"Extracted Letters ({len(letter_images)})\")\n",
        "    model_input_grid = create_image_grid([img for _, img in model_input_images],\n",
        "                                         f\"Model Input Images ({len(model_input_images)})\")\n",
        "\n",
        "    if letter_grid is not None:\n",
        "        visualization_images.append((\"Extracted Letters\", letter_grid))\n",
        "\n",
        "    if model_input_grid is not None:\n",
        "        visualization_images.append((\"Model Input Images\", model_input_grid))\n",
        "\n",
        "    return visualization_images, detected_text\n",
        "\n",
        "def create_image_grid(images, title):\n",
        "    \"\"\"Create a grid of images for better visualization\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    num_images = len(images)\n",
        "    cols = min(5, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 2 * rows))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Add images to the grid\n",
        "    for i, img in enumerate(images):\n",
        "        if i < rows * cols:\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Image {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Convert matplotlib figure to image\n",
        "    fig = plt.gcf()\n",
        "    fig.canvas.draw()\n",
        "    grid_image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    grid_image = grid_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.close()\n",
        "\n",
        "    return grid_image\n",
        "\n",
        "def launch_interface(model, class_list):\n",
        "    \"\"\"Create the most basic compatible interface for older Gradio versions\"\"\"\n",
        "\n",
        "    def process_sketch(sketch):\n",
        "        # Convert the sketch to a proper image\n",
        "        if sketch is None:\n",
        "            return \"No image drawn\", None\n",
        "\n",
        "        # Make sure sketch is 3-channel RGB\n",
        "        if len(sketch.shape) == 2:\n",
        "            sketch = np.stack([sketch, sketch, sketch], axis=2)\n",
        "\n",
        "        # Process the drawn image\n",
        "        visualization_images, detected_text = process_drawn_image(sketch, model, class_list)\n",
        "\n",
        "        # Combine all images into a single display\n",
        "        combined_img = create_combined_image(visualization_images)\n",
        "\n",
        "        return detected_text, combined_img\n",
        "\n",
        "    def create_combined_image(visualization_images):\n",
        "        \"\"\"Create a single image with all visualization steps\"\"\"\n",
        "        # Determine a reasonable layout\n",
        "        num_images = len(visualization_images)\n",
        "        cols = min(3, num_images)\n",
        "        rows = (num_images + cols - 1) // cols\n",
        "\n",
        "        plt.figure(figsize=(15, 5 * rows))\n",
        "\n",
        "        for i, (title, img) in enumerate(visualization_images):\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "            # Handle grayscale images\n",
        "            if len(img.shape) == 2:\n",
        "                plt.imshow(img, cmap='gray')\n",
        "            else:\n",
        "                plt.imshow(img)\n",
        "\n",
        "            plt.title(title)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert matplotlib figure to image\n",
        "        fig = plt.gcf()\n",
        "        fig.canvas.draw()\n",
        "        combined_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        combined_img = combined_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close()\n",
        "\n",
        "        return combined_img\n",
        "\n",
        "    # Create a very basic interface that should work with any Gradio version\n",
        "    interface = gr.Interface(\n",
        "        fn=process_sketch,\n",
        "        inputs=gr.Sketchpad(),\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Recognized Text\"),\n",
        "            gr.Image(label=\"Processing Steps\")\n",
        "        ],\n",
        "        title=\"Handwritten Letter Recognition\",\n",
        "        description=\"Draw letters or a sentence using your mouse, then click submit to recognize the text.\"\n",
        "    )\n",
        "\n",
        "    interface.launch(share=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming model and class_names are already defined in your code\n",
        "    launch_interface(model, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DexXBMfA7ufj",
        "outputId": "53d3a3d8-0b71-48df-cb94-8d1275f66286"
      },
      "outputs": [],
      "source": [
        "print(gr.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "XNQ5WjFKp4MQ",
        "outputId": "eae7d12b-44f1-4115-f637-c08bda72d5d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_uploaded_image(image, model, class_list, spacing=24):\n",
        "    \"\"\"\n",
        "    Process an uploaded image and extract/classify letters\n",
        "    \"\"\"\n",
        "    # Check if image is None\n",
        "    if image is None:\n",
        "        return [], \"No image provided\"\n",
        "\n",
        "    output_dir = \"extracted_letters\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # Make sure image is in the right format for processing\n",
        "    # Convert PIL image to numpy if needed\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        image = np.array(image)\n",
        "\n",
        "    # Make sure image is in RGB format\n",
        "    if len(image.shape) == 2:  # If grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    elif len(image.shape) == 3 and image.shape[2] == 4:  # If RGBA\n",
        "        image = image[:, :, :3]  # Drop alpha channel\n",
        "\n",
        "    # Convert to BGR for OpenCV functions\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Create a list to store all the visualization images\n",
        "    visualization_images = []\n",
        "\n",
        "    # 1. Original Image\n",
        "    orig_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Original Image\", orig_img))\n",
        "\n",
        "    # 2. Grayscale conversion\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    visualization_images.append((\"Grayscale Image\", gray))\n",
        "\n",
        "    # 3a. Binary image for contour detection\n",
        "    _, binary_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Contours (Inverted OTSU)\", binary_contours))\n",
        "\n",
        "    # 3b. Binary image for model input\n",
        "    _, binary_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    visualization_images.append((\"Binary Image for Model Input\", binary_model))\n",
        "\n",
        "    # 4. Find contours\n",
        "    contours, _ = cv2.findContours(binary_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contour_img = image.copy()\n",
        "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
        "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"All Contours Found: {len(contours)}\", contour_img_rgb))\n",
        "\n",
        "    # Check if we found any contours\n",
        "    if len(contours) == 0:\n",
        "        return visualization_images, \"No letters detected\"\n",
        "\n",
        "    # 5. Filter small contours\n",
        "    min_area = 50\n",
        "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "    filtered_contour_img = image.copy()\n",
        "    cv2.drawContours(filtered_contour_img, letter_contours, -1, (0, 255, 0), 2)\n",
        "    filtered_contour_img_rgb = cv2.cvtColor(filtered_contour_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((f\"Filtered Contours: {len(letter_contours)}\", filtered_contour_img_rgb))\n",
        "\n",
        "    # Check if we have any letters after filtering\n",
        "    if len(letter_contours) == 0:\n",
        "        return visualization_images, \"No valid letters detected after filtering\"\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
        "\n",
        "    # Process each letter\n",
        "    letters_info = []\n",
        "    letter_images = []\n",
        "    model_input_images = []\n",
        "\n",
        "    # Ensure model is available\n",
        "    if model is None:\n",
        "        return visualization_images, \"Model not loaded\"\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Ensure device compatibility\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    for i, contour in enumerate(letter_contours):\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Skip extremely small contours that passed the area filter\n",
        "        if w < 5 or h < 5:\n",
        "            continue\n",
        "\n",
        "        # Extract letter from binary image\n",
        "        letter_image = binary_model[y:y+h, x:x+w]\n",
        "        letter_images.append((f\"Letter {i}\", letter_image))\n",
        "\n",
        "        # Convert to PIL image\n",
        "        letter_image_pil = Image.fromarray(letter_image)\n",
        "\n",
        "        # Maintain aspect ratio during resize\n",
        "        target_size = 64 - spacing\n",
        "\n",
        "        if w > h:\n",
        "            new_width = target_size\n",
        "            new_height = int((h / w) * target_size)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((w / h) * target_size)\n",
        "\n",
        "        # Resize while maintaining aspect ratio\n",
        "        resized_image = letter_image_pil.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create a new square white image\n",
        "        padded_image = Image.new('L', (target_size, target_size), 255)\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        pad_x = (target_size - new_width) // 2\n",
        "        pad_y = (target_size - new_height) // 2\n",
        "\n",
        "        # Paste the resized image at the center\n",
        "        padded_image.paste(resized_image, (pad_x, pad_y))\n",
        "\n",
        "        # Create the final 64x64 image with specified spacing\n",
        "        final_image = Image.new('L', (64, 64), 255)\n",
        "        spacing_padding = spacing // 2\n",
        "        final_image.paste(padded_image, (spacing_padding, spacing_padding))\n",
        "\n",
        "        # Convert to tensor and normalize\n",
        "        normalize_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        letter_tensor = normalize_transform(final_image).unsqueeze(0)  # Add batch dimension\n",
        "        letter_tensor = letter_tensor.to(device)\n",
        "\n",
        "        # Save the processed image for visualization\n",
        "        transformed_image = letter_tensor[0].cpu().numpy()\n",
        "        transformed_image = (transformed_image * 0.5 + 0.5) * 255\n",
        "        transformed_image = transformed_image[0].astype(np.uint8)\n",
        "        model_input_images.append((f\"Letter {i} Model Input\", transformed_image))\n",
        "\n",
        "        try:\n",
        "            # Classify the letter using the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(letter_tensor)\n",
        "                predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "                detected_char = class_list[predicted_idx]\n",
        "                confidence = torch.softmax(outputs, dim=1)[0][predicted_idx].item()\n",
        "\n",
        "            letters_info.append((detected_char, confidence, (x, y, w, h)))\n",
        "        except Exception as e:\n",
        "            print(f\"Error classifying letter {i}: {e}\")\n",
        "            # Continue with next letter instead of failing completely\n",
        "            continue\n",
        "\n",
        "    # Create final result image with classifications\n",
        "    result_img = image.copy()\n",
        "    detected_text = \"\"\n",
        "\n",
        "    for i, (char, confidence, (x, y, w, h)) in enumerate(letters_info):\n",
        "        cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(result_img, f\"{char} ({confidence:.2f})\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        detected_text += char\n",
        "\n",
        "    result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    visualization_images.append((\"Final Result with Classifications\", result_img_rgb))\n",
        "\n",
        "    # Combine all letter images and model inputs into group images for better display\n",
        "    if letter_images:\n",
        "        letter_grid = create_image_grid([img for _, img in letter_images], f\"Extracted Letters ({len(letter_images)})\")\n",
        "        if letter_grid is not None:\n",
        "            visualization_images.append((\"Extracted Letters\", letter_grid))\n",
        "\n",
        "    if model_input_images:\n",
        "        model_input_grid = create_image_grid([img for _, img in model_input_images],\n",
        "                                            f\"Model Input Images ({len(model_input_images)})\")\n",
        "        if model_input_grid is not None:\n",
        "            visualization_images.append((\"Model Input Images\", model_input_grid))\n",
        "\n",
        "    return visualization_images, detected_text\n",
        "\n",
        "def create_image_grid(images, title):\n",
        "    \"\"\"Create a grid of images for better visualization\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    num_images = len(images)\n",
        "    cols = min(5, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 2 * rows))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Add images to the grid\n",
        "    for i, img in enumerate(images):\n",
        "        if i < rows * cols:\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Image {i}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    try:\n",
        "        # Convert matplotlib figure to image\n",
        "        fig = plt.gcf()\n",
        "        fig.canvas.draw()\n",
        "        grid_image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        grid_image = grid_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close()\n",
        "        return grid_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating image grid: {e}\")\n",
        "        plt.close()\n",
        "        return None\n",
        "\n",
        "def create_combined_image(visualization_images):\n",
        "    \"\"\"Create a single image with all visualization steps\"\"\"\n",
        "    # Check if we have any images\n",
        "    if not visualization_images:\n",
        "        # Return empty image\n",
        "        empty_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "        cv2.putText(empty_img, \"No visualization available\", (50, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
        "        return empty_img\n",
        "\n",
        "    # Determine a reasonable layout\n",
        "    num_images = len(visualization_images)\n",
        "    cols = min(3, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 5 * rows))\n",
        "\n",
        "        for i, (title, img) in enumerate(visualization_images):\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "            # Handle grayscale images\n",
        "            if len(img.shape) == 2:\n",
        "                plt.imshow(img, cmap='gray')\n",
        "            else:\n",
        "                plt.imshow(img)\n",
        "\n",
        "            plt.title(title)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert matplotlib figure to image\n",
        "        fig = plt.gcf()\n",
        "        fig.canvas.draw()\n",
        "        combined_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        combined_img = combined_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close()\n",
        "\n",
        "        return combined_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating combined image: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "        # Return a simple error image instead\n",
        "        error_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "        cv2.putText(error_img, f\"Error: {str(e)}\", (50, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        return error_img\n",
        "\n",
        "def process_image(image, model, class_list):\n",
        "    \"\"\"Process the uploaded image and handle potential errors\"\"\"\n",
        "    try:\n",
        "        if image is None:\n",
        "            error_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "            cv2.putText(error_img, \"No image uploaded\", (50, 150),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
        "            return \"No image uploaded\", error_img\n",
        "\n",
        "        # Process the uploaded image\n",
        "        visualization_images, detected_text = process_uploaded_image(image, model, class_list)\n",
        "\n",
        "        # Combine all images into a single display\n",
        "        combined_img = create_combined_image(visualization_images)\n",
        "\n",
        "        return detected_text, combined_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in process_image: {e}\")\n",
        "\n",
        "        # Create error image\n",
        "        error_img = np.ones((300, 500, 3), dtype=np.uint8) * 255\n",
        "        cv2.putText(error_img, f\"Error: {str(e)}\", (50, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        return f\"Error: {str(e)}\", error_img\n",
        "\n",
        "def launch_interface(model, class_list):\n",
        "    \"\"\"Create a robust interface with image upload that handles errors gracefully\"\"\"\n",
        "\n",
        "    # Create a function that pre-binds the model and class_list\n",
        "    def process_image_bound(image):\n",
        "        return process_image(image, model, class_list)\n",
        "\n",
        "    # Create a user-friendly interface with image upload\n",
        "    with gr.Blocks() as interface:\n",
        "        gr.Markdown(\"# Handwritten Letter Recognition\")\n",
        "        gr.Markdown(\"Upload an image with handwritten letters to recognize the text.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload Handwritten Image\")\n",
        "                submit_btn = gr.Button(\"Recognize Text\")\n",
        "\n",
        "                # Add example images if you have any\n",
        "                example_folder = \"example_images\"\n",
        "                if os.path.exists(example_folder):\n",
        "                    example_images = [os.path.join(example_folder, f) for f in os.listdir(example_folder)\n",
        "                                     if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "                    if example_images:\n",
        "                        gr.Examples(examples=example_images, inputs=image_input)\n",
        "\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Recognized Text\")\n",
        "                image_output = gr.Image(label=\"Processing Steps\")\n",
        "\n",
        "        # Add preprocessing options\n",
        "        with gr.Accordion(\"Advanced Options\", open=False):\n",
        "            gr.Markdown(\"These options will be implemented in a future version\")\n",
        "            # Placeholder for future enhancements\n",
        "            # resize_slider = gr.Slider(minimum=0.1, maximum=2.0, value=1.0, step=0.1, label=\"Resize Factor\")\n",
        "            # invert_checkbox = gr.Checkbox(label=\"Invert Image\", value=False)\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=process_image_bound,\n",
        "            inputs=image_input,\n",
        "            outputs=[text_output, image_output]\n",
        "        )\n",
        "\n",
        "        # Also process when the image is uploaded\n",
        "        image_input.change(\n",
        "            fn=process_image_bound,\n",
        "            inputs=image_input,\n",
        "            outputs=[text_output, image_output]\n",
        "        )\n",
        "\n",
        "    # Launch the interface\n",
        "    interface.launch(share=True)\n",
        "\n",
        "# Example of how to define class_names if not already defined\n",
        "# class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "#               'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        "#               'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        "#               'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "#               'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
        "#               'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
        "#               'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure model and class_names are properly defined before launching\n",
        "    # If they're not defined, you'll need code like this:\n",
        "    #\n",
        "    # import torch\n",
        "    # model = YourModelClass(...)  # Initialize your model\n",
        "    # model.load_state_dict(torch.load('your_model_path.pth'))\n",
        "    # class_names = [...]  # Define your class names list\n",
        "\n",
        "    try:\n",
        "        launch_interface(model, class_names)\n",
        "    except NameError as e:\n",
        "        print(f\"Error: {e}. Make sure 'model' and 'class_names' are defined before calling launch_interface.\")\n",
        "        print(\"Example code:\")\n",
        "        print(\"model = YourModelClass(...)\")\n",
        "        print(\"model.load_state_dict(torch.load('your_model_path.pth'))\")\n",
        "        print(\"class_names = ['A', 'B', 'C', ...] # Your list of characters\")\n",
        "        print(\"launch_interface(model, class_names)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
