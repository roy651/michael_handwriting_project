{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition - Gradio Application\n",
    "\n",
    "This notebook launches an interactive Gradio application for handwritten character recognition. You can either draw characters/words directly in the interface or upload an image containing handwritten text. The application will then attempt to segment and recognize the characters using a pre-trained model.\n",
    "\n",
    "**Prerequisites:**\n",
    "1.  **Trained Models:** Ensure you have trained models saved in the appropriate checkpoint directories (e.g., `./model_checkpoints/cnn/best_model.pth` or `./model_checkpoints/vgg/best_model.pth`). The training notebook (`training.ipynb`) should produce these.\n",
    "2.  **Dataset for Class Labels:** The path to the root of the training dataset is needed to derive the class labels (character names). Update `data_root_for_app_labels` if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Image processing and display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 # OpenCV for image operations\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch essentials\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim # May not be needed directly, but models might reference it if part of saved state\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets # For get_class_labels\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from pathlib import Path \n",
    "\n",
    "# Gradio for the web application\n",
    "!pip install gradio -q\n",
    "import gradio as gr\n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "The definitions of the models are needed to load the saved weights. Ensure these match the definitions used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Custom CNNs (`LetterCNN64`, `ImprovedLetterCNN`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterCNN64(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LetterCNN64, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImprovedLetterCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ImprovedLetterCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = self.dropout1(self.relu_fc1(self.bn_fc1(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu_fc2(self.bn_fc2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. VGG19 Transfer Learning Model (`VGG19HandwritingModel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19HandwritingModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, pretrained=True):\n",
    "        super(VGG19HandwritingModel, self).__init__()\n",
    "        self.device = device\n",
    "        vgg19 = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        vgg19 = vgg19.to(device)\n",
    "        self.features = vgg19.features\n",
    "        if pretrained:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_features_output = 512 * 2 * 2 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features_output, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        ).to(device)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_inference(model, checkpoint_path):\n",
    "    \"\"\"Loads a model checkpoint for inference.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"ERROR: Checkpoint path {checkpoint_path} does not exist. Cannot load model.\")\n",
    "        return None # Return None if checkpoint not found\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint) # Fallback for older checkpoints\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {checkpoint_path} and set to evaluation mode.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading model from {checkpoint_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_class_labels_from_dir(data_root_for_labels):\n",
    "    \"\"\"Gets class labels from the folder names in data_root (ImageFolder structure).\"\"\"\n",
    "    if not os.path.exists(data_root_for_labels):\n",
    "        print(f\"Error: Data root for labels '{data_root_for_labels}' not found.\")\n",
    "        return []\n",
    "    try:\n",
    "        temp_dataset = datasets.ImageFolder(root=data_root_for_labels)\n",
    "        return temp_dataset.classes\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting class labels from '{data_root_for_labels}': {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio Application Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for Gradio App ---\n",
    "MODEL_PATH_CNN_APP = os.path.join('model_checkpoints', 'cnn', 'best_model.pth') \n",
    "MODEL_PATH_VGG_APP = os.path.join('model_checkpoints', 'vgg', 'best_model.pth')\n",
    "DATA_ROOT_FOR_APP_LABELS = \"./datasets/handwritten-english/augmented_images/augmented_images1\" # <<< USER: Update if your dataset path is different\n",
    "\n",
    "app_class_labels = get_class_labels_from_dir(DATA_ROOT_FOR_APP_LABELS)\n",
    "app_num_classes = len(app_class_labels)\n",
    "\n",
    "if not app_class_labels:\n",
    "    print(\"WARNING: Could not load class labels. Predictions might be incorrect or app might fail.\")\n",
    "    print(\"Ensure DATA_ROOT_FOR_APP_LABELS points to a valid ImageFolder dataset structure.\")\n",
    "    # Fallback to a generic list if labels can't be loaded, though this is not ideal\n",
    "    app_class_labels = [str(i) for i in range(62)] # Assuming 62 classes if specific labels are missing\n",
    "    app_num_classes = 62 \n",
    "\n",
    "app_model = None\n",
    "selected_model_name = \"ImprovedLetterCNN\" # Default choice\n",
    "\n",
    "# Try loading CNN model first, then VGG if CNN is not found\n",
    "if os.path.exists(MODEL_PATH_CNN_APP):\n",
    "    print(f\"Attempting to load {selected_model_name} from {MODEL_PATH_CNN_APP}\")\n",
    "    temp_model_cnn = ImprovedLetterCNN(app_num_classes).to(device)\n",
    "    app_model = load_model_for_inference(temp_model_cnn, MODEL_PATH_CNN_APP)\n",
    "elif os.path.exists(MODEL_PATH_VGG_APP):\n",
    "    selected_model_name = \"VGG19HandwritingModel\"\n",
    "    print(f\"CNN model not found. Attempting to load {selected_model_name} from {MODEL_PATH_VGG_APP}\")\n",
    "    temp_model_vgg = VGG19HandwritingModel(app_num_classes, device, pretrained=False).to(device)\n",
    "    app_model = load_model_for_inference(temp_model_vgg, MODEL_PATH_VGG_APP)\n",
    "else:\n",
    "    print(f\"ERROR: No model checkpoint found at {MODEL_PATH_CNN_APP} or {MODEL_PATH_VGG_APP}.\")\n",
    "    print(\"Please ensure a trained model is available at one of these paths for the app to function.\")\n",
    "\n",
    "if app_model:\n",
    "    print(f\"Successfully loaded {selected_model_name} for the Gradio app.\")\n",
    "else:\n",
    "    print(f\"WARNING: Failed to load any model. The Gradio app might not work correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_for_gradio(image_input_np, model_to_use, class_list, spacing=24):\n",
    "    \"\"\"\n",
    "    Processes an image (from sketchpad or upload) for Gradio: segments, preprocesses, and predicts.\n",
    "    image_input_np: NumPy array (RGB) from Gradio input.\n",
    "    model_to_use: The pre-loaded PyTorch model.\n",
    "    class_list: List of class names.\n",
    "    spacing: Spacing to add around characters before resizing for model input.\n",
    "    Returns: Tuple (list_of_visualization_images, detected_text_string)\n",
    "    \"\"\"\n",
    "    if image_input_np is None:\n",
    "        empty_img = np.ones((100,100,3), dtype=np.uint8) * 255\n",
    "        cv2.putText(empty_img, \"No Image\", (10,50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
    "        return [(\"No Image Provided\", empty_img)], \"No image provided\"\n",
    "    \n",
    "    if model_to_use is None:\n",
    "        empty_img = np.ones((100,100,3), dtype=np.uint8) * 255\n",
    "        cv2.putText(empty_img, \"Model Not Loaded\", (10,50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1)\n",
    "        return [(\"Model Not Loaded\", empty_img)], \"ERROR: Model not loaded\"\n",
    "\n",
    "    # Convert RGB NumPy array to BGR for OpenCV\n",
    "    image_bgr = cv2.cvtColor(image_input_np, cv2.COLOR_RGB2BGR)\n",
    "    visualization_steps = [(\"Original Input\", image_input_np.copy())]\n",
    "\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    visualization_steps.append((\"Grayscale\", cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    # Invert colors if background is dark (common for sketchpad if not inverted there)\n",
    "    # Simple heuristic: if mean < 128, assume dark background / light text\n",
    "    if np.mean(gray) < 128:\n",
    "        gray = 255 - gray\n",
    "        visualization_steps.append((\"Inverted Gray (if needed)\", cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    _, binary_for_contours = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    visualization_steps.append((\"Binary for Contours\", cv2.cvtColor(binary_for_contours, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    _, binary_for_model = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # Black text on white\n",
    "    visualization_steps.append((\"Binary for Model Prep\", cv2.cvtColor(binary_for_model, cv2.COLOR_GRAY2RGB)))\n",
    "\n",
    "    contours, _ = cv2.findContours(binary_for_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_img_viz = image_input_np.copy(); cv2.drawContours(contour_img_viz, contours, -1, (0,255,0),2)\n",
    "    visualization_steps.append((f\"All Contours ({len(contours)})\", contour_img_viz))\n",
    "\n",
    "    if not contours: return visualization_steps, \"No contours found.\"\n",
    "\n",
    "    min_area = 30 \n",
    "    letter_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    if not letter_contours: return visualization_steps, f\"No contours > area {min_area}.\"\n",
    "    letter_contours = sorted(letter_contours, key=lambda cnt: cv2.boundingRect(cnt)[0])\n",
    "\n",
    "    filtered_contour_img_viz = image_input_np.copy(); cv2.drawContours(filtered_contour_img_viz, letter_contours, -1, (255,0,0),2)\n",
    "    visualization_steps.append((f\"Filtered Contours ({len(letter_contours)})\", filtered_contour_img_viz))\n",
    "\n",
    "    detected_text_str = \"\"\n",
    "    model_inputs_visual_list = []\n",
    "    final_preds_on_image = image_input_np.copy()\n",
    "    model_to_use.eval() # Ensure model is in eval mode\n",
    "\n",
    "    # Define the transformation for model input (consistent with training)\n",
    "    gradio_input_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    for i, contour in enumerate(letter_contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w < 5 or h < 5: continue\n",
    "\n",
    "        letter_roi = binary_for_model[y:y+h, x:x+w] # Use the non-inverted binary image for the ROI\n",
    "        letter_pil = Image.fromarray(letter_roi).convert('L')\n",
    "\n",
    "        target_s = 64 - spacing\n",
    "        if w > h: new_w, new_h = target_s, int((h/w)*target_s)\n",
    "        else: new_h, new_w = target_s, int((w/h)*target_s)\n",
    "        \n",
    "        resized = letter_pil.resize((new_w, new_h), Image.LANCZOS)\n",
    "        padded = Image.new('L', (target_s, target_s), 255) # White background for padding\n",
    "        px, py = (target_s - new_w)//2, (target_s - new_h)//2\n",
    "        padded.paste(resized, (px, py))\n",
    "        \n",
    "        final_img_for_model_input = Image.new('L', (64,64), 255)\n",
    "        spacing_offset = spacing//2\n",
    "        final_img_for_model_input.paste(padded, (spacing_offset, spacing_offset))\n",
    "        model_inputs_visual_list.append(np.array(final_img_for_model_input))\n",
    "\n",
    "        img_tensor = gradio_input_transform(final_img_for_model_input).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "            char_name = class_list[predicted_idx.item()]\n",
    "        \n",
    "        detected_text_str += char_name\n",
    "        cv2.rectangle(final_preds_on_image, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "        cv2.putText(final_preds_on_image, f\"{char_name} ({confidence.item():.2f})\", (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1)\n",
    "\n",
    "    if model_inputs_visual_list:\n",
    "        model_inputs_grid = create_image_grid_for_gradio([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in model_inputs_visual_list], \"Processed Segments for Model\")\n",
    "        if model_inputs_grid is not None: visualization_steps.append((\"Model Inputs\", model_inputs_grid))\n",
    "    \n",
    "    visualization_steps.append((\"Predictions on Image\", final_preds_on_image))\n",
    "    return visualization_steps, detected_text_str\n",
    "\n",
    "def create_image_grid_for_gradio(images_list, title):\n",
    "    \"\"\"Helper to create a grid of images for Gradio gallery.\"\"\"\n",
    "    if not images_list: return None\n",
    "    cols = 5\n",
    "    rows = (len(images_list) + cols -1) // cols\n",
    "    fig_w, fig_h = cols*2.5, rows*2.5\n",
    "    if os.environ.get('DISPLAY','') == '' and os.name != 'posix': plt.switch_backend('Agg')\n",
    "        \n",
    "    fig = plt.figure(figsize=(fig_w, fig_h)); plt.suptitle(title)\n",
    "    for i, img_data in enumerate(images_list):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        img_title, img = (None, img_data) if not isinstance(img_data, tuple) else img_data\n",
    "        if img_title: ax.set_title(img_title)\n",
    "        cmap = 'gray' if len(img.shape)==2 else None\n",
    "        ax.imshow(img, cmap=cmap); ax.axis('off')\n",
    "    plt.tight_layout(rect=[0,0,1,0.95 if title else 1])\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    grid_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    grid_img = grid_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    return grid_img\n",
    "\n",
    "def launch_gradio_app(model_for_app, class_names_for_app):\n",
    "    def handle_sketch_input(sketch_np_array):\n",
    "        if sketch_np_array is None: return [], \"Please draw something.\"\n",
    "        return process_image_for_gradio(sketch_np_array, model_for_app, class_names_for_app)\n",
    "    \n",
    "    def handle_upload_input(uploaded_pil_image):\n",
    "        if uploaded_pil_image is None: return [], \"Please upload an image.\"\n",
    "        return process_image_for_gradio(np.array(uploaded_pil_image), model_for_app, class_names_for_app)\n",
    "\n",
    "    with gr.Blocks(title=\"Handwritten Character Recognition App\") as app_interface:\n",
    "        gr.Markdown(\"## Handwritten Character Recognition App\")\n",
    "        gr.Markdown(f\"Using Model: **{selected_model_name}** with **{app_num_classes}** classes. Ensure this matches your intent.\")\n",
    "        if model_for_app is None: gr.Markdown(\"**WARNING: MODEL NOT LOADED. PREDICTIONS WILL FAIL.**\")\n",
    "\n",
    "        with gr.Tab(\"Draw Text\"):\n",
    "            sketch_input = gr.Sketchpad(label=\"Draw Here\", type=\"numpy\", image_mode=\"RGB\", invert_colors=False, shape=(700,250))\n",
    "            sketch_button = gr.Button(\"Recognize Drawing\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Tab(\"Upload Image\"):\n",
    "            upload_input = gr.Image(label=\"Upload Image\", type=\"pil\", image_mode=\"RGB\")\n",
    "            upload_button = gr.Button(\"Recognize Uploaded Image\", variant=\"primary\")\n",
    "\n",
    "        recognized_text_output = gr.Textbox(label=\"Recognized Text\")\n",
    "        gallery_output = gr.Gallery(label=\"Processing Steps & Results\", columns=[3], object_fit=\"contain\", height=\"auto\")\n",
    "\n",
    "        sketch_button.click(fn=handle_sketch_input, inputs=sketch_input, outputs=[gallery_output, recognized_text_output])\n",
    "        upload_button.click(fn=handle_upload_input, inputs=upload_input, outputs=[gallery_output, recognized_text_output])\n",
    "        \n",
    "        gr.Markdown(\"### Usage Notes:\n\"+\n",
    "                    \"- **Drawing:** Use your mouse to draw. For multiple characters, leave some space.\n\"+\n",
    "                    \"- **Uploading:** Black text on a white background is preferred.\n\"+\n",
    "                    \"- **Segmentation:** The quality of character segmentation can vary. Clear, well-spaced handwriting works best.\")\n",
    "    app_interface.launch(share=True, debug=True)\n",
    "\n",
    "# Launch the app if a model was loaded\n",
    "if app_model and app_class_labels:\n",
    "    launch_gradio_app(app_model, app_class_labels)\n",
    "else:\n",
    "    print(\"ERROR: Gradio app cannot be launched because the model or class labels could not be loaded.\")\n",
    "    print(\"Please check model paths and data root path for labels.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
